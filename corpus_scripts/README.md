# Quick start
[annotateTuring.py](annotateTuring.py) must be run first in order to tokenize and annotate the whole corpus; folder paths are to be configured in [config.ini](config.ini). Data is extracted through [asLemmata_oneFile.py](asLemmata_oneFile.py) into the output folder (`output` in [config.ini](config.ini)). Output options are selected via user prompt. The output is processed by Valerio's script, whose output is converted into human-readable form by [make_data_readable.py](make_data_readable.py).

The gold-standard for the evaluation of the model is generated through the manual annotation of target-word senses in the corpus using [annotate_senses.py](annotate_senses.py). Information is then to be  extracted by [extract_senses.py](extract_senses.py). 

The file prefixed with `TT_` are related to TreeTagger. [TT_training.py](TT_training.py) runs all training script in the correct sequence and then trains TreeTagger on the data; the parameter file is output as [TreeTaggerData/ancient_greek.txt](TreeTaggerData/ancient_greek.txt). [TT_training_test.py](TT_training_test.py) runs testing scripts in the correct sequence; test results are output as [TreeTaggerData/test_set_results.txt](TreeTaggerData/test_set_results.txt). The corpus is formatted as required by TreeTagger through [TT_format_our_data.py](TT_format_our_data.py). Tagging and the calculation of a disambiguation rate estimate are performed by [TT_corpus_run_and_compare.py](TT_corpus_run_and_compare.py).

# File descriptions
## Scripts
1. **[annotate_senses.py](annotate_senses.py)**: this script searches for target words from `word_senses.xslx` (same location as `file_list.xslx`, to specify in [config.ini](config.ini)) in corpus files (created by [corporaParser.py](corporaParser.py)) and allows the user to annotate the sense of each of them (senses are defined in `word_senses.xslx`).
2. **[annotateTuring.py](annotateTuring.py)**: this script executes in a sequence [tokenizePerseus.py](tokenizePerseus.py), [tokenizeConverted.py](tokenizeConverted.py), and [corporaParser.py](corporaParser.py). This script compiles the whole corpus. The script also runs [TT_format_our_data.py](TT_format_our_data.py), which formats the corpus as required by TreeTagger.
3. **[asLemmata_oneFile.py](asLemmata_oneFile.py)**: this script converts annotated XML files into a single text file with a each sentence on a new line and lemmata instead of words; the stop word filter can be (de)activated in [config.ini](config.ini). Each sentence is preceded by the year to which the work is dated. The user can decide if ID and location are to be included. The user can also filter files by metadata and filter sentences by loading a list of target words (identified by their ID). The user can decided whether to filter ambiguous lemma using TreeTagger data generated by [TT_corpus_run_and_compare.py](TT_corpus_run_and_compare.py).
4. **[corporaParser.py](corporaParser.py)**: this script parses tokenized corpus files.
5. **[extract_context_words.py](extract_context_words.py)**: extracts up to _n_ words to the left and right of the target words listed in `word_senses.xslx` (same location as `file_list.xslx`, to specify in [config.ini](config.ini)) from the whole corpus (file: `fctt.txt` with lemmas represented by ids, to be generated by [asLemmata_oneFile.py](asLemmata_oneFile.py) and placed in the output folder). The value of _n_ is a parameter to be specified in [config.ini](config.ini). The script outputs a CSV file for each target word, containing the collocates sorted by absolute frequency, and extra columns corresponding to the expected senses (specified in `word_senses.xslx`) for human evaluation (i.e. assignment of each collocate to a sense or to no sense (column `generic`)).
6. **[extract_senses.py](extract_senses.py)**: this script extracts sentences containing target words either from `word_senses.xslx` (same location as `file_list.xslx`, to specify in [config.ini](config.ini)) or to be passed as arguments to the script in the terminal shell (as lemma IDs, separated by spaces). The script outputs files (`senses_target word id.txt`) containing the following data (in columns separated by tabs): `work date`, `work genre`, `work author`, `work title`, `sentence location in work`, `sentence id`, `sentence text as word ids (stop-word filter applied)`, `full sentence text (polytonic Greek)`, `target word id`, `sense of target word (sense-id)`.
7. **[make_data_readable.py](make_data_readable.py)**: converts IDs into Greek lemmata (and retains ID information) in output of Valerio's script.
8. _obsolete_ **[match_output_to_senses.py](match_output_to_senses.py)**: reads decoded output files of Valerio's script (which can be selected via user prompt) and matches top word lists for each K with lists of words corresponding to each sense of the target word. The target word can be selected via user prompt from a list contained in `word_senses.xslx` (same location as `file_list.xslx`, to specify in [config.ini](config.ini)). This script outputs text files named _Senses of `word` in `file`.txt_ into the default output folder specified in [config.ini](config.ini).
9. **[tokenizeConverted.py](tokenizeConverted.py)**: tokenizer for other open source corpus files (preliminary converted into XML files).
10. **[tokenizePerseus.py](tokenizePerseus.py)**: tokenizer for [Perseus GitHub](https://github.com/PerseusDL/canonical-greekLit/tree/master/data) corpus files.
11. **[TT_corpus_run_and_compare.py](TT_corpus_run_and_compare.py)**: runs TreeTagger on annotated corpus data and checks how many tokens with multiple lemma annotation are disambiguable. For each token, TreeTagger may select a POS represented by _n_ lemmata (probability of disambiguation: 1/_n_). In the best case scenario, TreeTagger identifies a POS represented by only one lemma; in the worst case, it will identify a POS not represented by any lemma in the annotated corpus (probability = 0 by default). The script generate statistics for each file in the corpus. Paths are configured in  [config.ini](config.ini).
12. **[TT_format_our_data.py](TT_format_our_data.py)**: formats texts converted with [tokenizeConverted.py](tokenizeConverted.py) and [tokenizePerseus.py](tokenizePerseus.py) as input files for TreeTagger (output folder to be set in [config.ini](config.ini)).
13. **[TT_training.py](TT_training.py)**: runs [TT_training_create_openclass.py](TT_training_create_openclass.py), [TT_training_prepare_set.py](TT_training_prepare_set.py), and [TT_training_create_lexicon.py](TT_training_create_lexicon.py); it then runs the TreeTagger training programme (path to be set in [config.ini](config.ini)) and outputs [TreeTaggerData/ancient_greek.txt](TreeTaggerData/ancient_greek.txt).
14. **[TT_training_create_lexicon.py](TT_training_create_lexicon.py)**: generates a [lexicon](TreeTaggerData/lexicon.txt) for TreeTagger integrating [grkLemmata.py](grkLemmata.py) with [training set](TreeTaggerData/training_set.txt) data.
15. **[TT_training_create_openclass.py](TT_training_create_openclass.py)**: extract the POS tagset from [grkLemmata.py](grkLemmata.py) and saves it into [TreeTaggerData/openclass.txt](TreeTaggerData/openclass.txt) for TreeTagger.
16. **[TT_training_prepare_set.py](TT_training_prepare_set.py)**: transforms the PROIEL and Perseus Ancient Greek (except for 7 texts) treebanks into a [training set](TreeTaggerData/training_set.txt) for TreeTagger. The paths of the treebanks are to be specified in [config.ini](config.ini).
17. **[TT_training_prepare_test_set.py](TT_training_prepare_test_set.py)**: transforms 7 documents from the Perseus Ancient Greek treebanks into a [test set](TreeTaggerData/test_set.txt) to be tagged with TreeTagger. The 7 documents amount to ~20% of the available treebanks and are not inluded in the [training set](TreeTaggerData/training_set.txt) generated by [TT_training_prepare_set.py](TT_training_prepare_set.py). The script also generates an annotated [benchmark](TreeTaggerData/test_set_benchmark.txt) version of the same set, containing POS information from the treebank for comparison with the TreeTagger output generated by [TT_training_tag_test_set_and_compare.py](TT_training_tag_test_set_and_compare.py). The path of the Perseus treebank is to be specified in [config.ini](config.ini).
18. **[TT_training_tag_test_set_and_compare.py](TT_training_tag_test_set_and_compare.py)**: this script runs TreeTagger on the output of [TT_training_prepare_test_set.py](TT_training_prepare_test_set.py) and compares the results with the [benchmark](TreeTaggerData/test_set_benchmark.txt) data from the Perseus treebank. The results are output in [TreeTaggerData/test_set_results.txt](TreeTaggerData/test_set_results.txt). Precision, recall, and F- scores are calculated (general, by author, and by genre).
19. **[TT_training_test.py](TT_training_test.py)**: runs [TT_training_prepare_test_set.py](TT_training_prepare_test_set.py) and [TT_training_tag_test_set_and_compare.py](TT_training_tag_test_set_and_compare.py).
20. **[Turing_searchwords.py](Turing_searchwords.py)**: script that counts and stores occurences of words listed in an Excel document. _USED FOR PRELIMINARY DATA EXPLORATION_

## Modules
1. **[beta2utf.py](beta2utf.py)**: function converting betacode Greek into Unicode characters.
2. **[utf2beta.py](utf2beta.py)**: function converting Unicode Greek characters into betacode.

## Dictionaries
1. **[grkFrm.py](grkFrm.py)**: Python dictionary containing all analyses of Greek word forms.
2. **[grkLemmata.py](grkLemmata.py)**: Python dictionary containing all Greek lemmata.
3. **[stop_cltk.py](stop_cltk.py)**: Perseus stop word list (from [CLTK](https://github.com/cltk/cltk/blob/master/cltk/stop/greek/stops.py)); added support for oxia acute accent vowel glyphs and list as IDs.
4. **[tlgIndex.py](tlgIndex.py)**: Python dictionary containing the TLG IDs of Greek authors.

## Utilities
1. **[consult_dictionary.py](consult_dictionary.py)**: converts forms into ids and ids into forms. Multiple forms/ids accepted, separated with comma. If form corresponds to multiple ids, these are returned with a forward slash separating them.

## Configuration
1. **[config.ini](config.ini)**: configuration file. It stores the paths (1) of the Excel documents listing corpus files, storing metadata, and word senses; (2) of the folder containing the corpus source files; (3) of the destination folders of the tokenizer and of the parser, as well as those of logs, and the output folder for all scripts that extract and evaluate data; (4) of the word lists to be used by [asLemmata_oneFile.py](asLemmata_oneFile.py), (5) of the treebanks to be used by [TT_training_prepare_set.py](TT_training_prepare_set.py), (6) of TreeTagger. It also contains values of parameters (activate/deactivate stop word filter in [asLemmata_oneFile.py](asLemmata_oneFile.py)) and the cell ranges in `file_list.xslx` from which headers and file paths should be extracted and to which data should be written by [tokenizeConverted.py](tokenizeConverted.py), [tokenizePerseus.py](tokenizePerseus.py), [corporaParser.py](corporaParser.py), and [asLemmata_oneFile.py](asLemmata_oneFile.py).
