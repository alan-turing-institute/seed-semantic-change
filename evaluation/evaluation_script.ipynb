{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a work-in-progress notebook\n",
    "\n",
    "We wish to know this:\n",
    "\n",
    "1. How well does the model identify the correct number of senses for the target word?\n",
    "2. **How well does the model identify the correct senses for the target word?**\n",
    "3. **How well does the model assign the right words to a given sense of the target word?**\n",
    "4. How well does the model assign the senses to the time intervals for the target word?\n",
    "\n",
    "The script will evaluate **Q2** and **Q3**. Q4 will follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output filenames are based on target+param+iteration+K+variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target ID: 69419\n",
      "<_io.TextIOWrapper name='/Users/hengchen/git/seed-semantic-change/evaluation/evaluation_input/new_texts/senses_69419.txt' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='/Users/hengchen/git/seed-semantic-change/src/dynamic-senses/greek_input/all_results/3_output_K15/69419_fixed_time_no_ghost/output.dat' mode='r' encoding='UTF-8'>\n",
      "_v11_mus\n",
      "/Users/hengchen/git/seed-semantic-change/evaluation/evaluation_output/69419_v11_mus\n",
      "window_size 5\n",
      "num_top 15\n",
      "iterations 1000\n",
      "time_interval 100\n",
      "start_time -430\n",
      "end_time 365\n",
      "<_io.TextIOWrapper name='/Users/hengchen/git/seed-semantic-change/evaluation/evaluation_output/69419_v11_musgenre_all_i1000_k15_time_interval100.txt' mode='w' encoding='UTF-8'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "93"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic variables and imports:\n",
    "\n",
    "import codecs, csv, os, time, re, io\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from  more_itertools import unique_everseen\n",
    "\n",
    "# directories\n",
    "\n",
    "dir_in = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), \"src\", \"dynamic-senses\",\"greek_input\",\"all_results\"))\n",
    "dir_out = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), \"evaluation\", \"evaluation_output\"))\n",
    "#dir_expert = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), \"evaluation\", \"evaluation_input\"))\n",
    "\n",
    "dir_expert = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), \"evaluation\", \"evaluation_input\",\"new_texts\"))\n",
    "#  SENSES MERGED harmonia\n",
    "#dir_expert = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), \"corpus_scripts_output\")) \n",
    "\n",
    "dir_parameter = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), \"src\", \"dynamic-senses\",\"greek_input\",\"all_parameters\"))\n",
    "\n",
    "# change parameter file and senses etc here\n",
    "\n",
    "genre = \"all\"  # \"all\" by default. To focus on a specific genre change the value, cfr \"s_senses\" file\n",
    "\n",
    "s_senses = io.open(dir_expert+\"/senses_69419.txt\",\"r\")  # expert senses annotation\n",
    "k_senses = io.open(dir_in+\"/3_output_K15/69419_fixed_time_no_ghost/output.dat\",\"r\") # model output\n",
    "parameter_file = io.open(dir_parameter+\"/parameters_v11_mus.txt\",\"r\")\n",
    "\n",
    "bugfix = 0\n",
    "\n",
    "target_id = os.path.basename(s_senses.name)\n",
    "target_id = target_id.replace(\"senses_\",\"\")\n",
    "target_id = target_id.replace(\".txt\",\"\")\n",
    "print(\"Target ID:\",target_id)\n",
    "\n",
    "param_name = os.path.basename(parameter_file.name)\n",
    "param_name = param_name.replace(\"params\",\"\")\n",
    "param_name = param_name.replace(\"parameters\",\"\")\n",
    "param_name = param_name.replace(\".txt\",\"\")\n",
    "\n",
    "\n",
    "print(s_senses)\n",
    "print(k_senses)\n",
    "print(param_name)\n",
    "print(dir_out+\"/\"+target_id+param_name)\n",
    "\n",
    "\n",
    "# DEBUG:\n",
    "#s_senses = io.open(dir_in+\"/senses_69419_debug.txt\",\"r\")\n",
    "#k_senses = io.open(dir_in+\"/mus_debug.dat\",\"r\")\n",
    "# k0 = mus4\n",
    "# k1 = mus3\n",
    "# k2 = mus2\n",
    "# k3 = mus1\n",
    "# k4 = nothing\n",
    "\n",
    "\n",
    "\n",
    "file_senses = s_senses.readlines()[1:]\n",
    "output_senses = k_senses.read()\n",
    "\n",
    "#### SOME PARAMETERS ARE READ IN THE PARAMETER FILE\n",
    "\n",
    "i = 0\n",
    "for line in parameter_file.readlines():\n",
    "    line = line.split(\"\\t\")\n",
    "    if line[0] == \"window_size\":\n",
    "        window_size = int(line[1])\n",
    "        print(\"window_size\",window_size)\n",
    "            \n",
    "    if line[0] == \"iterations\":\n",
    "        iterations = int(line[1])\n",
    "        print(\"iterations\",iterations)\n",
    "            \n",
    "    if line[0] == \"time_interval\":\n",
    "        time_interval = int(line[1])\n",
    "        print(\"time_interval\",time_interval)\n",
    "        \n",
    "    if line[0] == \"num_top\":\n",
    "        num_top = int(line[1])\n",
    "        print(\"num_top\",num_top)\n",
    "            \n",
    "        \n",
    "#### START TIME AND END TIME ARE NOW DEFINED BY READING THE ANNOTATION\n",
    "## (because we have the same parameter file for all things)\n",
    "\n",
    "annotation_dates = list()\n",
    "for line in file_senses:\n",
    "    cells = line.split(\"\\t\")\n",
    "    \n",
    "    if genre == \"all\":  # if we take everything\n",
    "        annotation_dates.append(int(cells[0]))\n",
    "    \n",
    "    else:\n",
    "        if cells[1] == genre:\n",
    "            annotation_dates.append(int(cells[0]))\n",
    "            \n",
    "start_time = min(annotation_dates)\n",
    "end_time = max(annotation_dates)\n",
    "\n",
    "print(\"start_time\",start_time)\n",
    "print(\"end_time\",end_time)\n",
    "\n",
    "results_file = io.open(dir_out+\"/\"+target_id+param_name+\"genre_\"+genre+\"_i\"+str(iterations)+\"_k\"+str(num_top)+\"_time_interval\"+str(time_interval)+\".txt\",\"w\")\n",
    "print(results_file)\n",
    "\n",
    "results_file.write(\"Target ID %s Window size %s Start time %s End time %s Time Interval %s Iterations %s\\n\" % (target_id,window_size,start_time,end_time,time_interval,iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slice duration 100\n",
      "start time: -430 end time: 365\n"
     ]
    }
   ],
   "source": [
    "# Defining time periods from the output file\n",
    "\n",
    "total_years = end_time-start_time\n",
    "perioddd = 0\n",
    "temp_start_time = start_time\n",
    "\n",
    "for year in range(temp_start_time,end_time+1):\n",
    "    if temp_start_time + time_interval < end_time+1:\n",
    "        perioddd +=1\n",
    "        #print(perioddd)\n",
    "        temp_start_time += time_interval\n",
    "\n",
    "number_of_slices = perioddd + 1\n",
    "\n",
    "slice_duration = time_interval # read from the parameter file\n",
    "print(\"slice duration\",slice_duration)\n",
    "\n",
    "print(\"start time:\",start_time,\"end time:\",end_time)\n",
    "\n",
    "slice_years = dict()\n",
    "\n",
    "for period in range(0,number_of_slices):\n",
    "    slice_years[period] = list()\n",
    "    \n",
    "   # if period == number_of_slices-1:\n",
    "   #     for i in range(latest_i,end_time):\n",
    "   #         slice_years[period].append(i)  \n",
    "    \n",
    "    #if period != number_of_slices-1:\n",
    "    for i in range(start_time,end_time+1):\n",
    "        \n",
    "        if i >= int(period*slice_duration) + start_time:\n",
    "            if i < int((period+1)*slice_duration) + start_time:\n",
    "                slice_years[period].append(i)\n",
    "                latest_i = i\n",
    "                \n",
    "#slice_years[period].append(latest_i+1)  #  in case we have a slice that ends on a century\n",
    "                    \n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "#print(slice_years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- ~~create the notebook~~\n",
    "- ~~organise the notebook~~\n",
    "- ~~write \"general idea\" pseudocode for the evaluation~~\n",
    "- ~~get input files~~\n",
    "- ~~figure out data structures to store the variables~~\n",
    "- ~~write actual code~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: How well does the model identify the correct senses for the target word?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For each target word, we have a list of senses  s (given by the expert)\n",
    "# For each target word, we have a list of senses k (given by the model)\n",
    "# This Q consists in matching s and k, and doing so in a confident way --> confidence score\n",
    "\n",
    "for each k:\n",
    "    for each s:\n",
    "        create conf(k,s)\n",
    "\n",
    "# What is conf(k,s)?\n",
    "        conf(k,s) = (p1*match(w1,s)+p2*match(w1,s)+px(wx,s))/10 WHERE\n",
    "    \n",
    "            px = probability of word wx \n",
    "                \n",
    "                and\n",
    "            \n",
    "            match(wx,s) =   1/number_of_senses_assigned_to_wx if s_is_one_of_them \n",
    "            \n",
    "                    or \n",
    "                            0 if w_is_not_associated_to_s\n",
    "                \n",
    "# Once we have gone through all s for one k, we have to choose the best k for s. How? (TBD, cfr Valerio and Barbara)\n",
    "\n",
    "# Once all ks have been assigned to all ss (or NA), we can calculate a general confidence score for the model.\n",
    "# One easy way to do that: \n",
    "\n",
    "conf_score_model = number_of_non_NA/k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real code\n",
    "\n",
    "Steps:\n",
    "\n",
    "- extract all senses from the file\n",
    "- use those senses as keys for a dictionary, `dict_of_words`\n",
    "- fill the dictionary: for each key, we store a list of words pertaining to that sense\n",
    "- transform the lists as sets so as to remove duplicates within the same sense\n",
    "- create a dictionary with a word as a key and its weight as a value, depending on how many senses it appears\n",
    "- parse the model output and get the probability weights for each word\n",
    "- do not take into account the first line\n",
    "- take care of empty lines\n",
    "\n",
    "Todo:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of senses: 4 ['mus-1', 'NA', 'mus-4', 'mus-2']\n",
      "sentences smaller than window size 5 : 155\n",
      "number of NA words: 498\n",
      "same? 498\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expert_senses = list() # list where we store all sense ids provided by expert\n",
    "#sense_for_period_counter = 0\n",
    "\n",
    "\n",
    "for line in file_senses:\n",
    "    cells = line.split(\"\\t\")\n",
    "    \n",
    "    if genre == \"all\":  # if we take everything    \n",
    "        sense = cells[11] # The sense ID is after the 10th tab\n",
    "        if sense != 'w':\n",
    "            if int(cells[12]) == 1:   ## we only take the senses annotated because of collocates and nothing else\n",
    "                #print(int(s[12]))\n",
    "                expert_senses.append(sense)    \n",
    "            else:\n",
    "                expert_senses.append(\"NA\") # if the reason for finding the sense is not \"collocates\" (1), the sense NA is created\n",
    "        else:\n",
    "             expert_senses.append(\"NA\")\n",
    "                \n",
    "    else: # specific genre only\n",
    "        \n",
    "        if cells[1] == genre:\n",
    "\n",
    "            sense = cells[11] # The sense ID is after the 10th tab\n",
    "            if sense != 'w':\n",
    "                if int(cells[12]) == 1:   ## we only take the senses annotated because of collocates and nothing else\n",
    "                    #print(int(s[12]))\n",
    "                    expert_senses.append(sense)    \n",
    "                else:\n",
    "                    expert_senses.append(\"NA\") # if the reason for finding the sense is not \"collocates\" (1), the sense NA is created\n",
    "            else:\n",
    "                 expert_senses.append(\"NA\")\n",
    "\n",
    "\n",
    "#print(len(expert_senses),expert_senses,len(set(expert_senses)))\n",
    "\n",
    "\n",
    "expert_senses_set = list(set(expert_senses)) # we only keep the unique senses\n",
    "expert_senses = list(unique_everseen(expert_senses))\n",
    "#print(\"SET\",expert_senses_set)\n",
    "#print(\"ITERTOOLS\",expert_senses)\n",
    "\n",
    "number_of_s = len(expert_senses)  # we create a variable that stores the number of unique senses\n",
    "print(\"Number of senses:\",number_of_s,expert_senses)\n",
    "\n",
    "# This dictionary has a sense as a key, and a list of words as a value. \n",
    "dict_of_words = dict()\n",
    "# This list stores all words\n",
    "list_of_all_words = list()\n",
    "# This dictionary stores all words as keys and their weight as value\n",
    "word_weight = dict()\n",
    "# This dictionary stores the number of times a sense appears in a slice\n",
    "sense_date_amount = dict()\n",
    "\n",
    "# This list stores words in w and not collocates senses\n",
    "list_of_NA_words = list()\n",
    "\n",
    "dummy_counter = 0\n",
    "\n",
    "\n",
    "sentences_smaller_than_window_size = 0\n",
    "\n",
    "for i in range(0,number_of_s): # for each sense, we create a dictionary entry which has a list as value\n",
    "    dict_of_words[expert_senses[i]] = list()\n",
    "    #print(expert_senses[i])\n",
    "\n",
    "    for line in file_senses: # we go back in the file\n",
    "        \n",
    "        \n",
    "        \n",
    "        cells = line.split(\"\\t\") # splitting on tabs\n",
    "        \n",
    "        if genre == \"all\":   # all genres\n",
    "            \n",
    "            if int(cells[12]) == 1:  # senses inferred from collocates\n",
    "                if cells[11] == expert_senses[i]:      # we store all words for one sense \n",
    "\n",
    "\n",
    "                    sentence_of_ids = cells[8] # 8 is for IDs, 9 is for words\n",
    "                    list_of_ids = sentence_of_ids.split(\" \")  # splitting on spaces\n",
    "\n",
    "\n",
    "                    index_of_target = list_of_ids.index(target_id) # getting the location of the target word in the collocates\n",
    "                    list_of_ids_window = list()\n",
    "                    for n in range(index_of_target-window_size,index_of_target+window_size+1): # for every word in the window_size range\n",
    "                        try:   # if there's a word it's taken into account\n",
    "\n",
    "                            list_of_ids_window.append(list_of_ids[n])\n",
    "\n",
    "                        except IndexError: # if there isn't, too bad\n",
    "                            sentences_smaller_than_window_size += 1\n",
    "\n",
    "                    for word_id in list_of_ids_window:\n",
    "                        if int(cells[12]) == 1:\n",
    "\n",
    "\n",
    "                            if cells[11] == expert_senses[i]:  \n",
    "\n",
    "                                dict_of_words[expert_senses[i]].append(word_id)                    \n",
    "\n",
    "                        list_of_all_words.append(word_id) # we store all words, we'll iterate over that for scores\n",
    "\n",
    "\n",
    "                # if the sense is \"w\", the collocates are put in the \"NA words\"\n",
    "\n",
    "                if cells[11] == \"w\":\n",
    "                    #print(\"W\")\n",
    "                    sentence_of_ids = cells[8] # 8 is for IDs, 9 is for words\n",
    "                    list_of_ids = sentence_of_ids.split(\" \")  # splitting on spaces\n",
    "                    index_of_target = list_of_ids.index(target_id) # getting the location of the target word in the collocates\n",
    "                    list_of_ids_window = list()\n",
    "                    for n in range(index_of_target-window_size,index_of_target+window_size+1): # for every word in the window_size range\n",
    "                        try:   # if there's a word it's taken into account\n",
    "\n",
    "                            list_of_NA_words.append(list_of_ids[n])\n",
    "                            list_of_all_words.append(list_of_ids[n])\n",
    "                        except IndexError:\n",
    "                            dummy_counter +=1\n",
    "\n",
    "            else:  # words that are in \"non collocates senses\"\n",
    "                #print(\"NON COLOC\")\n",
    "                sentence_of_ids = cells[8] # 8 is for IDs, 9 is for words\n",
    "                #print(sentence_of_ids)\n",
    "                list_of_ids = sentence_of_ids.split(\" \")  # splitting on spaces\n",
    "                index_of_target = list_of_ids.index(target_id) # getting the location of the target word in the collocates\n",
    "                #print(index_of_target)\n",
    "                list_of_ids_window = list()\n",
    "                #print(index_of_target-window_size,index_of_target+window_size+1)\n",
    "                for n in range(index_of_target-window_size,index_of_target+window_size+1): # for every word in the window_size range\n",
    "                    try:   # if there's a word it's taken into account\n",
    "                        #print(list_of_ids_window)\n",
    "                        #print(list_of_ids[n])\n",
    "                        list_of_NA_words.append(list_of_ids[n])\n",
    "                        list_of_all_words.append(list_of_ids[n])\n",
    "                    except IndexError:\n",
    "                            dummy_counter +=1\n",
    "                #print(list_of_NA_words)\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            if cells[1] == genre:\n",
    "                if int(cells[12]) == 1:  # senses inferred from collocates\n",
    "                    if cells[11] == expert_senses[i]:      # we store all words for one sense \n",
    "\n",
    "\n",
    "                        sentence_of_ids = cells[8] # 8 is for IDs, 9 is for words\n",
    "                        list_of_ids = sentence_of_ids.split(\" \")  # splitting on spaces\n",
    "\n",
    "\n",
    "                        index_of_target = list_of_ids.index(target_id) # getting the location of the target word in the collocates\n",
    "                        list_of_ids_window = list()\n",
    "                        for n in range(index_of_target-window_size,index_of_target+window_size+1): # for every word in the window_size range\n",
    "                            try:   # if there's a word it's taken into account\n",
    "\n",
    "                                list_of_ids_window.append(list_of_ids[n])\n",
    "\n",
    "                            except IndexError: # if there isn't, too bad\n",
    "                                sentences_smaller_than_window_size += 1\n",
    "\n",
    "                        for word_id in list_of_ids_window:\n",
    "                            if int(cells[12]) == 1:\n",
    "\n",
    "\n",
    "                                if cells[11] == expert_senses[i]:  \n",
    "\n",
    "                                    dict_of_words[expert_senses[i]].append(word_id)                    \n",
    "\n",
    "                            list_of_all_words.append(word_id) # we store all words, we'll iterate over that for scores\n",
    "\n",
    "\n",
    "                    # if the sense is \"w\", the collocates are put in the \"NA words\"\n",
    "\n",
    "                    if cells[11] == \"w\":\n",
    "                        #print(\"W\")\n",
    "                        sentence_of_ids = cells[8] # 8 is for IDs, 9 is for words\n",
    "                        list_of_ids = sentence_of_ids.split(\" \")  # splitting on spaces\n",
    "                        index_of_target = list_of_ids.index(target_id) # getting the location of the target word in the collocates\n",
    "                        list_of_ids_window = list()\n",
    "                        for n in range(index_of_target-window_size,index_of_target+window_size+1): # for every word in the window_size range\n",
    "                            try:   # if there's a word it's taken into account\n",
    "\n",
    "                                list_of_NA_words.append(list_of_ids[n])\n",
    "                                list_of_all_words.append(list_of_ids[n])\n",
    "                            except IndexError:\n",
    "                                dummy_counter +=1\n",
    "\n",
    "                else:  # words that are in \"non collocates senses\"\n",
    "                    #print(\"NON COLOC\")\n",
    "                    sentence_of_ids = cells[8] # 8 is for IDs, 9 is for words\n",
    "                    #print(sentence_of_ids)\n",
    "                    list_of_ids = sentence_of_ids.split(\" \")  # splitting on spaces\n",
    "                    index_of_target = list_of_ids.index(target_id) # getting the location of the target word in the collocates\n",
    "                    #print(index_of_target)\n",
    "                    list_of_ids_window = list()\n",
    "                    #print(index_of_target-window_size,index_of_target+window_size+1)\n",
    "                    for n in range(index_of_target-window_size,index_of_target+window_size+1): # for every word in the window_size range\n",
    "                        try:   # if there's a word it's taken into account\n",
    "                            #print(list_of_ids_window)\n",
    "                            #print(list_of_ids[n])\n",
    "                            list_of_NA_words.append(list_of_ids[n])\n",
    "                            list_of_all_words.append(list_of_ids[n])\n",
    "                        except IndexError:\n",
    "                                dummy_counter +=1\n",
    "                    #print(list_of_NA_words)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    # Here, we remove duplicates\n",
    "    #dict_of_words[expert_senses[i]].append(\"79223\") #testing\n",
    "    \n",
    "    dict_of_words[expert_senses[i]] = list(set(dict_of_words[expert_senses[i]]))\n",
    "    \n",
    "    \n",
    "      \n",
    "    #print(\"i\",i,\"sense\",expert_senses[i],\"number of words\",len(dict_of_words[expert_senses[i]]))\n",
    "    #print(\"words\",set(dict_of_words[expert_senses[i]]))\n",
    "    #print(\"\\n\\n\")\n",
    "\n",
    "# NOW THAT WE HAVE A LIST OF NA WORDS \n",
    "# We can put that in the dict_of_words[expert_sense] dictionary\n",
    "#print(\"NA words\",list_of_NA_words)\n",
    "list_of_NA_words = list(set(list_of_NA_words))\n",
    "dict_of_words[\"NA\"] = list_of_NA_words\n",
    "\n",
    "print(\"sentences smaller than window size\",window_size,\":\",sentences_smaller_than_window_size)\n",
    "#print(\"dummy\",dummy_counter)\n",
    "print(\"number of NA words:\",len(list_of_NA_words))\n",
    "print(\"same?\",len(dict_of_words[\"NA\"]))\n",
    "\n",
    "results_file.write(\"Expert senses %s Total %s \\n\" %(expert_senses,len(expert_senses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sense_date_amount {('mus-1', 0): 3, ('mus-1', 1): 4, ('mus-1', 2): 1, ('mus-1', 3): 2, ('mus-1', 4): 2, ('mus-1', 5): 14, ('mus-1', 6): 14, ('mus-1', 7): 0, ('NA', 0): 16, ('NA', 1): 3, ('NA', 2): 3, ('NA', 3): 2, ('NA', 4): 6, ('NA', 5): 15, ('NA', 6): 45, ('NA', 7): 1, ('mus-4', 0): 21, ('mus-4', 1): 0, ('mus-4', 2): 0, ('mus-4', 3): 1, ('mus-4', 4): 0, ('mus-4', 5): 21, ('mus-4', 6): 8, ('mus-4', 7): 1, ('mus-2', 0): 0, ('mus-2', 1): 0, ('mus-2', 2): 0, ('mus-2', 3): 0, ('mus-2', 4): 0, ('mus-2', 5): 0, ('mus-2', 6): 22, ('mus-2', 7): 0}\n",
      "205\n"
     ]
    }
   ],
   "source": [
    "sense_date_amount = dict() # dict where we have the number of senses for [sense,period]\n",
    "\n",
    "# this dictionary is now initialised with 0 for each CORRECT sense,period pair we have\n",
    "for sense in expert_senses:\n",
    "    for period in slice_years.keys():\n",
    "        sense_date_amount[sense,period] = 0\n",
    "        \n",
    "#print(sense_date_amount,\"\\n\")        \n",
    "\n",
    "counter = 0\n",
    "\n",
    "for line in file_senses: \n",
    "    #print(len(file_senses))\n",
    "    cells = line.split(\"\\t\")\n",
    "    \n",
    "    if genre == \"all\":\n",
    "    \n",
    "        sense = cells[11] # The sense ID is after the 10th tab\n",
    "        if sense != 'w':\n",
    "            #print(sense)\n",
    "\n",
    "\n",
    "            if int(cells[12]) == 1:   ## we only take the senses annotated because of collocates and nothing else\n",
    "                #print(int(s[12]))\n",
    "                #expert_senses.append(sense)\n",
    "\n",
    "                for period in slice_years.keys():\n",
    "                    if int(cells[0]) in slice_years[period]:\n",
    "                        #sense_for_period_counter += 1\n",
    "                        sense_date_amount[sense,period] += 1\n",
    "                        counter += 1\n",
    "\n",
    "            else:\n",
    "                for period in slice_years.keys():\n",
    "                    if int(cells[0]) in slice_years[period]:\n",
    "                        #sense_for_period_counter += 1\n",
    "                        sense_date_amount[\"NA\",period] += 1\n",
    "                        counter += 1\n",
    "\n",
    "\n",
    "\n",
    "        if sense == \"w\":  # counting the NAs\n",
    "                for period in slice_years.keys():\n",
    "                    if int(cells[0]) in slice_years[period]:\n",
    "                        #sense_for_period_counter += 1\n",
    "                        sense_date_amount[\"NA\",period] += 1\n",
    "                        counter += 1\n",
    "                    \n",
    "\n",
    "    else:\n",
    "        if genre == cells[1]:\n",
    "            sense = cells[11] # The sense ID is after the 10th tab\n",
    "            if sense != 'w':\n",
    "                #print(sense)\n",
    "\n",
    "\n",
    "                if int(cells[12]) == 1:   ## we only take the senses annotated because of collocates and nothing else\n",
    "                    #print(int(s[12]))\n",
    "                    #expert_senses.append(sense)\n",
    "\n",
    "                    for period in slice_years.keys():\n",
    "                        if int(cells[0]) in slice_years[period]:\n",
    "                            #sense_for_period_counter += 1\n",
    "                            sense_date_amount[sense,period] += 1\n",
    "                            counter += 1\n",
    "\n",
    "                else:\n",
    "                    for period in slice_years.keys():\n",
    "                        if int(cells[0]) in slice_years[period]:\n",
    "                            #sense_for_period_counter += 1\n",
    "                            sense_date_amount[\"NA\",period] += 1\n",
    "                            counter += 1\n",
    "\n",
    "\n",
    "\n",
    "            if sense == \"w\":  # counting the NAs\n",
    "                    for period in slice_years.keys():\n",
    "                        if int(cells[0]) in slice_years[period]:\n",
    "                            #sense_for_period_counter += 1\n",
    "                            sense_date_amount[\"NA\",period] += 1\n",
    "                            counter += 1\n",
    "                    \n",
    "print(\"sense_date_amount\",sense_date_amount)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mus-1', 'NA', 'mus-4', 'mus-2']\n"
     ]
    }
   ],
   "source": [
    "# For every word in the list of words that we have\n",
    "# we count the number of senses it appears in\n",
    "# we use that number to divide its importance: 1 sense = 1 importance; 2 senses = 0.5 importance\n",
    "# this can be finetuned\n",
    "\n",
    "print(expert_senses)\n",
    "#print(\"list_of_NA_words\",list_of_NA_words,\"\\n\\n\")\n",
    "#print(\"list of mus 1 words\",dict_of_words['mus-1'])\n",
    "\n",
    "# This dictionary stores words in w and not collocates senses\n",
    "word_weight_NA = dict()\n",
    "\n",
    "for word in list_of_all_words:\n",
    "#word = \"113560\"\n",
    "    x = 0  # number of times word appears in different senses excluding NAs\n",
    "    z = 0 # number of times word appears in different senses including NAs\n",
    "#print(word)\n",
    "    for i in range(0,number_of_s):\n",
    "        if word in dict_of_words[expert_senses[i]]:\n",
    "            #if word == \"105344\":\n",
    "            #    print(\"YO\")\n",
    "                \n",
    "            if expert_senses[i] != \"NA\":\n",
    "                x += 1 \n",
    "                #print(\"froot the loop\")\n",
    "                \n",
    "\n",
    "    if x != 0:\n",
    "        word_weight[word] = float(1/x)\n",
    "        #print(word,\"this is a x!=0\",x,\"this is its weight\",float(1/x))\n",
    "        \n",
    "        if word in list_of_NA_words:\n",
    "            z = x + 1\n",
    "            #print(z)\n",
    "            #print(word,\"this is a z\",z,\"this is its weight\",float(1/z))\n",
    "            \n",
    "            word_weight_NA[word] = float(1/z)\n",
    "            \n",
    "    else:  # if word doesn't exist in non-NA senses\n",
    "        word_weight_NA[word] = 1/1\n",
    "        word_weight[word] = 0  # with this we prevent the case that a word that is ONLY in NA has no weight \n",
    "  \n",
    "        \n",
    "    #else: \n",
    "    #    word_weight_NA[word] = word_weight[word]\n",
    "        \n",
    "#print(word_weight_NA)    \n",
    "#print(word,word_weight[word])\n",
    "\n",
    "#print(word_weight_NA[\"53826\"])\n",
    "\n",
    "#for key in word_weight_NA.keys():\n",
    "#    print(key,word_weight_NA[key])\n",
    "\n",
    "\n",
    "#for word in list_of_all_words:\n",
    "#    try: \n",
    "#        print(word,word_weight[word])\n",
    "#    except KeyError:\n",
    "#        print(word,\"isnt in non-NA senses\")\n",
    "#    try:\n",
    "#        print(word,word_weight_NA[word])\n",
    "#    except KeyError:\n",
    "#        print(word,\"isnt in NA sense\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parsing output.dat\n",
    "- split on \"===============  per time  ===============\" and keep first part\n",
    "- transform that into a list, then\n",
    "- get lines that start with \"p(w|s)\"\n",
    "- count those, k = that number\n",
    "- split the line on \":\", keep the second part\n",
    "- split the rest on \";\", it's [ID] = prob_from_this_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_output = output_senses.split(\"===============  per time  ===============\")[0].split(\"\\n\")\n",
    "\n",
    "number_of_the_k = 0\n",
    "\n",
    "k_words_with_prob = dict()\n",
    "\n",
    "for line in lines_output:\n",
    "    if line[:6] == \"p(w|s)\":\n",
    "        #print(line)\n",
    "        line = line.split(\":\")[1]\n",
    "        line = line.split(\";\")\n",
    "        #print(number_of_the_k,line)\n",
    "        dico_word_prob = dict()\n",
    "        temp_dict = dict()\n",
    "        k_words_with_prob[number_of_the_k] = list()\n",
    "        \n",
    "        line = line[:-1] # last item of the list is empty\n",
    "        \n",
    "        total_probability = 0 # to have relative probs\n",
    "        #print(line)\n",
    "        for word_prob in line:\n",
    "            #print(word_prob)\n",
    "\n",
    "        \n",
    "            #word_prob = word_prob.split(\",\")\n",
    "            #for word in word_prob:\n",
    "            probability = re.findall(\"([\\d.\\w]*)\",word_prob)\n",
    "            if probability:\n",
    "                probability = list(filter(None,probability))\n",
    "                    \n",
    "            total_probability += float(probability[1])\n",
    "            #print(\"word id\",probability[0],\"; probability\",probability[1])\n",
    "        \n",
    "            dico_word_prob[probability[0]] = float(probability[1])\n",
    "        #print(type(k_words_with_prob[number_of_the_k]))\n",
    "        \n",
    "        for i in dico_word_prob.keys():\n",
    "            \n",
    "            temp_dict[i] = float(dico_word_prob[i]/total_probability)\n",
    "            k_words_with_prob[number_of_the_k] = temp_dict\n",
    "            \n",
    "            #print(k_words_with_prob[number_of_the_k])\n",
    "            \n",
    "        #k_words_with_prob[number_of_the_k] = [float(dico_word_prob[i]/total_probability) for i in dico_word_prob]\n",
    "        #print(k_words_with_prob[number_of_the_k])\n",
    "        #print(temp_dict)\n",
    "        number_of_the_k += 1\n",
    "        \n",
    "\n",
    "results_file.write(\"Output senses %s \\n\\n\" %(number_of_the_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k_words_with_prob\n",
    "This dictionary has the sense number 'k' as keys and the a dictionary of [word] = probability as values.\n",
    "Example below."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for key in k_words_with_prob.keys():\n",
    "    #print(\"output sense\",key)\n",
    "    #for i in range(0,number_of_s):\n",
    "        #print(\"\\texpert sense number \", i, expert_senses[i])\n",
    "        #for second_key in k_words_with_prob[key].keys(): # Barbara's note: shouldn't it be k_words_with_prob[i] here?\n",
    "            #print(\"\\t\\tword from annotation for sense\", key, \":\", second_key)\n",
    "            #if second_key in dict_of_words[expert_senses[i]]:\n",
    "                #if expert_senses[i] != \"NA\":\n",
    "                    #print(\"\\t\\t\\t\\tnormal\")\n",
    "                    #print(\"\\t\\t\\tword \", second_key, \"is in output for sense\", key, \"with probability:\", k_words_with_prob[key][second_key], \"and weight:\", word_weight[second_key])\n",
    "\n",
    "                #else:\n",
    "                    #print(\"\\t\\t\\t\\tNA\")\n",
    "                    #print(\"\\t\\t\\tword \", second_key, \"is in output for sense\", key, \"with probability:\", k_words_with_prob[key][second_key], \"and weight:\", word_weight_NA[second_key])\n",
    "\n",
    "                \n",
    "# Here we get all the senses and for each sense we do a matching between the k words and s words and get the probability\n",
    "# For some reason the first word for each sense arrives several times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mus-1', 'NA', 'mus-4', 'mus-2']\n"
     ]
    }
   ],
   "source": [
    "print(expert_senses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of expert senses s: 4\n",
      "number of model output senses k: 15\n"
     ]
    }
   ],
   "source": [
    "## Calculating confidence score for each (words_of_k,words_of_s) pair\n",
    "\n",
    "# conf(k,s) = (p1*match(w1,s)+p2*match(w1,s)+px(wx,s))\n",
    "        # match(wx,s) =   1/number_of_senses_assigned_to_wx if s_is_one_of_them \n",
    "\n",
    "    \n",
    "print(\"number of expert senses s:\",number_of_s)\n",
    "print(\"number of model output senses k:\",len(k_words_with_prob.keys()))\n",
    "compteur = 0\n",
    "\n",
    "match = dict()\n",
    "conf = dict()\n",
    "for k in k_words_with_prob.keys():  # for each output sense, we go through...\n",
    "    #print(\"\\n\")\n",
    "    #print(\"Choose best match for k =\",k)\n",
    "    for s in range(0,number_of_s):       # each expert sense\n",
    "        \n",
    "        conf[k,s] = 0 \n",
    "        \n",
    "        #print(\"expert sense\",s)\n",
    "        for mot in k_words_with_prob[k]:      # for each word within output by the model for the output sense\n",
    "            #print(\"k,mot\",k,mot)\n",
    "            #print(\"s\",s,\"expert_senses[s]\",expert_senses[s])\n",
    "            \n",
    "            if expert_senses[s] == \"NA\":\n",
    "                \n",
    "                if mot in dict_of_words[expert_senses[s]]:  # if that word exists in the list of expert words for that sense\n",
    "\n",
    "                    #print(\"s,dict_of_words[expert_senses[s]])\",dict_of_words[expert_senses[s]])\n",
    "                    #print(\"k_words_with_prob[k][mot]\",k_words_with_prob[k][mot])\n",
    "                    #print(\"word_weight[mot]\",word_weight_NA[mot])\n",
    "\n",
    "                    #for word in list_of_all_words:  # this help getting a key for a dictionary later on\n",
    "                     #   if mot == word:\n",
    "                    match_weighted = float((k_words_with_prob[k][mot]))*word_weight_NA[mot] #this dictionary cfr comment on line 24\n",
    "                            # word_weight[word] is already \"1/number_of_expert_senses_assigned_to_this_word\"\n",
    "\n",
    "                    #print(\"sense\",expert_senses[s],\"mot\",mot,\"match_weighted\",match_weighted)\n",
    "\n",
    "                            #print(k,s,conf[k,s])\n",
    "\n",
    "\n",
    "                            # To fix? \n",
    "                            # The way the code works is that all matches happen number_of_s times\n",
    "                            # (number_of_s = number of expert senses)\n",
    "                            # easy fix is to divide the match score by number_of_s\n",
    "\n",
    "                    conf[k,s] = conf[k,s] + match_weighted#/4\n",
    "\n",
    "                \n",
    "                \n",
    "            else:    \n",
    " \n",
    "                if mot in dict_of_words[expert_senses[s]]:  # if that word exists in the list of expert words for that sense\n",
    "\n",
    "                    #print(\"s,dict_of_words[expert_senses[s]])\",dict_of_words[expert_senses[s]])\n",
    "                    #print(\"k_words_with_prob[k][mot]\",k_words_with_prob[k][mot])\n",
    "                    #print(\"word_weight[mot]\",word_weight[mot])\n",
    "\n",
    "                    #for word in list_of_all_words:  # this help getting a key for a dictionary later on\n",
    "                     #   if mot == word:\n",
    "                    match_weighted = float((k_words_with_prob[k][mot]))*word_weight[mot] #this dictionary cfr comment on line 24\n",
    "                            # word_weight[word] is already \"1/number_of_expert_senses_assigned_to_this_word\"\n",
    "\n",
    "                    #print(\"sense\",expert_senses[s],\"mot\",mot,\"match_weighted\",match_weighted)\n",
    "\n",
    "                            #print(k,s,conf[k,s])\n",
    "\n",
    "\n",
    "                            # To fix? \n",
    "                            # The way the code works is that all matches happen number_of_s times\n",
    "                            # (number_of_s = number of expert senses)\n",
    "                            # easy fix is to divide the match score by number_of_s\n",
    "\n",
    "                    conf[k,s] = conf[k,s] + match_weighted#/4\n",
    "\n",
    "                    #else: \n",
    "                        #print(word,\"has no match for sense\",expert_senses[s])\n",
    "                        #print(word,word_weight[word],\"match\",k_words_with_prob[k][mot],\"match weighted\",match_weighted)\n",
    "                    #print(\"test1\")\n",
    "                #print(\"test2\")\n",
    "                \n",
    "                    #compteur += 1\n",
    "                \n",
    "        #if (k,s) in conf.keys():\n",
    "        \n",
    "            #conf[k,s] = conf[k,s] # with or without /10\n",
    "            #print(\"k =\",k,\"\\t s =\",s,\"(= expert sense\",expert_senses[s],\")\\t conf[k,s] =\",conf[k,s])\n",
    "            \n",
    "            #print(compteur)\n",
    "            \n",
    "    #print(k_words_with_prob[k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0): 0.08564231738035263, (0, 1): 0.7188287153652391, (0, 2): 0.260705289672544, (0, 3): 0.013853904282115867, (1, 0): 0.40566037735849053, (1, 1): 0.1761006289308176, (1, 2): 0.410377358490566, (1, 3): 0.1349056603773585, (2, 0): 0.15499999999999997, (2, 1): 0.8668749999999997, (2, 2): 0.024999999999999994, (2, 3): 0, (3, 0): 0.06990014265335234, (3, 1): 0.4957203994293865, (3, 2): 0.14407988587731813, (3, 3): 0.36233951497860195, (4, 0): 0.33542538354253826, (4, 1): 0.19200371920037185, (4, 2): 0.48884239888423975, (4, 3): 0.04323570432357042, (5, 0): 0.1430084745762712, (5, 1): 0.48022598870056493, (5, 2): 0.30508474576271183, (5, 3): 0.1260593220338983, (6, 0): 0.11850311850311851, (6, 1): 0.4873527373527373, (6, 2): 0.2806652806652807, (6, 3): 0.2006237006237006, (7, 0): 0.10248112189859762, (7, 1): 0.2337288745055735, (7, 2): 0.7605177993527508, (7, 3): 0, (8, 0): 0.4634502923976607, (8, 1): 0.6296296296296294, (8, 2): 0.045321637426900575, (8, 3): 0, (9, 0): 0.41531322505800466, (9, 1): 0.4137664346481052, (9, 2): 0.18561484918793508, (9, 3): 0.13457076566125292, (10, 0): 0, (10, 1): 0.1613691931540342, (10, 2): 0.33496332518337396, (10, 3): 0.5036674816625915, (11, 0): 0.010224948875255623, (11, 1): 0.9432515337423313, (11, 2): 0.03732106339468302, (11, 3): 0.0270961145194274, (12, 0): 0.1384439359267734, (12, 1): 0.3668954996186116, (12, 2): 0.40503432494279157, (12, 3): 0.2414187643020594, (13, 0): 0.3026819923371648, (13, 1): 0.399425287356322, (13, 2): 0.078544061302682, (13, 3): 0.2145593869731801, (14, 0): 0.6488888888888887, (14, 1): 0.3608333333333333, (14, 2): 0.03555555555555555, (14, 3): 0.03555555555555555}\n",
      "MATCH: k 0 s 1\n",
      "\tmax: 0.7188287153652391\n",
      "\tmax 2 + 3: 0.260705289672544 0.08564231738035263 sum 0.3463476070528967 \n",
      "\n",
      "NO MATCH: k 1 the best s was 2 reason: max < max2+max3\n",
      "\tmax 2 + 3: 0.40566037735849053 0.1761006289308176 sum 0.5817610062893082 \n",
      "\n",
      "MATCH: k 2 s 1\n",
      "\tmax: 0.8668749999999997\n",
      "\tmax 2 + 3: 0.15499999999999997 0.024999999999999994 sum 0.17999999999999997 \n",
      "\n",
      "NO MATCH: k 3 the best s was 1 reason: max < max2+max3\n",
      "\tmax 2 + 3: 0.36233951497860195 0.14407988587731813 sum 0.5064194008559201 \n",
      "\n",
      "NO MATCH: k 4 the best s was 2 reason: max < max2+max3\n",
      "\tmax 2 + 3: 0.33542538354253826 0.19200371920037185 sum 0.5274291027429101 \n",
      "\n",
      "MATCH: k 5 s 1\n",
      "\tmax: 0.48022598870056493\n",
      "\tmax 2 + 3: 0.30508474576271183 0.1430084745762712 sum 0.448093220338983 \n",
      "\n",
      "MATCH: k 6 s 1\n",
      "\tmax: 0.4873527373527373\n",
      "\tmax 2 + 3: 0.2806652806652807 0.2006237006237006 sum 0.4812889812889813 \n",
      "\n",
      "MATCH: k 7 s 2\n",
      "\tmax: 0.7605177993527508\n",
      "\tmax 2 + 3: 0.2337288745055735 0.10248112189859762 sum 0.33620999640417115 \n",
      "\n",
      "MATCH: k 8 s 1\n",
      "\tmax: 0.6296296296296294\n",
      "\tmax 2 + 3: 0.4634502923976607 0.045321637426900575 sum 0.5087719298245613 \n",
      "\n",
      "NO MATCH: k 9 the best s was 0 reason: max < max2+max3\n",
      "\tmax 2 + 3: 0.4137664346481052 0.18561484918793508 sum 0.5993812838360403 \n",
      "\n",
      "MATCH: k 10 s 3\n",
      "\tmax: 0.5036674816625915\n",
      "\tmax 2 + 3: 0.33496332518337396 0.1613691931540342 sum 0.49633251833740816 \n",
      "\n",
      "MATCH: k 11 s 1\n",
      "\tmax: 0.9432515337423313\n",
      "\tmax 2 + 3: 0.03732106339468302 0.0270961145194274 sum 0.06441717791411042 \n",
      "\n",
      "NO MATCH: k 12 the best s was 2 reason: max < max2+max3\n",
      "\tmax 2 + 3: 0.3668954996186116 0.2414187643020594 sum 0.608314263920671 \n",
      "\n",
      "NO MATCH: k 13 the best s was 1 reason: max < max2+max3\n",
      "\tmax 2 + 3: 0.3026819923371648 0.2145593869731801 sum 0.517241379310345 \n",
      "\n",
      "MATCH: k 14 s 0\n",
      "\tmax: 0.6488888888888887\n",
      "\tmax 2 + 3: 0.3608333333333333 0.03555555555555555 sum 0.39638888888888885 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(conf)\n",
    "\n",
    "k_s_match = dict()  # k_s_match[k] = s\n",
    "\n",
    "for k in range(0,number_of_the_k):\n",
    "    #print(\"k\",k)\n",
    "    liste_temp = list()\n",
    "    best_s_for_k = \"NA\"\n",
    "    conf[k,best_s_for_k] = -1\n",
    "\n",
    "    \n",
    "    for s in range(0,len(expert_senses)):\n",
    "        #print(\"conf[k,s]\",conf[k,s],\"s\",s,\"best s for k\",best_s_for_k)\n",
    "        #print(type(conf[k,s]))\n",
    "        liste_temp.append(conf[k,s])\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            if conf[k,s] > conf[k,best_s_for_k]: \n",
    "                #print(\"NEW best_s:\",s)\n",
    "                best_s_for_k = s\n",
    "                \n",
    "        except KeyError:\n",
    "            print(\"key error should not happen\")\n",
    "        \n",
    "    sorted_liste_temp = sorted(liste_temp, reverse=True)\n",
    "    \n",
    "\n",
    "    \n",
    "    #print(\"k, best s:\",k,best_s_for_k)\n",
    "    if conf[k,best_s_for_k] > sorted_liste_temp[1] + sorted_liste_temp[2]: # if the MAX is higher than the sum of the following two\n",
    "        \n",
    "        if conf[k,best_s_for_k] > 1/(len(expert_senses)+1): # if the MAX is higher than the random baseline (number of sense + NA)\n",
    "            k_s_match[k] = best_s_for_k\n",
    "            print(\"MATCH: k\",k,\"s\",best_s_for_k)\n",
    "            print(\"\\tmax:\",sorted_liste_temp[0])\n",
    "            print(\"\\tmax 2 + 3:\",sorted_liste_temp[1],sorted_liste_temp[2],\"sum\",sorted_liste_temp[1]+sorted_liste_temp[2],\"\\n\")\n",
    "            results_file.write(\"MATCH: k %s and s %s \\n\" %(k,best_s_for_k))\n",
    "            \n",
    "        else:\n",
    "            print(\"NO MATCH: k\",k,\"the best s was\",best_s_for_k,\"reason: max < 1/(# of expert senses+1)\")\n",
    "            print(\"\\tmax:\",sorted_liste_temp[0])\n",
    "            print(\"\\tmax 2 + 3:\",sorted_liste_temp[1],sorted_liste_temp[2],\"sum\",sorted_liste_temp[1]+sorted_liste_temp[2],\"\\n\")\n",
    "            k_s_match[k] = \"NA\"\n",
    "    else:\n",
    "        print(\"NO MATCH: k\",k,\"the best s was\",best_s_for_k,\"reason: max < max2+max3\")\n",
    "        print(\"\\tmax 2 + 3:\",sorted_liste_temp[1],sorted_liste_temp[2],\"sum\",sorted_liste_temp[1]+sorted_liste_temp[2],\"\\n\")\n",
    "        k_s_match[k] = \"NA\"\n",
    "        \n",
    "        #if conf[k,best_s_for_k] > 1/(len(expert_senses)+1):\n",
    "         #   print(\"also: reason: max < 1/(# of expert senses+1)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1, 1: 'NA', 2: 1, 3: 'NA', 4: 'NA', 5: 1, 6: 1, 7: 2, 8: 1, 9: 'NA', 10: 3, 11: 1, 12: 'NA', 13: 'NA', 14: 0}\n",
      "for each s, the k (or more) assigned to it: {1: [0, 2, 5, 6, 8, 11], 'NA': [1, 3, 4, 9, 12, 13], 2: [7], 3: [10], 0: [14]}\n"
     ]
    }
   ],
   "source": [
    "# For all k->s pair we have in the k_s_match dictionary, we create the inverted dictionary :\n",
    "# s_k_match[s] contains all the k assigned to that s\n",
    "\n",
    "s_k_match = dict()\n",
    "print(k_s_match)\n",
    "for key in k_s_match.keys():\n",
    "    try: \n",
    "        s_k_match[k_s_match[key]].append(key)\n",
    "        \n",
    "    except KeyError:\n",
    "        s_k_match[k_s_match[key]] = list()\n",
    "        s_k_match[k_s_match[key]].append(key)\n",
    "        \n",
    "print(\"for each s, the k (or more) assigned to it:\",s_k_match)\n",
    "#print(expert_senses[3])        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: How well does the model assign the right words to a given sense of the target word?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For each k, we use the words given by the expert as unquestionable truth.\n",
    "# Judging the model's assignment of words to a given sense becomes a question of precision and recall.\n",
    "\n",
    "# precision is all correct w weighted by their respective probabilities / all w weighted by their probabilities\n",
    "\n",
    "for each k:\n",
    "    for each w:\n",
    "        if w in expert_list:\n",
    "            w_weight = p*1\n",
    "            numerator += w_weight\n",
    "        w_weight = p*1\n",
    "        denominator += w_weight\n",
    "    precision = numerator/denominator\n",
    "    \n",
    "# recall is all correct w weighted by their respective probabilities / all w assigned to the sense by the expert\n",
    "for each k:\n",
    "    for each w:\n",
    "        if w in expert_list:\n",
    "            w_weight = p*1\n",
    "            numerator += w_weight\n",
    "    denominator = len(expert_list)\n",
    "    recall = numerator/denominator\n",
    "    \n",
    "# f-score can be used as well\n",
    "\n",
    "for each k:\n",
    "    f_score = 2 * precision * recall / (precision+recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO : \n",
    "1. ~~create another word_weight[] only for NA~~\n",
    "2. ~~in this word_weight[], a word that is in an NA (either because w or !=1) has its weight 1/senses, BUT THAT DOES NOT AFFECT THE real word_weight[]~~\n",
    "3. when a k,s match is correctly NA: precision and recall cfr picture\n",
    "\n",
    "ALSO:\n",
    "1. ~~create a s_k_match dictionary that maps s with one or more k assigned by the model. This allows to calculate a new P and R~~\n",
    "2. ~~s_k_match[s] = [kx, ky] (based on matches)~~\n",
    "3. ~~precision and recall for each s, and averaged precisions and recalls as well~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is Pr and Re for Ks -> not taken into account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "precision_recall_k = dict()\n",
    "\n",
    "print(\"\\t\\t\\tTHIS IS P and R FOR Ks, WE LOOK AT P and R for Ss now: below\\n\\n\\n\")\n",
    "\n",
    "for key in k_s_match.keys():\n",
    "    precision_recall_k[key] = list() # this list has first the recall then the precision then the f score\n",
    "    numerator_recall = 0\n",
    "    denominator_precision = 0\n",
    "    numerator_precision = 0\n",
    "    denominator_recall = 0\n",
    "   # print()\n",
    "\n",
    "\n",
    "\n",
    "############# NEED TO ADJUST FOR PAIRS THAT ARE NAs  --> actually no\n",
    "\n",
    "\n",
    "    if k_s_match[key] == \"NA\":\n",
    "        print(\"K\",key,\"s is NA\")\n",
    "\n",
    "    else: \n",
    "    \n",
    "        for word in k_words_with_prob[int(key)]: \n",
    "            w_weight_precision = k_words_with_prob[int(key)][word] * 1\n",
    "            denominator_precision += float(w_weight_precision)\n",
    "        \n",
    "        \n",
    "        \n",
    "            if word in dict_of_words[expert_senses[int(k_s_match[key])]]:   \n",
    "                w_weight_recall = k_words_with_prob[int(key)][word] * 1\n",
    "                numerator_recall += float(w_weight_recall)\n",
    "\n",
    "                numerator_precision += float(w_weight_precision)\n",
    "    \n",
    "        for mot in dict_of_words[expert_senses[int(k_s_match[key])]]:\n",
    "            denominator_recall += word_weight[mot]\n",
    "        \n",
    "\n",
    "    #denominator_recall = len(dict_of_words[expert_senses[int(key[2])]])\n",
    "    #numerator_recall = numerator_recall*10\n",
    "     \n",
    "    \n",
    "        #print(\"For pair ks\",key,k_s_match[key],\":\")\n",
    "        recall = numerator_recall*1/denominator_recall\n",
    "        recall = recall*len(dict_of_words[expert_senses[k_s_match[key]]])\n",
    "    \n",
    "        precision_recall_k[key].append(recall)\n",
    "    \n",
    "        #print(len(dict_of_words[expert_senses[k_s_match[key]]]))\n",
    "        #print(\"The RECALL is\",numerator_recall,\"/\",denominator_recall,\"=\",recall) \n",
    "        if numerator_precision == 0:\n",
    "            print(\"The PRECISION IS NA\")\n",
    "        else:\n",
    "            precision = numerator_precision/denominator_precision\n",
    "            print(\"The PRECISION is\",numerator_precision,\"* number of expert words in that sense/\",denominator_precision,\"=\",precision,\"\\n\")\n",
    "            precision_recall_k[key].append(precision)\n",
    "        \n",
    "        if (numerator_precision/denominator_precision)+(numerator_recall/denominator_recall) != 0: \n",
    "            fscore = (2*(precision)*(recall)/((precision)+(recall)))\n",
    "            print(\"The F-SCORE is\", fscore,\"\\n\")\n",
    "            precision_recall_k[key].append(fscore)\n",
    "        \n",
    "        else:\n",
    "            print(\"No F-SCORE, can't divide by 0\\n\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P and R based on S, with adapted word weight for NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [0, 2, 5, 6, 8, 11], 'NA': [1, 3, 4, 9, 12, 13], 2: [7], 3: [10], 0: [14]}\n",
      "['mus-1', 'NA', 'mus-4', 'mus-2']\n"
     ]
    }
   ],
   "source": [
    "print(s_k_match)\n",
    "print(expert_senses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [0, 2, 5, 6, 8, 11], 'NA': [1, 3, 4, 9, 12, 13], 2: [7], 3: [10], 0: [14]}\n",
      "num precision 3.55814716502066 for s 1 and a total of 6 k\n",
      "denom precision 5.999999999999999 for s 1 and a total of 6 k\n",
      "num recall 3.55814716502066 for s 1 and a total of 6 k\n",
      "denom recall 2632.0000000000014 for s 1 and a total of 6 k\n",
      "recall: 0.6732360517402309\n",
      "precision: 0.5930245275034435\n",
      "fscore: 0.6305897822705654\n",
      "\n",
      "\n",
      "num precision 3.55814716502066 for s NA and a total of 6 k\n",
      "denom precision 5.999999999999999 for s NA and a total of 6 k\n",
      "num recall 3.55814716502066 for s NA and a total of 6 k\n",
      "denom recall 2632.0000000000014 for s NA and a total of 6 k\n",
      "recall: 0.6732360517402309\n",
      "precision: 0.5930245275034435\n",
      "fscore: 0.6305897822705654\n",
      "\n",
      "\n",
      "num precision 0.28841309823677574 for s 2 and a total of 1 k\n",
      "denom precision 0.9999999999999997 for s 2 and a total of 1 k\n",
      "num recall 0.28841309823677574 for s 2 and a total of 1 k\n",
      "denom recall 236.66666666666669 for s 2 and a total of 1 k\n",
      "recall: 0.30709901727746824\n",
      "precision: 0.28841309823677586\n",
      "fscore: 0.29746289531651093\n",
      "\n",
      "\n",
      "num precision 0.041561712846347604 for s 3 and a total of 1 k\n",
      "denom precision 0.9999999999999997 for s 3 and a total of 1 k\n",
      "num recall 0.041561712846347604 for s 3 and a total of 1 k\n",
      "denom recall 106.16666666666666 for s 3 and a total of 1 k\n",
      "recall: 0.04423679954446417\n",
      "precision: 0.04156171284634762\n",
      "fscore: 0.04285755332292574\n",
      "\n",
      "\n",
      "num precision 0.11335012594458437 for s 0 and a total of 1 k\n",
      "denom precision 0.9999999999999997 for s 0 and a total of 1 k\n",
      "num recall 0.11335012594458437 for s 0 and a total of 1 k\n",
      "denom recall 249.16666666666669 for s 0 and a total of 1 k\n",
      "recall: 0.11964314297027032\n",
      "precision: 0.1133501259445844\n",
      "fscore: 0.1164116490338784\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### note\n",
    "recall_precision_s = dict()\n",
    "print(s_k_match)\n",
    "#print(dict_of_words)\n",
    "\n",
    "for key in s_k_match.keys():\n",
    "    recall_precision_s[key] = list() # this list has first the recall then the precision then the f score\n",
    "    numerator_recall = 0\n",
    "    denominator_precision = 0\n",
    "    numerator_precision = 0\n",
    "    denominator_recall = 0\n",
    "   # print()\n",
    "    #print(\"s and k:\",key,s_k_match[key])\n",
    "    \n",
    "    for any_k in range(0,len(s_k_match[key])):\n",
    "        #print(\"s\",key,\"k\",s_k_match[key],s_k_match[key][any_k])\n",
    "        #print(\"s_k_match[key][any_k]\",s_k_match[key][any_k])\n",
    "      \n",
    "        for word in k_words_with_prob[any_k]:\n",
    "              \n",
    "            denominator_precision += float(k_words_with_prob[any_k][word])\n",
    "                \n",
    "            #print(word,key)\n",
    "                        \n",
    "            # if key is not NA then do that, OTHERWISE use dict_of_words[key]\n",
    "            \n",
    "            if key != \"NA\":\n",
    "                if word in dict_of_words[expert_senses[key]]:\n",
    "                        #print(\"this word is in k\",any_k,\"and in sense\",expert_senses[key],word)\n",
    "\n",
    "                    numerator_precision += float(k_words_with_prob[any_k][word])\n",
    "                    numerator_recall += float(k_words_with_prob[any_k][word])\n",
    "\n",
    "            elif key == \"NA\":\n",
    "                if word in dict_of_words[key]:\n",
    "                        #print(\"this word is in k\",any_k,\"and in sense\",expert_senses[key],word)\n",
    "\n",
    "                    numerator_precision += float(k_words_with_prob[any_k][word])\n",
    "                    numerator_recall += float(k_words_with_prob[any_k][word])\n",
    "                \n",
    "        ### SINCE WE HAVE the possibility of having two (or more) k for each s, the expert s words should be counted k times\n",
    "        \n",
    "        if key != \"NA\":\n",
    "            if expert_senses[key] != \"NA\":\n",
    "                for mot in dict_of_words[expert_senses[key]]:\n",
    "                    denominator_recall += word_weight[mot]\n",
    "                \n",
    "            if expert_senses[key] == \"NA\":\n",
    "                for mot in dict_of_words[expert_senses[key]]:\n",
    "                    denominator_recall += word_weight_NA[mot]\n",
    "                \n",
    "        if key == \"NA\":\n",
    "            for mot in dict_of_words[key]:\n",
    "                denominator_recall += word_weight_NA[mot]\n",
    "                \n",
    "    #if key != \"NA\":\n",
    "    \n",
    "    \n",
    "    if key != \"NA\":\n",
    "        recall = numerator_recall*1/denominator_recall\n",
    "        recall = recall*len(dict_of_words[expert_senses[key]])\n",
    "        recall_precision_s[key].append(recall)\n",
    "\n",
    "        precision = numerator_precision/denominator_precision\n",
    "        recall_precision_s[key].append(precision)\n",
    "\n",
    "        fscore = (2*(precision)*(recall)/((precision)+(recall)))\n",
    "        recall_precision_s[key].append(fscore)\n",
    "        \n",
    "    if key == \"NA\":\n",
    "        recall = numerator_recall*1/denominator_recall\n",
    "        recall = recall*len(dict_of_words[key])\n",
    "        recall_precision_s[key].append(recall)\n",
    "\n",
    "        precision = numerator_precision/denominator_precision\n",
    "        recall_precision_s[key].append(precision)\n",
    "\n",
    "        fscore = (2*(precision)*(recall)/((precision)+(recall)))\n",
    "        recall_precision_s[key].append(fscore)\n",
    "        \n",
    "        \n",
    "   # else: # this must be changed for NAs\n",
    "        #recall_precision_s[key].append(0)\n",
    "    \n",
    "                    \n",
    "    print(\"num precision\",numerator_precision,\"for s\",key,\"and a total of\",len(s_k_match[key]),\"k\")\n",
    "    print(\"denom precision\",denominator_precision,\"for s\",key,\"and a total of\",len(s_k_match[key]),\"k\")\n",
    "    print(\"num recall\",numerator_recall,\"for s\",key,\"and a total of\",len(s_k_match[key]),\"k\")\n",
    "    print(\"denom recall\",denominator_recall,\"for s\",key,\"and a total of\",len(s_k_match[key]),\"k\")\n",
    "    print(\"recall:\",recall)\n",
    "    print(\"precision:\",precision)\n",
    "    print(\"fscore:\",fscore)\n",
    "    results_file.write(\"SCORES FOR PAIR S= %s <-> {K...K}= %s: P %s R %s F %s \\n\" %(key,s_k_match[key],precision,recall,fscore))\n",
    "        \n",
    "    \n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mus-1', 'NA', 'mus-4', 'mus-2']\n"
     ]
    }
   ],
   "source": [
    "type(expert_senses)\n",
    "print(str(expert_senses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [3.9903682184080425, 0.818639798488665, 1.358564686411625], 1: [], 2: [4.453972267536703, 0.9137500000000001, 1.5164037767286864], 3: [], 4: [], 5: [2.870932065142256, 0.5889830508474577, 0.9774403531975582], 6: [2.9590880879624764, 0.607068607068607, 1.0074540393896128], 7: [0.8798577875017091, 0.8263214670981662, 0.8522496986715056], 8: [3.890959045247702, 0.7982456140350876, 1.3247197415922476], 9: [], 10: [0.5360856398229781, 0.5036674816625917, 0.5193711826117674], 11: [4.684994845825118, 0.9611451942740287, 1.5950579508418758], 12: [], 13: [], 14: [0.759973244147157, 0.72, 0.7394467946631954]}\n",
      "nothing for k 1\n",
      "nothing for k 1\n",
      "nothing for k 1\n",
      "nothing for k 3\n",
      "nothing for k 3\n",
      "nothing for k 3\n",
      "nothing for k 4\n",
      "nothing for k 4\n",
      "nothing for k 4\n",
      "nothing for k 9\n",
      "nothing for k 9\n",
      "nothing for k 9\n",
      "nothing for k 12\n",
      "nothing for k 12\n",
      "nothing for k 12\n",
      "nothing for k 13\n",
      "nothing for k 13\n",
      "nothing for k 13\n",
      "AVERAGED SCORES BASED ON Ks:\n",
      "averaged recall =  1.6684154134396099\n",
      "averaged precision =  0.4491880808983069\n",
      "averaged fscore =  0.6593805482738715\n",
      "\n",
      "\n",
      "{1: [0.6732360517402309, 0.5930245275034435, 0.6305897822705654], 'NA': [0.6732360517402309, 0.5930245275034435, 0.6305897822705654], 2: [0.30709901727746824, 0.28841309823677586, 0.29746289531651093], 3: [0.04423679954446417, 0.04156171284634762, 0.04285755332292574], 0: [0.11964314297027032, 0.1133501259445844, 0.1164116490338784]}\n",
      "\n",
      "\n",
      "\tAVERAGED SCORES BASED ON Ss:\n",
      "averaged recall =  0.45436276581816615\n",
      "averaged precision =  0.4073434980086487\n",
      "averaged fscore =  0.4294779155536115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(precision_recall_k)\n",
    "#total_recall = 0\n",
    "#total_precision = 0\n",
    "#total_fscore = 0\n",
    "#for key in precision_recall_k:\n",
    "#    try:\n",
    "#        total_recall += precision_recall_k[key][0]\n",
    "#    except IndexError:\n",
    "#        print(\"nothing for k\",key)\n",
    "#    try: \n",
    "#        total_precision += precision_recall_k[key][1]\n",
    "#    except IndexError:\n",
    "#        print(\"nothing for k\",key)\n",
    "#        \n",
    "#    try: \n",
    "#        total_fscore += precision_recall_k[key][2]\n",
    "#    except IndexError:\n",
    "#        print(\"nothing for k\",key)\n",
    "#        \n",
    "#        \n",
    "#print(\"AVERAGED SCORES BASED ON Ks:\")    \n",
    "#print(\"averaged recall = \",total_recall/number_of_the_k)\n",
    "#print(\"averaged precision = \",total_precision/number_of_the_k)\n",
    "#print(\"averaged fscore = \",total_fscore/number_of_the_k)\n",
    "#print(\"\\n\")\n",
    "\n",
    "\n",
    "print(recall_precision_s)\n",
    "total_recall = 0\n",
    "total_precision = 0\n",
    "total_fscore = 0\n",
    "for key in recall_precision_s:\n",
    "    try:\n",
    "        total_recall += recall_precision_s[key][0]\n",
    "    except IndexError:\n",
    "        print(\"nothing for s\",key)\n",
    "    try: \n",
    "        total_precision += recall_precision_s[key][1]\n",
    "    except IndexError:\n",
    "        print(\"nothing for s\",key)\n",
    "        \n",
    "    try: \n",
    "        total_fscore += recall_precision_s[key][2]\n",
    "    except IndexError:\n",
    "        print(\"nothing for s\",key)\n",
    "        \n",
    "\n",
    "\n",
    "print(\"\\n\\n\\tAVERAGED SCORES BASED ON Ss:\")    \n",
    "print(\"averaged recall = \",total_recall/len(expert_senses))  # or should I divide by the number of Ks?\n",
    "print(\"averaged precision = \",total_precision/len(expert_senses))\n",
    "print(\"averaged fscore = \",total_fscore/len(expert_senses))\n",
    "results_file.write(\"Averaged scores: P %s R %s F %s\\n\\n\" %(total_precision/len(expert_senses),total_recall/len(expert_senses),total_fscore/len(expert_senses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mus-1', 'NA', 'mus-4', 'mus-2']\n"
     ]
    }
   ],
   "source": [
    "print(expert_senses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in expert_senses:\n",
    "    #print(i,dict_of_words[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qx: Model(s) comparison againstannotated subcorpus (sense importance evolution + sense emergence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for key in slice_years.keys():\n",
    "#    print(key,slice_years[key],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting the number of hits per sense per period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sense_date_amount = dict()\n",
    "\n",
    "for sense in sense_year.keys():\n",
    "   \n",
    "    print(\"Sense:\",sense)\n",
    "    counter = 0\n",
    "    for i in range(0,number_of_slices):\n",
    "        #print(\"period\",i,\"years for that sense in that period\",sense_year[sense])\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(sense_year[sense])\n",
    "        for year in sense_year[sense]:\n",
    "        \n",
    "            if year in slice_years[i]:\n",
    "                counter += 1\n",
    "                #print(sense_year[sense][i])\n",
    "                \n",
    "        sense_date_amount[sense,i] = counter           \n",
    "    print(sense,counter)\n",
    "    \n",
    "print(sense_date_amount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting the relative number of hits per sense per period\n",
    "(for plotting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total period {0: 40, 1: 7, 2: 4, 3: 5, 4: 8, 5: 50, 6: 89, 7: 2}\n",
      "sense date amount {('mus-1', 0): 3, ('mus-1', 1): 4, ('mus-1', 2): 1, ('mus-1', 3): 2, ('mus-1', 4): 2, ('mus-1', 5): 14, ('mus-1', 6): 14, ('mus-1', 7): 0, ('NA', 0): 16, ('NA', 1): 3, ('NA', 2): 3, ('NA', 3): 2, ('NA', 4): 6, ('NA', 5): 15, ('NA', 6): 45, ('NA', 7): 1, ('mus-4', 0): 21, ('mus-4', 1): 0, ('mus-4', 2): 0, ('mus-4', 3): 1, ('mus-4', 4): 0, ('mus-4', 5): 21, ('mus-4', 6): 8, ('mus-4', 7): 1, ('mus-2', 0): 0, ('mus-2', 1): 0, ('mus-2', 2): 0, ('mus-2', 3): 0, ('mus-2', 4): 0, ('mus-2', 5): 0, ('mus-2', 6): 22, ('mus-2', 7): 0}\n",
      "{('mus-1', 0): 0.075, ('mus-1', 1): 0.5714285714285714, ('mus-1', 2): 0.25, ('mus-1', 3): 0.4, ('mus-1', 4): 0.25, ('mus-1', 5): 0.28, ('mus-1', 6): 0.15730337078651685, ('mus-1', 7): 0.0, ('NA', 0): 0.4, ('NA', 1): 0.42857142857142855, ('NA', 2): 0.75, ('NA', 3): 0.4, ('NA', 4): 0.75, ('NA', 5): 0.3, ('NA', 6): 0.5056179775280899, ('NA', 7): 0.5, ('mus-4', 0): 0.525, ('mus-4', 1): 0.0, ('mus-4', 2): 0.0, ('mus-4', 3): 0.2, ('mus-4', 4): 0.0, ('mus-4', 5): 0.42, ('mus-4', 6): 0.0898876404494382, ('mus-4', 7): 0.5, ('mus-2', 0): 0.0, ('mus-2', 1): 0.0, ('mus-2', 2): 0.0, ('mus-2', 3): 0.0, ('mus-2', 4): 0.0, ('mus-2', 5): 0.0, ('mus-2', 6): 0.24719101123595505, ('mus-2', 7): 0.0}\n",
      "{0: [0.075, 0.4, 0.525, 0.0], 1: [0.5714285714285714, 0.42857142857142855, 0.0, 0.0], 2: [0.25, 0.75, 0.0, 0.0], 3: [0.4, 0.4, 0.2, 0.0], 4: [0.25, 0.75, 0.0, 0.0], 5: [0.28, 0.3, 0.42, 0.0], 6: [0.15730337078651685, 0.5056179775280899, 0.0898876404494382, 0.24719101123595505], 7: [0.0, 0.5, 0.5, 0.0]}\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "total_period = dict()\n",
    "\n",
    "sense_period_relative = dict()\n",
    "\n",
    "for i in range(0,number_of_slices):\n",
    "    for entry in expert_senses:\n",
    "        \n",
    "# for period i we store for each sense the number of times the sense is seen\n",
    "        \n",
    "        try:\n",
    "            total_period[i] += sense_date_amount[entry,i]\n",
    "        except KeyError:\n",
    "            total_period[i] = 0\n",
    "            total_period[i] += sense_date_amount[entry,i]\n",
    "            \n",
    "        #print(i,entry,\"+\",sense_date_amount[entry,i],\"=\",total_period[i])\n",
    "        \n",
    "        \n",
    "print(\"total period\",total_period)\n",
    "print(\"sense date amount\",sense_date_amount)\n",
    "        \n",
    "for key in sense_date_amount:\n",
    "    \n",
    "    # for each (sense,period) pair we divide the number by the total number of words at that period\n",
    "    \n",
    "    #print(key,\"total for this sense at this period\",sense_date_amount[key],\"total period\",total_period[key[1]])\n",
    "    \n",
    "    if total_period[key[1]] != 0:\n",
    "        sense_period_relative[key] = float(sense_date_amount[key]/total_period[key[1]])\n",
    "    else:\n",
    "        sense_period_relative[key] = 0\n",
    "    #print(\"relative\",sense_period_relative[key])\n",
    "  \n",
    "print(sense_period_relative)        \n",
    "\n",
    "period_relative = dict()\n",
    "temp_list = list()\n",
    "\n",
    "for i in range(0,number_of_slices):\n",
    "    temp_list = list()\n",
    "    for entry in expert_senses:\n",
    "        if len(temp_list) < len(expert_senses):\n",
    "            temp_list.append(sense_period_relative[entry,i])\n",
    "            #print(entry,i,sense_period_relative[entry,i])\n",
    "        \n",
    "    period_relative[i] = temp_list\n",
    "        \n",
    "        \n",
    "print(period_relative)\n",
    "print(number_of_slices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0 [0.075, 0.4, 0.525, 0.0]\n",
      "len vals 4\n",
      "\n",
      " 1 [0.5714285714285714, 0.42857142857142855, 0.0, 0.0]\n",
      "len vals 4\n",
      "\n",
      " 2 [0.25, 0.75, 0.0, 0.0]\n",
      "len vals 4\n",
      "\n",
      " 3 [0.4, 0.4, 0.2, 0.0]\n",
      "len vals 4\n",
      "\n",
      " 4 [0.25, 0.75, 0.0, 0.0]\n",
      "len vals 4\n",
      "\n",
      " 5 [0.28, 0.3, 0.42, 0.0]\n",
      "len vals 4\n",
      "\n",
      " 6 [0.15730337078651685, 0.5056179775280899, 0.0898876404494382, 0.24719101123595505]\n",
      "len vals 4\n",
      "\n",
      " 7 [0.0, 0.5, 0.5, 0.0]\n",
      "len vals 4\n"
     ]
    }
   ],
   "source": [
    "for key,vals in period_relative.items():\n",
    "    print(\"\\n\",key,vals)\n",
    "    print(\"len vals\",len(vals))\n",
    "\n",
    "#for key in valeurs.keys():\n",
    "#    print(\"period\",key,valeurs[key])\n",
    "#    print(type(key))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "liste_number_year = list() # creating a list because matplotlib wants a tuple\n",
    "for key in sense_date_amount.keys():\n",
    "    #print(key)\n",
    "    liste_number_year.append(sense_date_amount[key])\n",
    "    \n",
    "tuple_number_year = tuple(liste_number_year)\n",
    "#print(tuple_number_year)\n",
    "\n",
    "period_number = dict()\n",
    "\n",
    "for key in sense_date_amount.keys():\n",
    "    compteur = 0\n",
    "    if key[1] in range(0,number_of_slices):\n",
    "        print(key,sense_date_amount[key[0],key[1]])\n",
    "        compteur += sense_date_amount[key[0],key[1]]\n",
    "        \n",
    "        try :\n",
    "            period_number[key[1]] += compteur\n",
    "        except KeyError:\n",
    "            period_number[key[1]] = 0\n",
    "            period_number[key[1]] += compteur\n",
    "            \n",
    "        \n",
    "for entry in period_number:\n",
    "    print(\"priode\",entry,\"number of uses\",period_number[entry])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading model output for plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lines_output_plot = output_senses.split(\"===============  per time  ===============\")[1].split(\"\\n\")\n",
    "period_relative_model = dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in range(0,len(lines_output_plot)):\n",
    "    if lines_output_plot[i][0:5] == \"Time=\":  # if a line starts with \"time\" we take it into account\n",
    "        for x in range(i,i+number_of_the_k+1): # for every \"number of  the k\" lines that follow\n",
    "            #print(lines_output_plot[x])\n",
    "            if lines_output_plot[x][0:5] == \"Time=\": # if a line starts with \"time\" we take the value for the slice\n",
    "                period = lines_output_plot[x][5:6]\n",
    "                templist = list()\n",
    "                \n",
    "            if lines_output_plot[x][0:5] != \"Time=\":  # if a line doesn't start with \"time\" but is considered(cf line3)\n",
    "                ligne = re.split(\"\\s{3,}\",lines_output_plot[x]) # we take the first part of the line (importance of that K)\n",
    "                templist.append(float(ligne[0]))\n",
    "            #print(period,templist)\n",
    "            \n",
    "        period_relative_model[str(period)] = templist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading model output (period_relative_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_output_plot = output_senses.split(\"===============  per time  ===============\")[1].split(\"\\n\")\n",
    "period_relative_model = dict()\n",
    "\n",
    "\n",
    "for i in range(0,len(lines_output_plot)):\n",
    "    if lines_output_plot[i][0:5] == \"Time=\":  # if a line starts with \"time\" we take it into account\n",
    "        for x in range(i,i+number_of_the_k+1): # for every \"number of  the k\" lines that follow\n",
    "            #print(i,x)\n",
    "            #print(lines_output_plot[x])\n",
    "            if lines_output_plot[x][0:5] == \"Time=\": # if a line starts with \"time\" we take the value for the slice\n",
    "                #print(lines_output_plot[x][0:15])\n",
    "                string = lines_output_plot[x][0:15].replace(\"Time=\",\"\")\n",
    "                string = string.replace(\"  \",\"\")\n",
    "                period = string\n",
    "                templist = list()\n",
    "                \n",
    "            if lines_output_plot[x][0:5] != \"Time=\":  # if a line doesn't start with \"time\" but is considered(cf line3)\n",
    "                ligne = re.split(\"\\s{3,}\",lines_output_plot[x]) # we take the first part of the line (importance of that K)\n",
    "                \n",
    "                \n",
    "                templist.append(float(ligne[0]))\n",
    "            \n",
    "            #print(period,templist)\n",
    "            \n",
    "        period_relative_model[str(period)] = templist\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plotting model output (draft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best pair: the one with the maximum above a certain threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO post meeting with B and V\n",
    "1. plot distribution of senses across genres\n",
    "2. plot distribution of genres across time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO post 10/4 3pm:\n",
    "\n",
    "- ~~when parsing senses_3874965.txt, if mus-x is followed by something else than \"1\", mark the sense as \"NA\"~~\n",
    "- ~~when parsing senses_3874965.txt, add a parameter for window size~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mus-1', 'NA', 'mus-4', 'mus-2']\n",
      "{0: [0.075, 0.4, 0.525, 0.0], 1: [0.5714285714285714, 0.42857142857142855, 0.0, 0.0], 2: [0.25, 0.75, 0.0, 0.0], 3: [0.4, 0.4, 0.2, 0.0], 4: [0.25, 0.75, 0.0, 0.0], 5: [0.28, 0.3, 0.42, 0.0], 6: [0.15730337078651685, 0.5056179775280899, 0.0898876404494382, 0.24719101123595505], 7: [0.0, 0.5, 0.5, 0.0]} \n",
      "\n",
      "{'0': [0.005835990749893935, 0.00973748393420663, 0.00011145256360933004, 0.0008432249710684154, 0.07961183913963674, 0.09540291409853277, 1.8873777004895453e-05, 0.0018326841351719914, 0.046146614850083756, 0.06483784091567095, 0.4598627585314245, 3.767136186597287e-07, 0.220853725928918, 0.011840925100948625, 0.003063294590210846], '1': [0.023851139520140844, 0.005099950883716106, 0.0015513543071630602, 0.0020838282839452968, 0.2560179191683854, 0.07549316720881327, 0.00046274791581247263, 0.0007686401783428464, 0.22486041448388924, 0.09355769340173178, 0.27279757691741496, 9.81509225164446e-07, 0.029856580922117643, 0.003792137417774484, 0.009805867881527517], '2': [0.027277131478797602, 0.016170455833592095, 0.004592938160457079, 0.001568446593433365, 0.023909710973675533, 0.09645716599935711, 5.908262940025764e-05, 0.0016363271236620525, 0.26374890224691877, 0.26637568386937016, 0.08226454433945193, 9.796921687929983e-07, 0.008789114726369725, 0.002176741021029699, 0.2049727753123158], '3': [0.00898077271193721, 0.22251823819167468, 0.0042155300199189895, 0.003103755089620816, 0.0024120058198723823, 0.12009547117251007, 5.170991049115985e-06, 0.001975024472088859, 0.3739180051737028, 0.1326983539470614, 0.07824180602853661, 2.6640078335133093e-06, 0.037237506982722615, 0.0051578036969024115, 0.009437891694568375], '4': [0.004551856165508848, 0.5953243644714058, 0.06689114084070781, 0.010867077242010147, 0.0029994746852452132, 0.06573759663882746, 6.559005183380266e-06, 0.006611759389002713, 0.04694086283697128, 0.04655691293257393, 0.09973357643065342, 9.832129770788233e-07, 0.01251885604075259, 0.0017250341954916648, 0.03953394591268872], '5': [0.03271662068914859, 0.1594781835828137, 0.052398780297810275, 0.04865924836939091, 0.009092559354187036, 0.18777006841027583, 1.4785163888638214e-06, 0.010570950508187491, 0.127935177135233, 0.10802258529075844, 0.03825175529561393, 9.065848821732702e-08, 0.032684054434559476, 0.09810518627226791, 0.09431326118487644], '6': [0.012770235173226481, 0.22900650041693896, 0.024352037644995297, 0.040350846049218785, 0.0012412722220219588, 0.08151368649476916, 3.0851615346386736e-07, 0.01607057420083549, 0.06917007241843798, 0.052148025052477176, 0.01584654308537433, 1.7076314336593398e-08, 0.11609882904869734, 0.3247410806394698, 0.016689971961069455], '7': [0.008449499918757486, 0.1447603648023397, 0.011254126007544335, 0.19455410815253552, 0.00025018041490535966, 0.06703111126576994, 9.524239096312217e-08, 0.0007182913392576578, 0.07813329538422913, 0.03730255780276838, 0.020302645106294648, 2.908401077901487e-09, 0.038336704250315254, 0.37145451654374584, 0.027452500860744692]}\n"
     ]
    }
   ],
   "source": [
    "print(expert_senses)\n",
    "print(period_relative,\"\\n\")\n",
    "print(period_relative_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mus-1', 'NA', 'mus-4', 'mus-2']\n",
      "period relative: 8\n",
      "period relative model: 8 \n",
      "\n",
      "period 0 \n",
      "S [0.075, 0.4, 0.525, 0.0] \n",
      "K [0.005835990749893935, 0.00973748393420663, 0.00011145256360933004, 0.0008432249710684154, 0.07961183913963674, 0.09540291409853277, 1.8873777004895453e-05, 0.0018326841351719914, 0.046146614850083756, 0.06483784091567095, 0.4598627585314245, 3.767136186597287e-07, 0.220853725928918, 0.011840925100948625, 0.003063294590210846] \n",
      " 4 15 \n",
      "\n",
      "period 1 \n",
      "S [0.5714285714285714, 0.42857142857142855, 0.0, 0.0] \n",
      "K [0.023851139520140844, 0.005099950883716106, 0.0015513543071630602, 0.0020838282839452968, 0.2560179191683854, 0.07549316720881327, 0.00046274791581247263, 0.0007686401783428464, 0.22486041448388924, 0.09355769340173178, 0.27279757691741496, 9.81509225164446e-07, 0.029856580922117643, 0.003792137417774484, 0.009805867881527517] \n",
      " 4 15 \n",
      "\n",
      "period 2 \n",
      "S [0.25, 0.75, 0.0, 0.0] \n",
      "K [0.027277131478797602, 0.016170455833592095, 0.004592938160457079, 0.001568446593433365, 0.023909710973675533, 0.09645716599935711, 5.908262940025764e-05, 0.0016363271236620525, 0.26374890224691877, 0.26637568386937016, 0.08226454433945193, 9.796921687929983e-07, 0.008789114726369725, 0.002176741021029699, 0.2049727753123158] \n",
      " 4 15 \n",
      "\n",
      "period 3 \n",
      "S [0.4, 0.4, 0.2, 0.0] \n",
      "K [0.00898077271193721, 0.22251823819167468, 0.0042155300199189895, 0.003103755089620816, 0.0024120058198723823, 0.12009547117251007, 5.170991049115985e-06, 0.001975024472088859, 0.3739180051737028, 0.1326983539470614, 0.07824180602853661, 2.6640078335133093e-06, 0.037237506982722615, 0.0051578036969024115, 0.009437891694568375] \n",
      " 4 15 \n",
      "\n",
      "period 4 \n",
      "S [0.25, 0.75, 0.0, 0.0] \n",
      "K [0.004551856165508848, 0.5953243644714058, 0.06689114084070781, 0.010867077242010147, 0.0029994746852452132, 0.06573759663882746, 6.559005183380266e-06, 0.006611759389002713, 0.04694086283697128, 0.04655691293257393, 0.09973357643065342, 9.832129770788233e-07, 0.01251885604075259, 0.0017250341954916648, 0.03953394591268872] \n",
      " 4 15 \n",
      "\n",
      "period 5 \n",
      "S [0.28, 0.3, 0.42, 0.0] \n",
      "K [0.03271662068914859, 0.1594781835828137, 0.052398780297810275, 0.04865924836939091, 0.009092559354187036, 0.18777006841027583, 1.4785163888638214e-06, 0.010570950508187491, 0.127935177135233, 0.10802258529075844, 0.03825175529561393, 9.065848821732702e-08, 0.032684054434559476, 0.09810518627226791, 0.09431326118487644] \n",
      " 4 15 \n",
      "\n",
      "period 6 \n",
      "S [0.15730337078651685, 0.5056179775280899, 0.0898876404494382, 0.24719101123595505] \n",
      "K [0.012770235173226481, 0.22900650041693896, 0.024352037644995297, 0.040350846049218785, 0.0012412722220219588, 0.08151368649476916, 3.0851615346386736e-07, 0.01607057420083549, 0.06917007241843798, 0.052148025052477176, 0.01584654308537433, 1.7076314336593398e-08, 0.11609882904869734, 0.3247410806394698, 0.016689971961069455] \n",
      " 4 15 \n",
      "\n",
      "period 7 \n",
      "S [0.0, 0.5, 0.5, 0.0] \n",
      "K [0.008449499918757486, 0.1447603648023397, 0.011254126007544335, 0.19455410815253552, 0.00025018041490535966, 0.06703111126576994, 9.524239096312217e-08, 0.0007182913392576578, 0.07813329538422913, 0.03730255780276838, 0.020302645106294648, 2.908401077901487e-09, 0.038336704250315254, 0.37145451654374584, 0.027452500860744692] \n",
      " 4 15 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print(period_relative_model) # period_relative_model[PERIOD] = [list of probabilities for Ks]\n",
    "period_relative # period_relative[PERIOD] = [list of relative frequencies for Ss]\n",
    "\n",
    "print(expert_senses)\n",
    "print(\"period relative:\",len(period_relative))\n",
    "print(\"period relative model:\",len(period_relative_model),\"\\n\") \n",
    "\n",
    "for i in range(0,len(period_relative)):\n",
    "    print(\"period\",i,\"\\nS\",period_relative[i],\"\\nK\",period_relative_model[str(i)],\"\\n\",len(period_relative[i]),len(period_relative_model[str(i)]),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of periods for model: 8\n",
      "number of periods for expert: 8\n"
     ]
    }
   ],
   "source": [
    "print(\"number of periods for model:\",len(period_relative_model))\n",
    "print(\"number of periods for expert:\",len(period_relative))\n",
    "#period_relative_model[str(0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_k_match: {1: [0, 2, 5, 6, 8, 11], 'NA': [1, 3, 4, 9, 12, 13], 2: [7], 3: [10], 0: [14]}\n"
     ]
    }
   ],
   "source": [
    "print(\"s_k_match:\",s_k_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for key in s_k_match.keys():\n",
    "    #print(\"looking at S\",key,\"with Ks\",s_k_match[key])\n",
    "    correl_pairs[key] = list()\n",
    "    temp_list_s = list()\n",
    "    temp_list_k = list()\n",
    "    \n",
    "    for i in range(0,len(period_relative)):  # for every period\n",
    "        #print(\"KEY LOOKED AT \",key)\n",
    "        if (key != \"NA\" and key !=NA_key):         \n",
    "            #print(\"we have\",len(s_k_match[key]),\"Ks for S\",key)\n",
    "            #print(\"period\",i,\"freq\",period_relative[i][key])\n",
    "            #print(\"vs\")\n",
    "            #print(\"period\",i,\"freq\",period_relative[i][key])\n",
    "            #print(\"TEMP LIST S is appended\",temp_list_s.append(period_relative[i][key]))\n",
    "            temp_score_k = 0\n",
    "\n",
    "            \n",
    "            for k in s_k_match[key]:\n",
    "                #print(\"K:\",k,\"period\",i)\n",
    "                #print(\"priode\",i,\"prob\",period_relative_model[str(i)][k])\n",
    "                temp_score_k += period_relative_model[str(i)][k]\n",
    "                \n",
    "            #print(\"total score for Ks\",temp_score_k,\"\\n\")\n",
    "            #print(\"TEMP LIST K is appended\")\n",
    "            temp_list_k.append(temp_score_k)\n",
    "            \n",
    "    #print(\"TEMP LIST S=\",key,\"at PERIOD\",i,temp_list_s)\n",
    "    #print(\"TEMP LIST K for S=\",key,\"at PERIOD\",i,temp_list_k)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NA sense is at index 1\n",
      "NA_counter 12\n",
      "{1: [[0.4, 0.42857142857142855, 0.75, 0.4, 0.75, 0.3, 0.5056179775280899, 0.5], [0.024586037125457223, 0.05436996749084067, 0.06535603336784994, 0.0845362690128253, 0.03068816645002931, 0.0668037026178908, 0.03130105955398278, 0.02747802178784882]], 'NA': [[0.4, 0.42857142857142855, 0.75, 0.4, 0.75, 0.3, 0.5056179775280899, 0.5], [0.06462083999840823, 0.06506801834627846, 0.053165025502911765, 0.06718794395464238, 0.11166528659457987, 0.07600696955066293, 0.12726442557147066, 0.13110973866110168]], 2: [[0.525, 0.0, 0.0, 0.2, 0.0, 0.42, 0.0898876404494382, 0.5], [0.0018326841351719914, 0.0007686401783428464, 0.0016363271236620525, 0.001975024472088859, 0.006611759389002713, 0.010570950508187491, 0.01607057420083549, 0.0007182913392576578]], 3: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24719101123595505, 0.0], [0.4598627585314245, 0.27279757691741496, 0.08226454433945193, 0.07824180602853661, 0.09973357643065342, 0.03825175529561393, 0.01584654308537433, 0.020302645106294648]], 0: [[0.075, 0.5714285714285714, 0.25, 0.4, 0.25, 0.28, 0.15730337078651685, 0.0], [0.003063294590210846, 0.009805867881527517, 0.2049727753123158, 0.009437891694568375, 0.03953394591268872, 0.09431326118487644, 0.016689971961069455, 0.027452500860744692]]}\n",
      "{'NA': [[0.4, 0.42857142857142855, 0.75, 0.4, 0.75, 0.3, 0.5056179775280899, 0.5], [0.044603438561932725, 0.05971899291855956, 0.05926052943538085, 0.07586210648373384, 0.07117672652230458, 0.07140533608427686, 0.07928274256272672, 0.07929388022447525]], 2: [[0.525, 0.0, 0.0, 0.2, 0.0, 0.42, 0.0898876404494382, 0.5], [0.0018326841351719914, 0.0007686401783428464, 0.0016363271236620525, 0.001975024472088859, 0.006611759389002713, 0.010570950508187491, 0.01607057420083549, 0.0007182913392576578]], 3: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24719101123595505, 0.0], [0.4598627585314245, 0.27279757691741496, 0.08226454433945193, 0.07824180602853661, 0.09973357643065342, 0.03825175529561393, 0.01584654308537433, 0.020302645106294648]], 0: [[0.075, 0.5714285714285714, 0.25, 0.4, 0.25, 0.28, 0.15730337078651685, 0.0], [0.003063294590210846, 0.009805867881527517, 0.2049727753123158, 0.009437891694568375, 0.03953394591268872, 0.09431326118487644, 0.016689971961069455, 0.027452500860744692]]}\n"
     ]
    }
   ],
   "source": [
    "correl_pairs = dict() # correl_pairs[s] = list(  liste_freq_s_for_each_t , liste_probs_k_for_each_t )\n",
    "\n",
    "NA_key = expert_senses.index(\"NA\") #get index of the expert sense for sense NA, so we can use it in period_relative\n",
    "print(\"NA sense is at index\",NA_key)\n",
    "\n",
    "\n",
    "NA_counter = 0\n",
    "for key in s_k_match.keys():\n",
    "    if key == \"NA\":\n",
    "        NA_counter += len(s_k_match[key])\n",
    "        #print(\"NA\",NA_counter)\n",
    "    if key == NA_key:\n",
    "        NA_counter += len(s_k_match[key])\n",
    "        #print(\"NA_key\",NA_counter)\n",
    "        \n",
    "print(\"NA_counter\",NA_counter)\n",
    "\n",
    "for key in s_k_match.keys():\n",
    "    #print(\"looking at S\",key,\"with Ks\",s_k_match[key])\n",
    "    correl_pairs[key] = list()\n",
    "    temp_list_s = list()\n",
    "    temp_list_k = list()\n",
    "    \n",
    "    for i in range(0,len(period_relative)):  # for every period\n",
    "        \n",
    "        if (key != \"NA\" and key != NA_key):         \n",
    "            #print(\"we have\",len(s_k_match[key]),\"Ks for S\",key)\n",
    "            #print(\"period\",i,\"freq\",period_relative[i][key])\n",
    "            temp_list_s.append(period_relative[i][key])  # temp_list_s is the list of relative freq of expert sense key for each period i\n",
    "            temp_score_k = 0\n",
    "\n",
    "            \n",
    "            for k in s_k_match[key]:\n",
    "                #print(\"K:\",k)\n",
    "                #print(\"priode\",i,\"prob\",period_relative_model[str(i)][k])\n",
    "                temp_score_k += period_relative_model[str(i)][k]\n",
    "                \n",
    "            #print(\"total score for Ks\",temp_score_k,\"\\n\")\n",
    "            temp_list_k.append(temp_score_k/len(s_k_match[key])) # temp_list_s is the list of probabilities of all k matching expert sense key for each period i\n",
    "            # normalising to account for more than 1 k matching the same key\n",
    "\n",
    "            \n",
    "        if key == \"NA\":  # this is when a k cannot be matched to any expert sense\n",
    "                        # In this case we take the frequencies that we have, i.e. the \"true NA\" from the annotation\n",
    "            \n",
    "            #print(\"period\",i,\"freq\",period_relative[i][NA_key])  # we need the frequency from the annotation (\"true\" NA sense)\n",
    "            temp_list_s.append(period_relative[i][NA_key])\n",
    "            #print(\"PERIOD RELATIVE FOR S = NA WITH PERIOD\",i,period_relative[i][NA_key])\n",
    "            temp_score_k = 0\n",
    "                \n",
    "            for k in s_k_match[key]:\n",
    "                #print(\"K:\",k)\n",
    "                #print(\"priode\",i,\"prob\",period_relative_model[str(i)][k])\n",
    "                temp_score_k += period_relative_model[str(i)][k]\n",
    "                \n",
    "            temp_list_k.append(temp_score_k/len(s_k_match[key]))\n",
    "            # normalising to account for more than 1 k matching the same key\n",
    "\n",
    "        if key == NA_key:  # these are the true NA senses which correspond to when a K has been matched to the expert sense \"NA\"\n",
    "                            # an expert sense NA is where the expert annotated non-collocates or non-1\n",
    "            #print(\"we have\",len(s_k_match[key]),\"Ks for S NA_no_k\")\n",
    "            temp_score_k = 0\n",
    "            temp_list_s.append(period_relative[i][NA_key])\n",
    "            \n",
    "            for k in s_k_match[key]:\n",
    "                #print(\"K:\",k)\n",
    "                #print(\"priode\",i,\"prob\",period_relative_model[str(i)][k])\n",
    "                temp_score_k += period_relative_model[str(i)][k]\n",
    "\n",
    "            #print(\"total score for Ks\",temp_score_k,\"\\n\")\n",
    "            temp_list_k.append(temp_score_k/len(s_k_match[key]))\n",
    "            # normalising to account for more than 1 k matching the same key\n",
    "    \n",
    "    #print(\"\\n\\tS\",key,temp_list_s)\n",
    "    #print(\"\\n\\tconsolidated_K for S\",temp_list_k)\n",
    "    correl_pairs[key].append(temp_list_s)\n",
    "    correl_pairs[key].append(temp_list_k)\n",
    "\n",
    "#print(\"NA0\",correl_pairs[\"NA\"][0])\n",
    "#print(\"NA1\",correl_pairs[\"NA\"][1])\n",
    "#print(\"NA_key0\",correl_pairs[NA_key][0])\n",
    "#print(\"NA_key1\",correl_pairs[NA_key][1],\"\\n\")\n",
    "\n",
    "print(correl_pairs)\n",
    "#for key in correl_pairs:\n",
    "#    print(key)\n",
    "#    print(\"LOOK HERE\",key,correl_pairs[key],\"\\n\")\n",
    "    \n",
    "    \n",
    "#print(correl_pairs[NA_key])\n",
    "    \n",
    "# Now we normalise the frequency lists and probability lists IF we have the two types of NA \n",
    "\n",
    "if ((NA_key and \"NA\") in s_k_match.keys()) and NA_key in correl_pairs.keys():\n",
    "    \n",
    "    \n",
    "    # and ((NA_key and \"NA\") in correl_pairs.keys())):\n",
    "    for i in range(0,len(correl_pairs[\"NA\"][0])):  # first item is list of frequencies, second is list of probabilities \n",
    "        # we replace the frequencies by the sum of them / 2\n",
    "        correl_pairs[\"NA\"][0][i] = (correl_pairs[NA_key][0][i] + correl_pairs[\"NA\"][0][i])/2\n",
    "        # we replace the probabilities by the sum of them / 2\n",
    "        correl_pairs[\"NA\"][1][i] = (correl_pairs[NA_key][1][i] + correl_pairs[\"NA\"][1][i])/2\n",
    "        \n",
    "    # we delete one of the two entries\n",
    "    del correl_pairs[NA_key]\n",
    "\n",
    "\n",
    "#correl_pairs[\"NA\"][1] = correl_pairs[NA_key][1]\n",
    "\n",
    "#del correl_pairs[NA_key]\n",
    "\n",
    "# if we have several Ks for sense NA\n",
    "#for item in range(0,len(correl_pairs[\"NA\"][1])):\n",
    "#    print(\"ancien item\",correl_pairs[\"NA\"][1][item])\n",
    "#    print(\"new item\", correl_pairs[\"NA\"][1][item]/NA_counter,\"\\n\")\n",
    "#    correl_pairs[\"NA\"][1][item] = correl_pairs[\"NA\"][1][item]/NA_counter\n",
    "    \n",
    "#normalising probabilities\n",
    "#for item in range(0,len(correl_pairs[\"NA\"][1])):\n",
    "    \n",
    "\n",
    "# if we have several Ks for any other sense\n",
    "#for key in s_k_match.keys():\n",
    "#    if (key != \"NA\" and key != NA_key):\n",
    "#        print(\"SENSE\",key)\n",
    "#        for item in range(0,len(correl_pairs[key][1])):\n",
    "#            print(\"ancien item non NA\",correl_pairs[key][1][item])\n",
    "#            print(\"new item non NA\", correl_pairs[key][1][item]/len(s_k_match[key]),\"\\n\")\n",
    "#            correl_pairs[key][1][item] = correl_pairs[key][1][item]/len(s_k_match[key])\n",
    "\n",
    "    \n",
    "print(correl_pairs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s_k_match: {1: [0, 2, 5, 6, 8, 11], 'NA': [1, 3, 4, 9, 12, 13], 2: [7], 3: [10], 0: [14]}\n",
      "\n",
      "for each sense in correl pairs:\n",
      "\n",
      "SENSE NA\n",
      "[0.4, 0.42857142857142855, 0.75, 0.4, 0.75, 0.3, 0.5056179775280899, 0.5]\n",
      "[0.044603438561932725, 0.05971899291855956, 0.05926052943538085, 0.07586210648373384, 0.07117672652230458, 0.07140533608427686, 0.07928274256272672, 0.07929388022447525]\n",
      "\n",
      "SENSE 2\n",
      "[0.525, 0.0, 0.0, 0.2, 0.0, 0.42, 0.0898876404494382, 0.5]\n",
      "[0.0018326841351719914, 0.0007686401783428464, 0.0016363271236620525, 0.001975024472088859, 0.006611759389002713, 0.010570950508187491, 0.01607057420083549, 0.0007182913392576578]\n",
      "\n",
      "SENSE 3\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24719101123595505, 0.0]\n",
      "[0.4598627585314245, 0.27279757691741496, 0.08226454433945193, 0.07824180602853661, 0.09973357643065342, 0.03825175529561393, 0.01584654308537433, 0.020302645106294648]\n",
      "\n",
      "SENSE 0\n",
      "[0.075, 0.5714285714285714, 0.25, 0.4, 0.25, 0.28, 0.15730337078651685, 0.0]\n",
      "[0.003063294590210846, 0.009805867881527517, 0.2049727753123158, 0.009437891694568375, 0.03953394591268872, 0.09431326118487644, 0.016689971961069455, 0.027452500860744692]\n"
     ]
    }
   ],
   "source": [
    "print(\"s_k_match:\",s_k_match)\n",
    "\n",
    "\n",
    "print(\"\\nfor each sense in correl pairs:\")\n",
    "for key in correl_pairs.keys():\n",
    "    print(\"\\nSENSE\",key)\n",
    "    for item in correl_pairs[key]:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S NA freq then prob [0.4, 0.42857142857142855, 0.75, 0.4, 0.75, 0.3, 0.5056179775280899, 0.5]\n",
      "S NA freq then prob [0.044603438561932725, 0.05971899291855956, 0.05926052943538085, 0.07586210648373384, 0.07117672652230458, 0.07140533608427686, 0.07928274256272672, 0.07929388022447525]\n",
      "\n",
      "correlation: -0.012049067317796999\n",
      "p-value: 0.9774101852887366\n",
      "\n",
      "\n",
      "S 2 freq then prob [0.525, 0.0, 0.0, 0.2, 0.0, 0.42, 0.0898876404494382, 0.5]\n",
      "S 2 freq then prob [0.0018326841351719914, 0.0007686401783428464, 0.0016363271236620525, 0.001975024472088859, 0.006611759389002713, 0.010570950508187491, 0.01607057420083549, 0.0007182913392576578]\n",
      "\n",
      "correlation: -0.048795003647426664\n",
      "p-value: 0.9086544876553411\n",
      "\n",
      "\n",
      "S 3 freq then prob [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24719101123595505, 0.0]\n",
      "S 3 freq then prob [0.4598627585314245, 0.27279757691741496, 0.08226454433945193, 0.07824180602853661, 0.09973357643065342, 0.03825175529561393, 0.01584654308537433, 0.020302645106294648]\n",
      "\n",
      "correlation: -0.5773502691896258\n",
      "p-value: 0.13397459621556113\n",
      "\n",
      "\n",
      "S 0 freq then prob [0.075, 0.5714285714285714, 0.25, 0.4, 0.25, 0.28, 0.15730337078651685, 0.0]\n",
      "S 0 freq then prob [0.003063294590210846, 0.009805867881527517, 0.2049727753123158, 0.009437891694568375, 0.03953394591268872, 0.09431326118487644, 0.016689971961069455, 0.027452500860744692]\n",
      "\n",
      "correlation: 0.0\n",
      "p-value: 1.0\n",
      "\n",
      "\n",
      "significant correlations: []\n",
      "significant correlations that are positive: 0\n",
      "above divided by number of s-k matches: 0.0\n",
      "sk matches for reference: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats\n",
    "\n",
    "list_significant_correl = list()\n",
    "\n",
    "for key in correl_pairs.keys():\n",
    "    for item in correl_pairs[key]:\n",
    "        print(\"S\",key,\"freq then prob\",item)\n",
    "    print(\"\\ncorrelation:\",scipy.stats.stats.spearmanr(correl_pairs[key][0],correl_pairs[key][1])[0])\n",
    "    print(\"p-value:\",scipy.stats.stats.spearmanr(correl_pairs[key][0],correl_pairs[key][1])[1])\n",
    "    \n",
    "    if scipy.stats.stats.spearmanr(correl_pairs[key][0],correl_pairs[key][1])[1] <= 0.05:\n",
    "        list_significant_correl.append(scipy.stats.stats.spearmanr(correl_pairs[key][0],correl_pairs[key][1])[0])\n",
    "\n",
    "    print(\"\\n\")\n",
    "    \n",
    "positive_significant_correl = list()\n",
    "print(\"significant correlations:\",list_significant_correl)\n",
    "\n",
    "for i in list_significant_correl:\n",
    "    if i > 0:\n",
    "        positive_significant_correl.append(i)\n",
    "        \n",
    "print(\"significant correlations that are positive:\",len(positive_significant_correl))\n",
    "print(\"above divided by number of s-k matches:\",len(positive_significant_correl)/len(s_k_match.keys()))\n",
    "print(\"sk matches for reference:\",len(s_k_match.keys()))\n",
    "\n",
    "results_file.write(\"Significant correlations %s Positive significant correlations %s Pos signi. correlations/number of sk matches %s sk matches %s\\n\" % (list_significant_correl,len(positive_significant_correl),len(positive_significant_correl)/len(s_k_match.keys()),len(s_k_match.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### print(correl_pairs)\n",
    "#print(\"\\n\",correl_pairs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plotting correl_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freq\n",
      "[0.4, 0.525, 0.0, 0.075]\n",
      "[0.42857142857142855, 0.0, 0.0, 0.5714285714285714]\n",
      "[0.75, 0.0, 0.0, 0.25]\n",
      "[0.4, 0.2, 0.0, 0.4]\n",
      "[0.75, 0.0, 0.0, 0.25]\n",
      "[0.3, 0.42, 0.0, 0.28]\n",
      "[0.5056179775280899, 0.0898876404494382, 0.24719101123595505, 0.15730337078651685]\n",
      "[0.5, 0.5, 0.0, 0.0]\n",
      "prob\n",
      "[0.08756723737925362, 0.0035979980889357687, 0.9028207832516205, 0.0060139812801901895]\n",
      "[0.1740616319282105, 0.002240338580230259, 0.7951170825847895, 0.02858094690676966]\n",
      "[0.17022324576227751, 0.004700277178966721, 0.23630125957423126, 0.5887752174845244]\n",
      "[0.45833470281678984, 0.011932469271266939, 0.47271209008185733, 0.05702073783008588]\n",
      "[0.32791871137149203, 0.030461075195144183, 0.45948314093036435, 0.18213707250299938]\n",
      "[0.33282792199689165, 0.0492723329110798, 0.17829552980111446, 0.43960421529091415]\n",
      "[0.6199299931874936, 0.12565951470410913, 0.1239077639019501, 0.13050272820644718]\n",
      "[0.6206116067622508, 0.0056218707032388016, 0.1589032743166486, 0.21486324821786187]\n",
      "0 [0.08756723737925362, 0.0035979980889357687, 0.9028207832516205, 0.0060139812801901895]\n",
      "1 [0.1740616319282105, 0.002240338580230259, 0.7951170825847895, 0.02858094690676966]\n",
      "2 [0.17022324576227751, 0.004700277178966721, 0.23630125957423126, 0.5887752174845244]\n",
      "3 [0.45833470281678984, 0.011932469271266939, 0.47271209008185733, 0.05702073783008588]\n",
      "4 [0.32791871137149203, 0.030461075195144183, 0.45948314093036435, 0.18213707250299938]\n",
      "5 [0.33282792199689165, 0.0492723329110798, 0.17829552980111446, 0.43960421529091415]\n",
      "6 [0.6199299931874936, 0.12565951470410913, 0.1239077639019501, 0.13050272820644718]\n",
      "7 [0.6206116067622508, 0.0056218707032388016, 0.1589032743166486, 0.21486324821786187]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors as mcolors\n",
    "\n",
    "\n",
    "colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\n",
    "couleurs = list()\n",
    "\n",
    "for couleur in colors.keys():\n",
    "    couleurs.append(couleur)\n",
    "\n",
    "\n",
    "#valeurs = {\"p1\":[0.4, 0.55, 0.05, 0.0], \"p2\":[0.2, 0.3, 0.5, 0.0], \"p3\":[0.4, 0.2, 0.2, 0.2], \"p4\":[0.2, 0.2, 0.2, 0.4], \"p5\":[0.4, 0.55, 0.05, 0.0], \"p6\":[0.4, 0.55, 0.05, 0.0], \"p7\":[0.4, 0.55, 0.05, 0.0]}\n",
    "colours = ['teal','wheat','aquamarine','navajowhite','darksalmon','cadetblue','coral','limeturquoise','deeppink']\n",
    "\n",
    "valeurs_freq = dict()\n",
    "valeurs_prob = dict()\n",
    "\n",
    "for x in range(0,number_of_slices):   # initialising values[timeperiod] = \"list of freq\" dictionaries\n",
    "    valeurs_freq[x] = list()\n",
    "    valeurs_prob[x] = list()\n",
    "\n",
    "\n",
    "for x in range(0,number_of_slices):\n",
    "    list_freq = list()\n",
    "    list_prob = list()\n",
    "    total_prob_slice = 0\n",
    "    \n",
    "    for sense in correl_pairs.keys():\n",
    "        #print(\"\\t\\t\",x,sense)\n",
    "        list_freq.append(correl_pairs[sense][0][x])\n",
    "        list_prob.append(correl_pairs[sense][1][x])\n",
    "        total_prob_slice += correl_pairs[sense][1][x]  # this counts the total probability for Tx, for normalising\n",
    "        \n",
    "    valeurs_freq[x] = list_freq\n",
    "    valeurs_prob[x] = list_prob\n",
    "    for i in range(0,len(valeurs_prob[x])):\n",
    "        valeurs_prob[x][i] = valeurs_prob[x][i]/total_prob_slice\n",
    "    \n",
    "print(\"freq\")\n",
    "for key in valeurs_freq:\n",
    "    print(valeurs_freq[key])\n",
    "    \n",
    "print(\"prob\")\n",
    "for key in valeurs_prob:\n",
    "    print(valeurs_prob[key])\n",
    "\n",
    "\n",
    "#for key in valeurs.keys():\n",
    "    #print(key)\n",
    "#    list_temp = list()\n",
    "#    for item in valeurs[key]:\n",
    "#        list_temp.append(int(item*100))\n",
    "#    valeurs2[key] = list_temp\n",
    "\n",
    "    #for value in valeurs\n",
    "    \n",
    "\n",
    "#for key,vals in valeurs_freq.items():\n",
    "#    print(key,vals)\n",
    "    \n",
    "#    for i in range(0,len(vals)):        \n",
    "#        if i == 0:\n",
    "#            previous = 0\n",
    "#            plt.bar(x=key, height=vals[i],bottom=previous,color=colours[i])\n",
    "            #plt.bar(x=key, height=vals[i],bottom=previous)\n",
    "            \n",
    "#        else:         \n",
    "#            previous = vals[i-1] + previous\n",
    "#            plt.bar(x=key, height=vals[i],bottom=previous,color=colours[i])\n",
    "            #plt.bar(x=key, height=vals[i],bottom=previous)\n",
    "            \n",
    "#plt.xticks(range(len(valeurs_freq)), valeurs_freq.keys())\n",
    "\n",
    "#plt.show()\n",
    "\n",
    "#for key in valeurs.keys():\n",
    "    #print(key)\n",
    "#    list_temp = list()\n",
    "#    for item in valeurs[key]:\n",
    "#        list_temp.append(int(item*100))\n",
    "#    valeurs2[key] = list_temp\n",
    "\n",
    "    #for value in valeurs\n",
    "\n",
    "previous = 0\n",
    "\n",
    "for key,vals in valeurs_prob.items():\n",
    "    print(key,vals)\n",
    "    \n",
    "    for i in range(0,len(vals)):        \n",
    "        if i == 0:\n",
    "            previous = 0\n",
    "            plt.bar(x=key, height=vals[i],bottom=previous,color=couleurs[i])\n",
    "            #plt.bar(x=key, height=vals[i],bottom=previous)\n",
    "            \n",
    "        else:         \n",
    "            previous = vals[i-1] + previous\n",
    "            plt.bar(x=key, height=vals[i],bottom=previous,color=couleurs[i])\n",
    "            #plt.bar(x=key, height=vals[i],bottom=previous)\n",
    "            \n",
    "plt.xticks(range(len(valeurs_prob)), valeurs_prob.keys())\n",
    "#plt.figure(figsize=(20,10))\n",
    "image = plt.gcf()\n",
    "image.savefig(dir_out+\"/\"+target_id+param_name+\"genre_\"+genre+\"_i\"+str(iterations)+\"_k\"+str(num_top)+\"_time_interval\"+str(time_interval)+\"_model.png\")\n",
    "plt.show()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plotting expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.075, 0.4, 0.525, 0.0]\n",
      "lenght of vals 4\n",
      "TOTAL HITS FOR SENSE mus-1 3\n",
      "TOTAL HITS FOR SENSE NA 16\n",
      "TOTAL HITS FOR SENSE mus-4 21\n",
      "TOTAL HITS FOR SENSE mus-2 0\n",
      "1 [0.5714285714285714, 0.42857142857142855, 0.0, 0.0]\n",
      "lenght of vals 4\n",
      "TOTAL HITS FOR SENSE mus-1 4\n",
      "TOTAL HITS FOR SENSE NA 3\n",
      "TOTAL HITS FOR SENSE mus-4 0\n",
      "TOTAL HITS FOR SENSE mus-2 0\n",
      "2 [0.25, 0.75, 0.0, 0.0]\n",
      "lenght of vals 4\n",
      "TOTAL HITS FOR SENSE mus-1 1\n",
      "TOTAL HITS FOR SENSE NA 3\n",
      "TOTAL HITS FOR SENSE mus-4 0\n",
      "TOTAL HITS FOR SENSE mus-2 0\n",
      "3 [0.4, 0.4, 0.2, 0.0]\n",
      "lenght of vals 4\n",
      "TOTAL HITS FOR SENSE mus-1 2\n",
      "TOTAL HITS FOR SENSE NA 2\n",
      "TOTAL HITS FOR SENSE mus-4 1\n",
      "TOTAL HITS FOR SENSE mus-2 0\n",
      "4 [0.25, 0.75, 0.0, 0.0]\n",
      "lenght of vals 4\n",
      "TOTAL HITS FOR SENSE mus-1 2\n",
      "TOTAL HITS FOR SENSE NA 6\n",
      "TOTAL HITS FOR SENSE mus-4 0\n",
      "TOTAL HITS FOR SENSE mus-2 0\n",
      "5 [0.28, 0.3, 0.42, 0.0]\n",
      "lenght of vals 4\n",
      "TOTAL HITS FOR SENSE mus-1 14\n",
      "TOTAL HITS FOR SENSE NA 15\n",
      "TOTAL HITS FOR SENSE mus-4 21\n",
      "TOTAL HITS FOR SENSE mus-2 0\n",
      "6 [0.15730337078651685, 0.5056179775280899, 0.0898876404494382, 0.24719101123595505]\n",
      "lenght of vals 4\n",
      "TOTAL HITS FOR SENSE mus-1 14\n",
      "TOTAL HITS FOR SENSE NA 45\n",
      "TOTAL HITS FOR SENSE mus-4 8\n",
      "TOTAL HITS FOR SENSE mus-2 22\n",
      "7 [0.0, 0.5, 0.5, 0.0]\n",
      "lenght of vals 4\n",
      "TOTAL HITS FOR SENSE mus-1 0\n",
      "TOTAL HITS FOR SENSE NA 1\n",
      "TOTAL HITS FOR SENSE mus-4 1\n",
      "TOTAL HITS FOR SENSE mus-2 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\n",
    "couleurs = list()\n",
    "\n",
    "for couleur in colors.keys():\n",
    "    couleurs.append(couleur)\n",
    "\n",
    "\n",
    "valeurs = period_relative\n",
    "colours = ['teal','wheat','aquamarine','navajowhite','darksalmon','cadetblue','coral','limeturquoise','magenta','peru']\n",
    "#colours = ['b','g','r','c','m','y','k','w']\n",
    "colors = couleurs\n",
    "\n",
    "for key,vals in valeurs.items():\n",
    "    print(key,vals)\n",
    "    print(\"lenght of vals\",len(vals))\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(0,len(vals)):        \n",
    "        if i == 0:\n",
    "            print(\"TOTAL HITS FOR SENSE\",expert_senses[i],sense_date_amount[expert_senses[i],key])\n",
    "            previous = 0\n",
    "            #print(i)\n",
    "            #print(colours[i],\"height=\",vals[i])\n",
    "            #plt.bar(x=key, height=vals[i],bottom=previous,color=colours[i])\n",
    "            #plt.bar(x=key, height=vals[i],bottom=previous)\n",
    "            plt.bar(x=key, height=vals[i],bottom=previous,color=colors[i])\n",
    "            \n",
    "        else:         \n",
    "            print(\"TOTAL HITS FOR SENSE\",expert_senses[i],sense_date_amount[expert_senses[i],key])\n",
    "            previous = vals[i-1] + previous\n",
    "            #print(\"i\",i,\"key=period\",key)\n",
    "            #print(colours[i],\"height=\",vals[i])\n",
    "            #print(vals[i])\n",
    "            #plt.bar(x=key, height=vals[i],bottom=previous,color=colours[i])\n",
    "            #plt.bar(x=key, height=vals[i],bottom=previous)\n",
    "            plt.bar(x=key, height=vals[i],bottom=previous,color=colors[i])\n",
    "            \n",
    "plt.xticks(range(len(valeurs)), valeurs.keys())\n",
    "\n",
    "\n",
    "#plt.figure(figsize=(20,10))\n",
    "image = plt.gcf()\n",
    "image.savefig(dir_out+\"/\"+target_id+param_name+\"genre_\"+genre+\"_i\"+str(iterations)+\"_k\"+str(num_top)+\"_time_interval\"+str(time_interval)+\"_expert.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
