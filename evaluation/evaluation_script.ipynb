{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a work-in-progress notebook\n",
    "\n",
    "We wish to know this:\n",
    "\n",
    "1. How well does the model identify the correct number of senses for the target word?\n",
    "2. **How well does the model identify the correct senses for the target word?**\n",
    "3. **How well does the model assign the right words to a given sense of the target word?**\n",
    "4. How well does the model assign the senses to the time intervals for the target word?\n",
    "\n",
    "The script will evaluate **Q2** and **Q3**. Q4 will follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filenames of different model outputs must be different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic variables and imports:\n",
    "\n",
    "import codecs, csv, os, time, re, io\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "# directories\n",
    "dir_in = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), \"evaluation\", \"evaluation_input\"))\n",
    "dir_out = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), \"evaluation\", \"evaluation_output\"))\n",
    "\n",
    "\n",
    "s_senses = io.open(dir_in+\"/senses_69419.txt\",\"r\")\n",
    "k_senses = io.open(dir_in+\"/mus.dat\",\"r\")\n",
    "\n",
    "# DEBUG:\n",
    "#s_senses = io.open(dir_in+\"/senses_69419_debug.txt\",\"r\")\n",
    "#k_senses = io.open(dir_in+\"/mus_debug.dat\",\"r\")\n",
    "# k0 = mus4\n",
    "# k1 = mus3\n",
    "# k2 = mus2\n",
    "# k3 = mus1\n",
    "# k4 = nothing\n",
    "\n",
    "file_senses = s_senses.readlines()[1:]\n",
    "output_senses = k_senses.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- ~~create the notebook~~\n",
    "- ~~organise the notebook~~\n",
    "- ~~write \"general idea\" pseudocode for the evaluation~~\n",
    "- ~~get input files~~\n",
    "- ~~figure out data structures to store the variables~~\n",
    "- ~~write actual code~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: How well does the model identify the correct senses for the target word?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-82fa54601c9a>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-82fa54601c9a>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    for each k:\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# For each target word, we have a list of senses  s (given by the expert)\n",
    "# For each target word, we have a list of senses k (given by the model)\n",
    "# This Q consists in matching s and k, and doing so in a confident way --> confidence score\n",
    "\n",
    "for each k:\n",
    "    for each s:\n",
    "        create conf(k,s)\n",
    "\n",
    "# What is conf(k,s)?\n",
    "        conf(k,s) = (p1*match(w1,s)+p2*match(w1,s)+px(wx,s))/10 WHERE\n",
    "    \n",
    "            px = probability of word wx \n",
    "                \n",
    "                and\n",
    "            \n",
    "            match(wx,s) =   1/number_of_senses_assigned_to_wx if s_is_one_of_them \n",
    "            \n",
    "                    or \n",
    "                            0 if w_is_not_associated_to_s\n",
    "                \n",
    "# Once we have gone through all s for one k, we have to choose the best k for s. How? (TBD, cfr Valerio and Barbara)\n",
    "\n",
    "# Once all ks have been assigned to all ss (or NA), we can calculate a general confidence score for the model.\n",
    "# One easy way to do that: \n",
    "\n",
    "conf_score_model = number_of_non_NA/k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real code\n",
    "\n",
    "Steps:\n",
    "\n",
    "- extract all senses from the file\n",
    "- use those senses as keys for a dictionary, `dict_of_words`\n",
    "- fill the dictionary: for each key, we store a list of words pertaining to that sense\n",
    "- transform the lists as sets so as to remove duplicates within the same sense\n",
    "- create a dictionary with a word as a key and its weight as a value, depending on how many senses it appears\n",
    "- parse the model output and get the probability weights for each word\n",
    "- do not take into account the first line\n",
    "- take care of empty lines\n",
    "\n",
    "Todo:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of senses: 3\n",
      "mus-4 {'94365', 'nlsj82490', '83605', '3630', '58142', '82451', '18334', '35244', '29883', '70521', '55383', '76516', '71404', '112703', '103085', '104355', '14269', '37405', '50521', '33426', '85306', '110639', '43616', '75989', '4474', '33969', '33770', '47735', '62204', '26226', '19972', '115887', '91583', '61231', '34522', '36289', '76765', '75005', '93341', '24289', '112720', '109733', '15011', '33238', '84234', '112559', '83732', '79069', '101851', '102000', '37726', '35570', '83834', '100379', '13813', '37961', '52571', '48291', '100964', '35166', '51785', '2392', '55981', '82190', '57460', '58972', '83665', '61245', '10223', '47442', '109918', '57474', '114083', '62528', '51358', '51369', 'nlsj177', '55391', '14266', '106263', '3364', '24340', 'nlsj71743', '51241', '70495', '82800', '88855', '39180', '621', '85099', '26039', '20836', '36180', '84534', '31609', '114339', '16051', '70556', '83174', '99647', '66389', '13715', '66494', '96981', '98234', '74463', '24132', '24946', '18062', '112367', '115374', '105458', '86108', '95970', '65703', '71585', 'nlsj1147', '10933', 'nlsj42558', '39238', '67555', '39016', '76513', '93812', '61553', '73672', '64703', '85953', '61128', '67009', '64316', '96698', '90347', '91634', '22209', '84034', '13376', '66342', '4470', '19528', '84421', '1199', '3325', '114753', '39078', '24184', '60840', '106502', '97241', '93435', '106206', '116216', '13039', '6339', '71373', '41271', '94677', '69532', '100495', 'nlsj99416', '111722', '93379', '39543', '97605', '104624', '63928', '65191', '109729', '110114', '112307', '21651', '65966', '97963', '49629', '111764', '83468', '57466', '92926', '75552', '11583', '34982', '67762', '114653', '104690', '68996', '115212', '56991', 'nlsj112819', '113477', '6616', '66750', '44660', '7608', '16804', '98869', '108652', '74735', '34366', '13942', '93332', '43975', '114615', '70364', '67526', '8909', '20830', '15609', '96853', '61176', '37075', '67507', '22210', '85876', '83825', '115748', '63550', '106676', '90504', '37262', '72202', '74752', '106222', '115102', '28349', '57457', '20885', '4493', '45996', '33486', '28588', '101034', '6898', '68641', '26002', '65552', '102676', '70558', '97252', '67352', '95961', '89405', '61946', '55313', 'nlsj33345', '53015', '104639', '30317', '19503', '63772', 'nlsj61122', '27092', '4845', '102795', '83650', '63873', '55497', '101714', '114387', '113560', '18790', '60646', '60703', '96999', '50908', '66761', '37720', '85665', '112794', 'nlsj86473', '45153', '26046', '87413', '75595', '15786', '76028', '42214', '105548', '90334', '114553', '98584', '31607', 'nlsj79694', '74621', '28569', '71065', '15893', '13315', '116416', '32657', '66755', '39299', '41570', 'nlsj82141', '97606', '66295', '98417', '71984', '93388', 'nlsj35116', '77640', '6174', '46646', '20621', '96560', '35417', '58319', '42071', '22660', '116218', '38472', '36216', 'nlsj40134', '22425', '2898', '48436', '96503', '48704', '116408', '7793', '11058', '72610', '45245', '36938', '74378', '31562', '58429', '69419', '13985', '56189', '26034', '90166', '50236', '83869', '48504', '108780', '13852', '59516'}\n",
      "\n",
      "\n",
      "\n",
      "mus-2 {'7362', '69220', '102474', '35633', '101982', '67762', '113163', 'nlsj112110', 'nlsj61217', '74675', '60878', '68144', 'nlsj76740', '57262', '92051', 'nlsj61519', '114437', '67422', '97266', '101257', 'nlsj70384', '65934', '65983', '103773', '86213', '5591', '4698', '30569', '110639', '110127', '16986', '19236', 'nlsj6085', '54990', '68706', '53254', '26328', '47735', '51737', '46452', 'nlsj32819', '61622', '26818', '105557', '82623', '64983', 'nlsj68042', '94291', '76910', '61185', '62274', '30564', '16884', '37343', '53538', '56223', '38744', '75992', '45526', '65565', '6184', '45525', '69751', '4763', '95095', '34243', '20270', '114347', '4670', '103667', '49176', '103648', '105691', '15198', '106267', '42678', '19789', '20674', '52571', '104416', 'nlsj114757', '21920', '60425', '45996', '112724', '21490', 'nlsj100389', '49444', '65552', '104637', '76335', '59121', '75652', '53222', '19623', '41918', '21168', '84089', '115638', '113881', '62337', '68185', '45757', '114381', '95144', '19641', '59626', '74126', '83209', '112811', '63032', '29390', '58262', '93624', '101790', '91198', 'nlsj3264', '45980', '21205', '18762', '56954', 'nlsj7772', '93880', '33822', '100673', 'nlsj4249', '4562', '92553', 'nlsj3072', '102930', '49182', '75539', 'nlsj8425', '16440', '82758', '66494', 'nlsj3191', '74225', '85672', '2586', '20508', '83458', 'nlsj4805', '112512', '65544', '29134', '95540', '22316', 'nlsj75531', '89321', '20469', 'nlsj10103', 'nlsj33192', '74610', 'nlsj36416', '110060', '80188', '15763', '45182', 'nlsj5634', '60316', '94164', '67364', '90350', '90303', '91541', '64559', 'nlsj86871', '92475', '60317', 'nlsj12526', '214', '93840', '93388', '1904', '68539', '73672', '32263', '103637', '53286', '80525', '64642', '75653', '60113', '47745', '66342', '70768', 'nlsj114797', '22389', 'nlsj10202', '94967', '106206', '58133', '116273', 'nlsj2729', '98955', '18561', '62535', '20413', 'nlsj3887', '83212', '64483', 'nlsj29538', '74378', '74165', '58429', '58527', '103038', '69419', 'nlsj99416', '23861', '104624', '62258', '106023', '49875', '22166', '57356', '49439', '112733', '93842', '33457', '98114', '17228', '67360', '58271'}\n",
      "\n",
      "\n",
      "\n",
      "mus-1 {'2731', 'nlsj78558', '55898', '26048', '15378', '31236', '3398', '61925', '80557', 'nlsj1499', '29883', '4906', '59124', '47665', '102869', '64757', '80239', '110598', '34372', '77699', '73128', '55149', '12641', '30911', '86307', '10056', '25007', '47513', '104355', '114587', '104289', '27415', '21335', '85306', '110639', '68174', '15380', 'nlsj40053', '47735', '33770', '95654', '66173', '27629', '63713', '92171', '37776', 'nlsj801', '19972', '94097', '76910', '52332', '43977', '26207', '43206', '64914', '109687', '95258', '36790', '73221', '64448', 'nlsj105676', '31491', '65565', '5112', '104538', '22882', '49886', '74127', '85807', '2845', '260', '24226', '37726', '33241', '55499', '35570', '52571', '56874', '47964', '109403', '72928', '34071', '56406', 'nlsj5437', '96089', '23628', '95819', '23283', '49933', '83665', 'nlsj9035', '53442', '73972', 'nlsj11066', '15047', '3986', '29962', '38743', '51300', '12948', '18166', '7529', '13579', '22739', '18937', '112347', 'nlsj4320', '40323', '114842', '111207', '26499', '62528', '19788', '78716', '98241', '110655', '62816', '89366', 'nlsj71743', '106566', '67104', '52095', '4536', '19282', '37851', '47876', '6325', '45980', '7612', '111416', '44888', '13098', '31161', '83928', '84534', '50644', '678', '94316', '5607', '47617', '20179', 'nlsj8970', '66494', '84248', '63352', '98234', '38732', 'nlsj68688', '21431', '105816', '72716', '15121', 'nlsj33192', '76530', '90329', '26114', '45671', '24523', '15763', '84422', '103871', '83783', '83113', '70708', '61040', '49437', '65697', '2671', '18331', 'nlsj9526', '93417', '34855', '17197', '103637', 'nlsj2610', '12035', 'nlsj100912', '31964', '26136', '77928', '113251', '15618', '100774', '37711', '113556', '14050', '50093', '64316', '75352', '42659', '104225', '37244', '1692', '41357', '59005', '95221', '83266', '51259', '106502', '34476', '112169', '110284', '23242', '80011', '51647', 'nlsj4113', '463', '112943', 'nlsj10580', '102669', '3241', '12485', 'nlsj69856', '3464', '41538', '42827', '73064', '49955', '9757', '47917', 'nlsj5118', '11970', 'nlsj8671', 'nlsj98815', '110114', '97386', '15571', '112070', '69714', '74126', '45513', '83310', '116059', '98312', '25228', '91055', '11305', '83251', '7133', '102474', '41032', '11583', '61177', '67762', '7561', '81343', '83718', '64586', '105344', '2583', '94098', '56810', '84150', '57262', '49506', 'nlsj80462', '48770', '45917', '65983', '18128', '31556', 'nlsj7856', '62205', '41705', '101853', '116470', 'nlsj11198', '3289', '67619', '49227', '23468', '42581', '84494', '53161', '60402', '82665', '8909', '51815', '116050', '116244', '26032', '2873', 'nlsj105619', '103922', '4335', '97285', '4670', '79947', '112472', 'nlsj4012', '71314', '28355', '27209', 'nlsj57918', '90504', '71673', '12176', '72202', '20674', '4587', '3327', 'nlsj114757', '92010', '45996', '69469', '65097', '92406', '82756', '16400', '65552', '13608', '40156', '50451', '2224', '91351', '2764', '93536', '86219', '40001', '33671', '104639', '13427', '66294', '35710', '49589', '5131', '4845', '7177', '1083', '79697', '18706', '46216', '2340', '101445', '19346', '76184', '67250', '24444', '107959', '46176', '92077', '11072', '74631', '30700', '76564', '92162', '41619', '46195', '110606', '103993', '96565', '100965', '79261', '436', '67485', '85672', 'nlsj43224', '50073', '57321', '82954', '3237', '23678', '28036', '111895', '75306', '21783', '74571', 'nlsj77405', '103957', '4068', '84552', '2061', '32657', '35267', '99638', '93796', '86305', '21589', '37095', '22100', '41082', '29828', '99622', '48670', '94474', '104872', '74602', '113060', '93449', '46646', '20621', '37274', '95432', '54399', '15162', '86352', '28002', '7724', '65089', '27005', '15858', '17007', '59276', '80555', '49331', '19711', 'nlsj48763', '105070', '41633', '102989', '56003', '89309', '48704', '112351', '37616', '113711', '11058', '72610', 'nlsj40132', '80043', '114731', '45285', '22036', 'nlsj32160', 'nlsj4579', '31562', '51849', '85417', '26197', '19546', '62383', '55815', '67868', '50679', '43305', '2834', '51373', 'nlsj86496', '113823', '108536', '61885', '4274', '110456', '98173', '8586', '104860', '51376', '34603', '22209', '84475', '8665', '21830', '70477', '27764', '51661', '79595', '3128', '18334', '24801', '66777', 'nlsj61205', '107905', '45656', '61265', '7712', '108537', '96979', '69863', '112769', '102381', '26447', '74523', '72273', '75948', '47447', '94713', '61791', '62204', '33956', '941', '58478', '63814', '115845', '13002', '53695', '61856', '51819', '114972', '29974', '76765', '67815', '95522', '60225', '75954', '6384', '51256', '83760', '26943', '112720', '38488', '21487', '39190', '84234', '112559', '6684', '54946', '55139', '12349', '100906', '20195', '5132', '66262', '46966', '102000', '21812', '48498', '11206', '104421', '51727', '73707', '72268', 'nlsj32167', '48291', '20945', '1377', '103221', '109730', '75263', '25018', '66639', '115810', '54812', '36165', '70958', '65628', '24003', '51358', '51369', '80357', '43060', '51241', '42686', 'nlsj1611', '53942', '41536', '107167', 'nlsj80958', '69052', '50616', '54592', '19123', '110302', '34848', '28360', '26684', '94483', '72321', '2219', '18788', '16051', '403', '75316', '93022', '60308', '37488', '59501', '69252', '106974', '115193', '85420', 'nlsj114103', '84434', '91944', '7944', '11229', '80327', '55137', '72287', '71559', '31948', 'nlsj12527', '41481', '71308', '112816', '62093', '70105', '42890', '10933', '108882', '52035', '93812', '16052', '7832', '95074', '7159', '68970', 'nlsj6617', '55498', '76157', 'nlsj4784', 'nlsj60710', '98031', '85953', '98723', '39847', '88498', '47745', '84263', '70768', '52460', '79103', 'nlsj10130', '28592', '55532', '46074', '66746', '65975', '4548', '16609', '112934', '7182', '114847', '17962', '95368', '42887', 'nlsj2729', '13039', 'nlsj96345', '70550', '40161', '13317', '38547', '92662', '33074', '83434', '43089', '114688', '112833', 'nlsj106503', '25870', '16171', '32686', '4778', 'nlsj59923', '3800', '24261', '113741', '114816', '106064', '83756', '114548', '23690', '112467', '75552', 'nlsj10876', 'nlsj17074', '67974', '2767', '25402', '21802', '54113', '18271', '104690', 'nlsj28819', '16430', '91516', '78746', '103701', '71118', '59588', '56991', '91800', '106114', '72357', '74735', '34366', '43124', '61291', '114615', '14181', '48479', '42842', '7779', '61176', '33647', '63827', '28566', '95853', '12409', '23794', '114706', '100524', '84725', '103975', '46804', '21920', '19536', '4493', '19452', '69036', '63845', '68641', '63314', '116058', '4927', '88464', '76335', '88716', '113379', '4378', '41918', '110089', '19891', '48341', '38966', 'nlsj68228', '59708', '80042', '6449', '50824', '17730', '103972', '20728', '110484', '22997', '19641', '39313', '83253', '12973', '61056', '46123', '80761', '68791', '103942', '113560', '24856', 'nlsj47984', '57057', '83774', '71262', '11197', 'nlsj75598', '76703', '102847', '48867', '82959', '54971', '61855', '116293', '105176', '82758', '63143', '62303', '5009', 'nlsj96033', '35708', '95523', '14362', '101716', '22316', '33160', 'nlsj5738', '89807', '46474', '105550', '17381', '28569', '71065', '15893', '31565', 'nlsj106628', '110027', '116416', 'nlsj5904', '80107', '77698', '35092', 'nlsj8979', '23658', '22502', '24292', '39125', '19268', '63212', '48095', '33494', '73277', '93388', '40545', '26233', '53956', '6174', '42830', '64956', '50252', '72275', '79223', 'nlsj7583', '1984', '84094', '97147', '86429', '36571', '100693', '103152', '80664', '21901', '55997', '70482', '7062', '36390', 'nlsj7783', '31996', 'nlsj52509', '104655', '72627', '104311', '31709', '41502', '76431', '69419', '75910', '103012', '49875', '7498', '8505', '2749', '65757', '1564', '91085', '5521', '83869', '29624', '108780', '39195', '74819'}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expert_senses = list() # list where we store all sense ids provided by expert\n",
    "for s in file_senses: # 60 for testing purposes\n",
    "    s = s.split(\"\\t\")\n",
    "    sense = s[11] # The sense ID is after the 10th tab\n",
    "    if sense != 'w':\n",
    "        expert_senses.append(sense)\n",
    "    \n",
    "#print(len(expert_senses),expert_senses,len(set(expert_senses)))\n",
    "\n",
    "expert_senses = list(set(expert_senses)) # we only keep the unique senses\n",
    "number_of_s = len(expert_senses)  # we create a variable that stores the number of unique senses\n",
    "print(\"Number of senses:\",number_of_s)\n",
    "\n",
    "# This dictionary has a sense as a key, and a list of words as a value. \n",
    "dict_of_words = dict()\n",
    "# This list stores all words\n",
    "list_of_all_words = list()\n",
    "# This dictionary stores all words as keys and their weight as value\n",
    "word_weight = dict()\n",
    "\n",
    "for i in range(0,number_of_s): # for each sense, we create a dictionary entry which has a list as value\n",
    "    dict_of_words[expert_senses[i]] = list()\n",
    "\n",
    "    for s in file_senses: # we go back in the file\n",
    "        s = s.split(\"\\t\") # splitting on tabs\n",
    "        \n",
    "        sentence_of_ids = s[8] # 8 is for IDs, 9 is for words\n",
    "        list_of_ids = sentence_of_ids.split(\" \")  # splitting on spaces\n",
    "        for word_id in list_of_ids:\n",
    "            if s[11] == expert_senses[i]:      # we store all words for one sense \n",
    "                dict_of_words[expert_senses[i]].append(word_id)\n",
    "            list_of_all_words.append(word_id) # we store all words, we'll iterate over that for scores\n",
    "        \n",
    "\n",
    "    # Here, we remove duplicates\n",
    "    #dict_of_words[expert_senses[i]].append(\"79223\") #testing\n",
    "    dict_of_words[expert_senses[i]] = set(dict_of_words[expert_senses[i]]) \n",
    "      \n",
    "    print(expert_senses[i],set(dict_of_words[expert_senses[i]]))\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every word in the list of words that we have\n",
    "# we count the number of senses it appears in\n",
    "# we use that number to divide its importance: 1 sense = 1 importance; 2 senses = 0.5 importance\n",
    "# this can be finetuned\n",
    "\n",
    "for word in list_of_all_words:\n",
    "    x = 0\n",
    "    for i in range(0,number_of_s):\n",
    "        if word in dict_of_words[expert_senses[i]]:\n",
    "            x += 1 \n",
    "        if x != 0:\n",
    "            word_weight[word] = 1/x\n",
    "        \n",
    "    \n",
    "    #print(word,word_weight[word])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parsing output.dat\n",
    "- split on \"===============  per time  ===============\" and keep first part\n",
    "- transform that into a list, then\n",
    "- get lines that start with \"p(w|s)\"\n",
    "- count those, k = that number\n",
    "- split the line on \":\", keep the second part\n",
    "- split the rest on \";\", it's [ID] = prob_from_this_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word id 28355 ; probability 0.088\n",
      "word id 69419 ; probability 0.069\n",
      "word id 57460 ; probability 0.056\n",
      "word id 114587 ; probability 0.042\n",
      "word id 42071 ; probability 0.041\n",
      "word id 35267 ; probability 0.035\n",
      "word id 51647 ; probability 0.035\n",
      "word id 64448 ; probability 0.023\n",
      "word id 45980 ; probability 0.018\n",
      "word id 53826 ; probability 0.017\n",
      "{'28355': 0.2075471698113207}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377, '57460': 0.1320754716981132}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377, '57460': 0.1320754716981132, '114587': 0.0990566037735849}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377, '57460': 0.1320754716981132, '114587': 0.0990566037735849, '42071': 0.09669811320754716}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377, '57460': 0.1320754716981132, '114587': 0.0990566037735849, '42071': 0.09669811320754716, '35267': 0.08254716981132075}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377, '57460': 0.1320754716981132, '114587': 0.0990566037735849, '42071': 0.09669811320754716, '35267': 0.08254716981132075, '51647': 0.08254716981132075}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377, '57460': 0.1320754716981132, '114587': 0.0990566037735849, '42071': 0.09669811320754716, '35267': 0.08254716981132075, '51647': 0.08254716981132075, '64448': 0.05424528301886792}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377, '57460': 0.1320754716981132, '114587': 0.0990566037735849, '42071': 0.09669811320754716, '35267': 0.08254716981132075, '51647': 0.08254716981132075, '64448': 0.05424528301886792, '45980': 0.042452830188679236}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377, '57460': 0.1320754716981132, '114587': 0.0990566037735849, '42071': 0.09669811320754716, '35267': 0.08254716981132075, '51647': 0.08254716981132075, '64448': 0.05424528301886792, '45980': 0.042452830188679236, '53826': 0.04009433962264151}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377, '57460': 0.1320754716981132, '114587': 0.0990566037735849, '42071': 0.09669811320754716, '35267': 0.08254716981132075, '51647': 0.08254716981132075, '64448': 0.05424528301886792, '45980': 0.042452830188679236, '53826': 0.04009433962264151}\n",
      "word id 79223 ; probability 0.063\n",
      "word id 92927 ; probability 0.057\n",
      "word id 46574 ; probability 0.038\n",
      "word id 67660 ; probability 0.038\n",
      "word id 103085 ; probability 0.036\n",
      "word id 86112 ; probability 0.030\n",
      "word id 101982 ; probability 0.029\n",
      "word id 75808 ; probability 0.026\n",
      "word id 68539 ; probability 0.024\n",
      "word id 54607 ; probability 0.024\n",
      "{'79223': 0.17260273972602735}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438, '46574': 0.10410958904109587}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438, '46574': 0.10410958904109587, '67660': 0.10410958904109587}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438, '46574': 0.10410958904109587, '67660': 0.10410958904109587, '103085': 0.09863013698630134}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438, '46574': 0.10410958904109587, '67660': 0.10410958904109587, '103085': 0.09863013698630134, '86112': 0.08219178082191778}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438, '46574': 0.10410958904109587, '67660': 0.10410958904109587, '103085': 0.09863013698630134, '86112': 0.08219178082191778, '101982': 0.07945205479452053}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438, '46574': 0.10410958904109587, '67660': 0.10410958904109587, '103085': 0.09863013698630134, '86112': 0.08219178082191778, '101982': 0.07945205479452053, '75808': 0.07123287671232874}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438, '46574': 0.10410958904109587, '67660': 0.10410958904109587, '103085': 0.09863013698630134, '86112': 0.08219178082191778, '101982': 0.07945205479452053, '75808': 0.07123287671232874, '68539': 0.06575342465753423}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438, '46574': 0.10410958904109587, '67660': 0.10410958904109587, '103085': 0.09863013698630134, '86112': 0.08219178082191778, '101982': 0.07945205479452053, '75808': 0.07123287671232874, '68539': 0.06575342465753423, '54607': 0.06575342465753423}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438, '46574': 0.10410958904109587, '67660': 0.10410958904109587, '103085': 0.09863013698630134, '86112': 0.08219178082191778, '101982': 0.07945205479452053, '75808': 0.07123287671232874, '68539': 0.06575342465753423, '54607': 0.06575342465753423}\n",
      "word id 62258 ; probability 0.080\n",
      "word id 64586 ; probability 0.052\n",
      "word id 58271 ; probability 0.048\n",
      "word id nlsj86871 ; probability 0.041\n",
      "word id 101851 ; probability 0.039\n",
      "word id nlsj183 ; probability 0.037\n",
      "word id 75653 ; probability 0.031\n",
      "word id nlsj59923 ; probability 0.030\n",
      "word id 70105 ; probability 0.026\n",
      "word id 82758 ; probability 0.024\n",
      "{'62258': 0.19607843137254902}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685, '58271': 0.11764705882352941}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685, '58271': 0.11764705882352941, 'nlsj86871': 0.10049019607843138}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685, '58271': 0.11764705882352941, 'nlsj86871': 0.10049019607843138, '101851': 0.09558823529411764}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685, '58271': 0.11764705882352941, 'nlsj86871': 0.10049019607843138, '101851': 0.09558823529411764, 'nlsj183': 0.0906862745098039}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685, '58271': 0.11764705882352941, 'nlsj86871': 0.10049019607843138, '101851': 0.09558823529411764, 'nlsj183': 0.0906862745098039, '75653': 0.07598039215686274}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685, '58271': 0.11764705882352941, 'nlsj86871': 0.10049019607843138, '101851': 0.09558823529411764, 'nlsj183': 0.0906862745098039, '75653': 0.07598039215686274, 'nlsj59923': 0.07352941176470587}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685, '58271': 0.11764705882352941, 'nlsj86871': 0.10049019607843138, '101851': 0.09558823529411764, 'nlsj183': 0.0906862745098039, '75653': 0.07598039215686274, 'nlsj59923': 0.07352941176470587, '70105': 0.06372549019607843}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685, '58271': 0.11764705882352941, 'nlsj86871': 0.10049019607843138, '101851': 0.09558823529411764, 'nlsj183': 0.0906862745098039, '75653': 0.07598039215686274, 'nlsj59923': 0.07352941176470587, '70105': 0.06372549019607843, '82758': 0.058823529411764705}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685, '58271': 0.11764705882352941, 'nlsj86871': 0.10049019607843138, '101851': 0.09558823529411764, 'nlsj183': 0.0906862745098039, '75653': 0.07598039215686274, 'nlsj59923': 0.07352941176470587, '70105': 0.06372549019607843, '82758': 0.058823529411764705}\n",
      "word id 102000 ; probability 0.035\n",
      "word id 70495 ; probability 0.035\n",
      "word id 7182 ; probability 0.029\n",
      "word id 70958 ; probability 0.024\n",
      "word id 49589 ; probability 0.021\n",
      "word id 83174 ; probability 0.021\n",
      "word id 106676 ; probability 0.019\n",
      "word id 22209 ; probability 0.018\n",
      "word id 93388 ; probability 0.017\n",
      "word id 16132 ; probability 0.016\n",
      "{'102000': 0.14893617021276598}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598, '7182': 0.12340425531914895}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598, '7182': 0.12340425531914895, '70958': 0.1021276595744681}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598, '7182': 0.12340425531914895, '70958': 0.1021276595744681, '49589': 0.08936170212765958}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598, '7182': 0.12340425531914895, '70958': 0.1021276595744681, '49589': 0.08936170212765958, '83174': 0.08936170212765958}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598, '7182': 0.12340425531914895, '70958': 0.1021276595744681, '49589': 0.08936170212765958, '83174': 0.08936170212765958, '106676': 0.08085106382978724}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598, '7182': 0.12340425531914895, '70958': 0.1021276595744681, '49589': 0.08936170212765958, '83174': 0.08936170212765958, '106676': 0.08085106382978724, '22209': 0.07659574468085106}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598, '7182': 0.12340425531914895, '70958': 0.1021276595744681, '49589': 0.08936170212765958, '83174': 0.08936170212765958, '106676': 0.08085106382978724, '22209': 0.07659574468085106, '93388': 0.0723404255319149}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598, '7182': 0.12340425531914895, '70958': 0.1021276595744681, '49589': 0.08936170212765958, '83174': 0.08936170212765958, '106676': 0.08085106382978724, '22209': 0.07659574468085106, '93388': 0.0723404255319149, '16132': 0.06808510638297872}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598, '7182': 0.12340425531914895, '70958': 0.1021276595744681, '49589': 0.08936170212765958, '83174': 0.08936170212765958, '106676': 0.08085106382978724, '22209': 0.07659574468085106, '93388': 0.0723404255319149, '16132': 0.06808510638297872}\n",
      "word id nlsj5634 ; probability 0.039\n",
      "word id 61925 ; probability 0.037\n",
      "word id 12620 ; probability 0.030\n",
      "word id 69419 ; probability 0.027\n",
      "word id 104421 ; probability 0.027\n",
      "word id 91085 ; probability 0.024\n",
      "word id 71308 ; probability 0.022\n",
      "word id 115748 ; probability 0.021\n",
      "word id 19641 ; probability 0.020\n",
      "word id 5390 ; probability 0.019\n",
      "{'nlsj5634': 0.14661654135338348}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228, '12620': 0.11278195488721805}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228, '12620': 0.11278195488721805, '69419': 0.10150375939849625}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228, '12620': 0.11278195488721805, '69419': 0.10150375939849625, '104421': 0.10150375939849625}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228, '12620': 0.11278195488721805, '69419': 0.10150375939849625, '104421': 0.10150375939849625, '91085': 0.09022556390977446}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228, '12620': 0.11278195488721805, '69419': 0.10150375939849625, '104421': 0.10150375939849625, '91085': 0.09022556390977446, '71308': 0.08270676691729324}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228, '12620': 0.11278195488721805, '69419': 0.10150375939849625, '104421': 0.10150375939849625, '91085': 0.09022556390977446, '71308': 0.08270676691729324, '115748': 0.07894736842105265}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228, '12620': 0.11278195488721805, '69419': 0.10150375939849625, '104421': 0.10150375939849625, '91085': 0.09022556390977446, '71308': 0.08270676691729324, '115748': 0.07894736842105265, '19641': 0.07518796992481204}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228, '12620': 0.11278195488721805, '69419': 0.10150375939849625, '104421': 0.10150375939849625, '91085': 0.09022556390977446, '71308': 0.08270676691729324, '115748': 0.07894736842105265, '19641': 0.07518796992481204, '5390': 0.07142857142857144}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228, '12620': 0.11278195488721805, '69419': 0.10150375939849625, '104421': 0.10150375939849625, '91085': 0.09022556390977446, '71308': 0.08270676691729324, '115748': 0.07894736842105265, '19641': 0.07518796992481204, '5390': 0.07142857142857144}\n"
     ]
    }
   ],
   "source": [
    "lines_output = output_senses.split(\"===============  per time  ===============\")[0].split(\"\\n\")\n",
    "\n",
    "number_of_the_k = 0\n",
    "\n",
    "k_words_with_prob = dict()\n",
    "\n",
    "for line in lines_output:\n",
    "    if line[:6] == \"p(w|s)\":\n",
    "        line = line.split(\":\")[1]\n",
    "        line = line.split(\";\")\n",
    "        #print(number_of_the_k,line)\n",
    "        dico_word_prob = dict()\n",
    "        temp_dict = dict()\n",
    "        k_words_with_prob[number_of_the_k] = list()\n",
    "        \n",
    "        line = line[:-1] # last item of the list is empty\n",
    "        \n",
    "        total_probability = 0 # to have relative probs\n",
    "        for word_prob in line:\n",
    "            \n",
    "\n",
    "        \n",
    "            #word_prob = word_prob.split(\",\")\n",
    "            #for word in word_prob:\n",
    "            probability = re.findall(\"([\\d.\\w]*)\",word_prob)\n",
    "            if probability:\n",
    "                probability = list(filter(None,probability))\n",
    "                    \n",
    "            total_probability += float(probability[1])\n",
    "            print(\"word id\",probability[0],\"; probability\",probability[1])\n",
    "        \n",
    "            dico_word_prob[probability[0]] = float(probability[1])\n",
    "        #print(type(k_words_with_prob[number_of_the_k]))\n",
    "        \n",
    "        for i in dico_word_prob.keys():\n",
    "            \n",
    "            temp_dict[i] = float(dico_word_prob[i]/total_probability)\n",
    "            k_words_with_prob[number_of_the_k] = temp_dict\n",
    "            \n",
    "            print(k_words_with_prob[number_of_the_k])\n",
    "            \n",
    "        #k_words_with_prob[number_of_the_k] = [float(dico_word_prob[i]/total_probability) for i in dico_word_prob]\n",
    "        #print(k_words_with_prob[number_of_the_k])\n",
    "        print(temp_dict)\n",
    "        number_of_the_k += 1\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k_words_with_prob\n",
    "This dictionary has the sense number 'k' as keys and the a dictionary of [word] = probability as values.\n",
    "Example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "#print(\"Probability for word ID 5390 in sense k = 4:\",k_words_with_prob[4][\"5390\"])\n",
    "print(type(k_words_with_prob[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output sense 0\n",
      "\texpert sense number  0 mus-4\n",
      "\t\tword from annotation for sense 0 : 28355\n",
      "\t\tword from annotation for sense 0 : 69419\n",
      "\t\t\tword  69419 is in output for sense 0 with probability: 0.16273584905660377 and weight: 0.3333333333333333\n",
      "\t\tword from annotation for sense 0 : 57460\n",
      "\t\t\tword  57460 is in output for sense 0 with probability: 0.1320754716981132 and weight: 1.0\n",
      "\t\tword from annotation for sense 0 : 114587\n",
      "\t\tword from annotation for sense 0 : 42071\n",
      "\t\t\tword  42071 is in output for sense 0 with probability: 0.09669811320754716 and weight: 1.0\n",
      "\t\tword from annotation for sense 0 : 35267\n",
      "\t\tword from annotation for sense 0 : 51647\n",
      "\t\tword from annotation for sense 0 : 64448\n",
      "\t\tword from annotation for sense 0 : 45980\n",
      "\t\tword from annotation for sense 0 : 53826\n",
      "\texpert sense number  1 mus-2\n",
      "\t\tword from annotation for sense 0 : 28355\n",
      "\t\tword from annotation for sense 0 : 69419\n",
      "\t\t\tword  69419 is in output for sense 0 with probability: 0.16273584905660377 and weight: 0.3333333333333333\n",
      "\t\tword from annotation for sense 0 : 57460\n",
      "\t\tword from annotation for sense 0 : 114587\n",
      "\t\tword from annotation for sense 0 : 42071\n",
      "\t\tword from annotation for sense 0 : 35267\n",
      "\t\tword from annotation for sense 0 : 51647\n",
      "\t\tword from annotation for sense 0 : 64448\n",
      "\t\tword from annotation for sense 0 : 45980\n",
      "\t\t\tword  45980 is in output for sense 0 with probability: 0.042452830188679236 and weight: 0.5\n",
      "\t\tword from annotation for sense 0 : 53826\n",
      "\texpert sense number  2 mus-1\n",
      "\t\tword from annotation for sense 0 : 28355\n",
      "\t\t\tword  28355 is in output for sense 0 with probability: 0.2075471698113207 and weight: 1.0\n",
      "\t\tword from annotation for sense 0 : 69419\n",
      "\t\t\tword  69419 is in output for sense 0 with probability: 0.16273584905660377 and weight: 0.3333333333333333\n",
      "\t\tword from annotation for sense 0 : 57460\n",
      "\t\tword from annotation for sense 0 : 114587\n",
      "\t\t\tword  114587 is in output for sense 0 with probability: 0.0990566037735849 and weight: 1.0\n",
      "\t\tword from annotation for sense 0 : 42071\n",
      "\t\tword from annotation for sense 0 : 35267\n",
      "\t\t\tword  35267 is in output for sense 0 with probability: 0.08254716981132075 and weight: 1.0\n",
      "\t\tword from annotation for sense 0 : 51647\n",
      "\t\t\tword  51647 is in output for sense 0 with probability: 0.08254716981132075 and weight: 1.0\n",
      "\t\tword from annotation for sense 0 : 64448\n",
      "\t\t\tword  64448 is in output for sense 0 with probability: 0.05424528301886792 and weight: 1.0\n",
      "\t\tword from annotation for sense 0 : 45980\n",
      "\t\t\tword  45980 is in output for sense 0 with probability: 0.042452830188679236 and weight: 0.5\n",
      "\t\tword from annotation for sense 0 : 53826\n",
      "output sense 1\n",
      "\texpert sense number  0 mus-4\n",
      "\t\tword from annotation for sense 1 : 79223\n",
      "\t\tword from annotation for sense 1 : 92927\n",
      "\t\tword from annotation for sense 1 : 46574\n",
      "\t\tword from annotation for sense 1 : 67660\n",
      "\t\tword from annotation for sense 1 : 103085\n",
      "\t\t\tword  103085 is in output for sense 1 with probability: 0.09863013698630134 and weight: 1.0\n",
      "\t\tword from annotation for sense 1 : 86112\n",
      "\t\tword from annotation for sense 1 : 101982\n",
      "\t\tword from annotation for sense 1 : 75808\n",
      "\t\tword from annotation for sense 1 : 68539\n",
      "\t\tword from annotation for sense 1 : 54607\n",
      "\texpert sense number  1 mus-2\n",
      "\t\tword from annotation for sense 1 : 79223\n",
      "\t\tword from annotation for sense 1 : 92927\n",
      "\t\tword from annotation for sense 1 : 46574\n",
      "\t\tword from annotation for sense 1 : 67660\n",
      "\t\tword from annotation for sense 1 : 103085\n",
      "\t\tword from annotation for sense 1 : 86112\n",
      "\t\tword from annotation for sense 1 : 101982\n",
      "\t\t\tword  101982 is in output for sense 1 with probability: 0.07945205479452053 and weight: 1.0\n",
      "\t\tword from annotation for sense 1 : 75808\n",
      "\t\tword from annotation for sense 1 : 68539\n",
      "\t\t\tword  68539 is in output for sense 1 with probability: 0.06575342465753423 and weight: 1.0\n",
      "\t\tword from annotation for sense 1 : 54607\n",
      "\texpert sense number  2 mus-1\n",
      "\t\tword from annotation for sense 1 : 79223\n",
      "\t\t\tword  79223 is in output for sense 1 with probability: 0.17260273972602735 and weight: 1.0\n",
      "\t\tword from annotation for sense 1 : 92927\n",
      "\t\tword from annotation for sense 1 : 46574\n",
      "\t\tword from annotation for sense 1 : 67660\n",
      "\t\tword from annotation for sense 1 : 103085\n",
      "\t\tword from annotation for sense 1 : 86112\n",
      "\t\tword from annotation for sense 1 : 101982\n",
      "\t\tword from annotation for sense 1 : 75808\n",
      "\t\tword from annotation for sense 1 : 68539\n",
      "\t\tword from annotation for sense 1 : 54607\n",
      "output sense 2\n",
      "\texpert sense number  0 mus-4\n",
      "\t\tword from annotation for sense 2 : 62258\n",
      "\t\tword from annotation for sense 2 : 64586\n",
      "\t\tword from annotation for sense 2 : 58271\n",
      "\t\tword from annotation for sense 2 : nlsj86871\n",
      "\t\tword from annotation for sense 2 : 101851\n",
      "\t\t\tword  101851 is in output for sense 2 with probability: 0.09558823529411764 and weight: 1.0\n",
      "\t\tword from annotation for sense 2 : nlsj183\n",
      "\t\tword from annotation for sense 2 : 75653\n",
      "\t\tword from annotation for sense 2 : nlsj59923\n",
      "\t\tword from annotation for sense 2 : 70105\n",
      "\t\tword from annotation for sense 2 : 82758\n",
      "\texpert sense number  1 mus-2\n",
      "\t\tword from annotation for sense 2 : 62258\n",
      "\t\t\tword  62258 is in output for sense 2 with probability: 0.19607843137254902 and weight: 1.0\n",
      "\t\tword from annotation for sense 2 : 64586\n",
      "\t\tword from annotation for sense 2 : 58271\n",
      "\t\t\tword  58271 is in output for sense 2 with probability: 0.11764705882352941 and weight: 1.0\n",
      "\t\tword from annotation for sense 2 : nlsj86871\n",
      "\t\t\tword  nlsj86871 is in output for sense 2 with probability: 0.10049019607843138 and weight: 1.0\n",
      "\t\tword from annotation for sense 2 : 101851\n",
      "\t\tword from annotation for sense 2 : nlsj183\n",
      "\t\tword from annotation for sense 2 : 75653\n",
      "\t\t\tword  75653 is in output for sense 2 with probability: 0.07598039215686274 and weight: 1.0\n",
      "\t\tword from annotation for sense 2 : nlsj59923\n",
      "\t\tword from annotation for sense 2 : 70105\n",
      "\t\tword from annotation for sense 2 : 82758\n",
      "\t\t\tword  82758 is in output for sense 2 with probability: 0.058823529411764705 and weight: 0.5\n",
      "\texpert sense number  2 mus-1\n",
      "\t\tword from annotation for sense 2 : 62258\n",
      "\t\tword from annotation for sense 2 : 64586\n",
      "\t\t\tword  64586 is in output for sense 2 with probability: 0.12745098039215685 and weight: 1.0\n",
      "\t\tword from annotation for sense 2 : 58271\n",
      "\t\tword from annotation for sense 2 : nlsj86871\n",
      "\t\tword from annotation for sense 2 : 101851\n",
      "\t\tword from annotation for sense 2 : nlsj183\n",
      "\t\tword from annotation for sense 2 : 75653\n",
      "\t\tword from annotation for sense 2 : nlsj59923\n",
      "\t\t\tword  nlsj59923 is in output for sense 2 with probability: 0.07352941176470587 and weight: 1.0\n",
      "\t\tword from annotation for sense 2 : 70105\n",
      "\t\t\tword  70105 is in output for sense 2 with probability: 0.06372549019607843 and weight: 1.0\n",
      "\t\tword from annotation for sense 2 : 82758\n",
      "\t\t\tword  82758 is in output for sense 2 with probability: 0.058823529411764705 and weight: 0.5\n",
      "output sense 3\n",
      "\texpert sense number  0 mus-4\n",
      "\t\tword from annotation for sense 3 : 102000\n",
      "\t\t\tword  102000 is in output for sense 3 with probability: 0.14893617021276598 and weight: 0.5\n",
      "\t\tword from annotation for sense 3 : 70495\n",
      "\t\t\tword  70495 is in output for sense 3 with probability: 0.14893617021276598 and weight: 1.0\n",
      "\t\tword from annotation for sense 3 : 7182\n",
      "\t\tword from annotation for sense 3 : 70958\n",
      "\t\tword from annotation for sense 3 : 49589\n",
      "\t\tword from annotation for sense 3 : 83174\n",
      "\t\t\tword  83174 is in output for sense 3 with probability: 0.08936170212765958 and weight: 1.0\n",
      "\t\tword from annotation for sense 3 : 106676\n",
      "\t\t\tword  106676 is in output for sense 3 with probability: 0.08085106382978724 and weight: 1.0\n",
      "\t\tword from annotation for sense 3 : 22209\n",
      "\t\t\tword  22209 is in output for sense 3 with probability: 0.07659574468085106 and weight: 0.5\n",
      "\t\tword from annotation for sense 3 : 93388\n",
      "\t\t\tword  93388 is in output for sense 3 with probability: 0.0723404255319149 and weight: 0.3333333333333333\n",
      "\t\tword from annotation for sense 3 : 16132\n",
      "\texpert sense number  1 mus-2\n",
      "\t\tword from annotation for sense 3 : 102000\n",
      "\t\tword from annotation for sense 3 : 70495\n",
      "\t\tword from annotation for sense 3 : 7182\n",
      "\t\tword from annotation for sense 3 : 70958\n",
      "\t\tword from annotation for sense 3 : 49589\n",
      "\t\tword from annotation for sense 3 : 83174\n",
      "\t\tword from annotation for sense 3 : 106676\n",
      "\t\tword from annotation for sense 3 : 22209\n",
      "\t\tword from annotation for sense 3 : 93388\n",
      "\t\t\tword  93388 is in output for sense 3 with probability: 0.0723404255319149 and weight: 0.3333333333333333\n",
      "\t\tword from annotation for sense 3 : 16132\n",
      "\texpert sense number  2 mus-1\n",
      "\t\tword from annotation for sense 3 : 102000\n",
      "\t\t\tword  102000 is in output for sense 3 with probability: 0.14893617021276598 and weight: 0.5\n",
      "\t\tword from annotation for sense 3 : 70495\n",
      "\t\tword from annotation for sense 3 : 7182\n",
      "\t\t\tword  7182 is in output for sense 3 with probability: 0.12340425531914895 and weight: 1.0\n",
      "\t\tword from annotation for sense 3 : 70958\n",
      "\t\t\tword  70958 is in output for sense 3 with probability: 0.1021276595744681 and weight: 1.0\n",
      "\t\tword from annotation for sense 3 : 49589\n",
      "\t\t\tword  49589 is in output for sense 3 with probability: 0.08936170212765958 and weight: 1.0\n",
      "\t\tword from annotation for sense 3 : 83174\n",
      "\t\tword from annotation for sense 3 : 106676\n",
      "\t\tword from annotation for sense 3 : 22209\n",
      "\t\t\tword  22209 is in output for sense 3 with probability: 0.07659574468085106 and weight: 0.5\n",
      "\t\tword from annotation for sense 3 : 93388\n",
      "\t\t\tword  93388 is in output for sense 3 with probability: 0.0723404255319149 and weight: 0.3333333333333333\n",
      "\t\tword from annotation for sense 3 : 16132\n",
      "output sense 4\n",
      "\texpert sense number  0 mus-4\n",
      "\t\tword from annotation for sense 4 : nlsj5634\n",
      "\t\tword from annotation for sense 4 : 61925\n",
      "\t\tword from annotation for sense 4 : 12620\n",
      "\t\tword from annotation for sense 4 : 69419\n",
      "\t\t\tword  69419 is in output for sense 4 with probability: 0.10150375939849625 and weight: 0.3333333333333333\n",
      "\t\tword from annotation for sense 4 : 104421\n",
      "\t\tword from annotation for sense 4 : 91085\n",
      "\t\tword from annotation for sense 4 : 71308\n",
      "\t\tword from annotation for sense 4 : 115748\n",
      "\t\t\tword  115748 is in output for sense 4 with probability: 0.07894736842105265 and weight: 1.0\n",
      "\t\tword from annotation for sense 4 : 19641\n",
      "\t\tword from annotation for sense 4 : 5390\n",
      "\texpert sense number  1 mus-2\n",
      "\t\tword from annotation for sense 4 : nlsj5634\n",
      "\t\t\tword  nlsj5634 is in output for sense 4 with probability: 0.14661654135338348 and weight: 1.0\n",
      "\t\tword from annotation for sense 4 : 61925\n",
      "\t\tword from annotation for sense 4 : 12620\n",
      "\t\tword from annotation for sense 4 : 69419\n",
      "\t\t\tword  69419 is in output for sense 4 with probability: 0.10150375939849625 and weight: 0.3333333333333333\n",
      "\t\tword from annotation for sense 4 : 104421\n",
      "\t\tword from annotation for sense 4 : 91085\n",
      "\t\tword from annotation for sense 4 : 71308\n",
      "\t\tword from annotation for sense 4 : 115748\n",
      "\t\tword from annotation for sense 4 : 19641\n",
      "\t\t\tword  19641 is in output for sense 4 with probability: 0.07518796992481204 and weight: 0.5\n",
      "\t\tword from annotation for sense 4 : 5390\n",
      "\texpert sense number  2 mus-1\n",
      "\t\tword from annotation for sense 4 : nlsj5634\n",
      "\t\tword from annotation for sense 4 : 61925\n",
      "\t\t\tword  61925 is in output for sense 4 with probability: 0.13909774436090228 and weight: 1.0\n",
      "\t\tword from annotation for sense 4 : 12620\n",
      "\t\tword from annotation for sense 4 : 69419\n",
      "\t\t\tword  69419 is in output for sense 4 with probability: 0.10150375939849625 and weight: 0.3333333333333333\n",
      "\t\tword from annotation for sense 4 : 104421\n",
      "\t\t\tword  104421 is in output for sense 4 with probability: 0.10150375939849625 and weight: 1.0\n",
      "\t\tword from annotation for sense 4 : 91085\n",
      "\t\t\tword  91085 is in output for sense 4 with probability: 0.09022556390977446 and weight: 1.0\n",
      "\t\tword from annotation for sense 4 : 71308\n",
      "\t\t\tword  71308 is in output for sense 4 with probability: 0.08270676691729324 and weight: 1.0\n",
      "\t\tword from annotation for sense 4 : 115748\n",
      "\t\tword from annotation for sense 4 : 19641\n",
      "\t\t\tword  19641 is in output for sense 4 with probability: 0.07518796992481204 and weight: 0.5\n",
      "\t\tword from annotation for sense 4 : 5390\n"
     ]
    }
   ],
   "source": [
    "for key in k_words_with_prob.keys():\n",
    "    print(\"output sense\",key)\n",
    "    for i in range(0,number_of_s):\n",
    "        print(\"\\texpert sense number \", i, expert_senses[i])\n",
    "        for second_key in k_words_with_prob[key].keys(): # Barbara's note: shouldn't it be k_words_with_prob[i] here?\n",
    "            print(\"\\t\\tword from annotation for sense\", key, \":\", second_key)\n",
    "            if second_key in dict_of_words[expert_senses[i]]:\n",
    "                print(\"\\t\\t\\tword \", second_key, \"is in output for sense\", key, \"with probability:\", k_words_with_prob[key][second_key], \"and weight:\", word_weight[second_key])\n",
    "\n",
    "                \n",
    "# Here we get all the senses and for each sense we do a matching between the k words and s words and get the probability\n",
    "# For some reason the first word for each sense arrives several times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of expert senses s: 3\n",
      "number of model output senses k: 5\n",
      "\n",
      "\n",
      "Choose best match for k = 0\n",
      "k = 0 \t s = 0 (= expert sense mus-4 )\t conf[k,s] = 0.7977594339622712\n",
      "k = 0 \t s = 1 (= expert sense mus-2 )\t conf[k,s] = 0.7655660377358563\n",
      "k = 0 \t s = 2 (= expert sense mus-1 )\t conf[k,s] = 0.8172169811320821\n",
      "\n",
      "\n",
      "Choose best match for k = 1\n",
      "k = 1 \t s = 0 (= expert sense mus-4 )\t conf[k,s] = 0.0147945205479452\n",
      "k = 1 \t s = 1 (= expert sense mus-2 )\t conf[k,s] = 0.04664383561643835\n",
      "k = 1 \t s = 2 (= expert sense mus-1 )\t conf[k,s] = 0.051780821917808216\n",
      "\n",
      "\n",
      "Choose best match for k = 2\n",
      "k = 2 \t s = 0 (= expert sense mus-4 )\t conf[k,s] = 0.03584558823529412\n",
      "k = 2 \t s = 1 (= expert sense mus-2 )\t conf[k,s] = 0.15091911764705893\n",
      "k = 2 \t s = 2 (= expert sense mus-1 )\t conf[k,s] = 0.03382352941176468\n",
      "\n",
      "\n",
      "Choose best match for k = 3\n",
      "k = 3 \t s = 0 (= expert sense mus-4 )\t conf[k,s] = 0.2665425531914896\n",
      "k = 3 \t s = 1 (= expert sense mus-2 )\t conf[k,s] = 0.0072340425531914904\n",
      "k = 3 \t s = 2 (= expert sense mus-1 )\t conf[k,s] = 0.1746276595744685\n",
      "\n",
      "\n",
      "Choose best match for k = 4\n",
      "k = 4 \t s = 0 (= expert sense mus-4 )\t conf[k,s] = 0.49229323308270717\n",
      "k = 4 \t s = 1 (= expert sense mus-2 )\t conf[k,s] = 0.5052631578947381\n",
      "k = 4 \t s = 2 (= expert sense mus-1 )\t conf[k,s] = 0.6067669172932353\n"
     ]
    }
   ],
   "source": [
    "## Calculating confidence score for each (words_of_k,words_of_s) pair\n",
    "\n",
    "# conf(k,s) = (p1*match(w1,s)+p2*match(w1,s)+px(wx,s))/10\n",
    "        # match(wx,s) =   1/number_of_senses_assigned_to_wx if s_is_one_of_them \n",
    "\n",
    "##### TODO: for now conf[k,s] is multiplied by the number of expert senses --- FIX\n",
    "    \n",
    "print(\"number of expert senses s:\",number_of_s)\n",
    "print(\"number of model output senses k:\",len(k_words_with_prob.keys()))\n",
    "compteur = 0\n",
    "\n",
    "match = dict()\n",
    "conf = dict()\n",
    "for k in k_words_with_prob.keys():  # for each output sense, we go through...\n",
    "    print(\"\\n\")\n",
    "    print(\"Choose best match for k =\",k)\n",
    "    for s in range(0,number_of_s):       # each expert sense\n",
    "        \n",
    "        conf[k,s] = 0 \n",
    "        \n",
    "        #print(\"expert sense\",s)\n",
    "        for mot in k_words_with_prob[k]:      # for each word within output by the model for the output sense\n",
    "            #print(k,mot)\n",
    "            #print(expert_senses[s])\n",
    "            \n",
    "            if mot in dict_of_words[expert_senses[s]]:  # if that word exists in the list of expert words for that sense\n",
    "                \n",
    "                #print(s,dict_of_words[expert_senses[s]])\n",
    "                #print(k_words_with_prob[k][mot])\n",
    "                \n",
    "                for word in list_of_all_words:  # this help getting a key for a dictionary later on\n",
    "                    if mot == word:\n",
    "                        match_weighted = float((k_words_with_prob[k][mot]))*word_weight[word] #this dictionary cfr comment on line 24\n",
    "                        # word_weight[word] is already \"1/number_of_expert_senses_assigned_to_this_word\"\n",
    "                        \n",
    "                        #print(\"sense\",expert_senses[s],\"word\",word,\"match_weighted\",match_weighted)\n",
    "                        \n",
    "                        #print(k,s,conf[k,s])\n",
    "\n",
    "                        \n",
    "                        # To fix? \n",
    "                        # The way the code works is that all matches happen number_of_s times\n",
    "                        # (number_of_s = number of expert senses)\n",
    "                        # easy fix is to divide the match score by number_of_s\n",
    "                        \n",
    "                        conf[k,s] = conf[k,s] + match_weighted/4\n",
    "                        \n",
    "                    #else: \n",
    "                        #print(word,\"has no match for sense\",expert_senses[s])\n",
    "                        #print(word,word_weight[word],\"match\",k_words_with_prob[k][mot],\"match weighted\",match_weighted)\n",
    "                    #print(\"test1\")\n",
    "                #print(\"test2\")\n",
    "                \n",
    "                    #compteur += 1\n",
    "                \n",
    "        if (k,s) in conf.keys():\n",
    "        \n",
    "            conf[k,s] = conf[k,s]/10 # with or without /10\n",
    "            print(\"k =\",k,\"\\t s =\",s,\"(= expert sense\",expert_senses[s],\")\\t conf[k,s] =\",conf[k,s])\n",
    "            \n",
    "            #print(compteur)\n",
    "            \n",
    "    #print(k_words_with_prob[k])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Barbara's note: \"key\" and \"i\" may be different senses; for example, for key=0, this is the first sense in the output, and i=0 is the first sense annotated by the expert. I think what we want here is to try matching all key values with all i values; for each (key, i) pair, we get the conf(key, i), as in pages 5 ff. of the Goals and plan.docx document.\n",
    "\n",
    "Then, when possible, we can pick the best \"i\" for each \"key\"; we haven't yet decided how, but it will probably have to do with the maximum conf value.\n",
    "\n",
    "Once we have a key-->i mapping, we can calculate precision and recall.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: How well does the model assign the right words to a given sense of the target word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-61-f315d04cacf9>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-61-f315d04cacf9>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    for each k:\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# For each k, we use the words given by the expert as unquestionable truth.\n",
    "# Judging the model's assignment of words to a given sense becomes a question of precision and recall.\n",
    "\n",
    "# precision is all correct w weighted by their respective probabilities / all w weighted by their probabilities\n",
    "\n",
    "for each k:\n",
    "    for each w:\n",
    "        if w in expert_list:\n",
    "            w_weight = p*1\n",
    "            numerator += w_weight\n",
    "        w_weight = p*1\n",
    "        denominator += w_weight\n",
    "    precision = numerator/denominator\n",
    "    \n",
    "# recall is all correct w weighted by their respective probabilities / all w assigned to the sense by the expert\n",
    "for each k:\n",
    "    for each w:\n",
    "        if w in expert_list:\n",
    "            w_weight = p*1\n",
    "            numerator += w_weight\n",
    "    denominator = len(expert_list)\n",
    "    recall = numerator/denominator\n",
    "    \n",
    "# f-score can be used as well\n",
    "\n",
    "for each k:\n",
    "    f_score = 2 * precision * recall / (precision+recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For pair ks 0,0 :\n",
      "The RECALL is 3.915094339622641 / 337 = 0.01161749062202564\n",
      "The PRECISION is 0.3915094339622641 / 0.9999999999999999 = 0.3915094339622642 \n",
      "\n",
      "The F-SCORE is 0.022565385242781788 \n",
      "\n",
      "For pair ks 0,1 :\n",
      "The RECALL is 2.05188679245283 / 225 = 0.009119496855345912\n",
      "The PRECISION is 0.205188679245283 / 0.9999999999999999 = 0.20518867924528303 \n",
      "\n",
      "The F-SCORE is 0.01746286631874749 \n",
      "\n",
      "For pair ks 0,2 :\n",
      "The RECALL is 7.311320754716979 / 836 = 0.008745598988895908\n",
      "The PRECISION is 0.7311320754716979 / 0.9999999999999999 = 0.731132075471698 \n",
      "\n",
      "The F-SCORE is 0.01728444622864534 \n",
      "\n",
      "For pair ks 1,0 :\n",
      "The RECALL is 0.9863013698630134 / 337 = 0.0029267102963294166\n",
      "The PRECISION is 0.09863013698630134 / 0.9999999999999997 = 0.09863013698630137 \n",
      "\n",
      "The F-SCORE is 0.005684734120247915 \n",
      "\n",
      "For pair ks 1,1 :\n",
      "The RECALL is 1.4520547945205475 / 225 = 0.006453576864535767\n",
      "The PRECISION is 0.14520547945205475 / 0.9999999999999997 = 0.1452054794520548 \n",
      "\n",
      "The F-SCORE is 0.012357913144855725 \n",
      "\n",
      "For pair ks 1,2 :\n",
      "The RECALL is 1.7260273972602735 / 836 = 0.002064626073277839\n",
      "The PRECISION is 0.17260273972602735 / 0.9999999999999997 = 0.1726027397260274 \n",
      "\n",
      "The F-SCORE is 0.004080443019527833 \n",
      "\n",
      "For pair ks 2,0 :\n",
      "The RECALL is 0.9558823529411764 / 337 = 0.002836446151160761\n",
      "The PRECISION is 0.09558823529411764 / 0.9999999999999999 = 0.09558823529411765 \n",
      "\n",
      "The F-SCORE is 0.005509408374300728 \n",
      "\n",
      "For pair ks 2,1 :\n",
      "The RECALL is 5.490196078431372 / 225 = 0.024400871459694985\n",
      "The PRECISION is 0.5490196078431372 / 0.9999999999999999 = 0.5490196078431373 \n",
      "\n",
      "The F-SCORE is 0.046725073007926575 \n",
      "\n",
      "For pair ks 2,2 :\n",
      "The RECALL is 3.2352941176470584 / 836 = 0.0038699690402476776\n",
      "The PRECISION is 0.32352941176470584 / 0.9999999999999999 = 0.3235294117647059 \n",
      "\n",
      "The F-SCORE is 0.007648449450702266 \n",
      "\n",
      "For pair ks 3,0 :\n",
      "The RECALL is 6.170212765957448 / 337 = 0.018309236694235752\n",
      "The PRECISION is 0.6170212765957448 / 1.0000000000000002 = 0.6170212765957447 \n",
      "\n",
      "The F-SCORE is 0.0355631859709363 \n",
      "\n",
      "For pair ks 3,1 :\n",
      "The RECALL is 0.723404255319149 / 225 = 0.003215130023640662\n",
      "The PRECISION is 0.0723404255319149 / 1.0000000000000002 = 0.07234042553191489 \n",
      "\n",
      "The F-SCORE is 0.006156631960162971 \n",
      "\n",
      "For pair ks 3,2 :\n",
      "The RECALL is 6.127659574468086 / 836 = 0.0073297363330957965\n",
      "The PRECISION is 0.6127659574468086 / 1.0000000000000002 = 0.6127659574468085 \n",
      "\n",
      "The F-SCORE is 0.014486192847442283 \n",
      "\n",
      "For pair ks 4,0 :\n",
      "The RECALL is 1.804511278195489 / 337 = 0.005354632872983647\n",
      "The PRECISION is 0.18045112781954892 / 1.0 = 0.18045112781954892 \n",
      "\n",
      "The F-SCORE is 0.010400641372884663 \n",
      "\n",
      "For pair ks 4,1 :\n",
      "The RECALL is 3.2330827067669174 / 225 = 0.014369256474519632\n",
      "The PRECISION is 0.32330827067669177 / 1.0 = 0.32330827067669177 \n",
      "\n",
      "The F-SCORE is 0.027515597504399292 \n",
      "\n",
      "For pair ks 4,2 :\n",
      "The RECALL is 5.902255639097746 / 836 = 0.007060114400834624\n",
      "The PRECISION is 0.5902255639097745 / 1.0 = 0.5902255639097745 \n",
      "\n",
      "The F-SCORE is 0.01395332302387174 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now that we have prob + word for each k,s pair we can calculate precision and recall \n",
    "\n",
    "list_with_ks = list()  # this list stores the k,s matches found above\n",
    "list_with_ks = [\"0,0\",\"0,1\",\"0,2\",\"1,0\",\"1,1\",\"1,2\",\"2,0\",\"2,1\",\"2,2\",\"3,0\",\"3,1\",\"3,2\",\"4,0\",\"4,1\",\"4,2\"]\n",
    "\n",
    "\n",
    "for key in list_with_ks:\n",
    "    numerator_recall = 0\n",
    "    denominator_precision = 0\n",
    "    numerator_precision = 0\n",
    "    for word in k_words_with_prob[int(key[0])]: \n",
    "        w_weight_precision = k_words_with_prob[int(key[0])][word] * 1\n",
    "        if word in dict_of_words[expert_senses[int(key[2])]]:   \n",
    "            w_weight_recall = k_words_with_prob[int(key[0])][word] * 1\n",
    "            numerator_recall += float(w_weight_recall)\n",
    "            #w_weight_precision = k_words_with_prob[int(key[0])][word] * 1  this was moved above the if\n",
    "            # cfr Valerio's email from March 28\n",
    "            numerator_precision += float(w_weight_precision)\n",
    "    \n",
    "        \n",
    "        denominator_precision += float(w_weight_precision)\n",
    "    denominator_recall = len(dict_of_words[expert_senses[int(key[2])]])\n",
    "    numerator_recall = numerator_recall*10\n",
    "            \n",
    "    print(\"For pair ks\",key,\":\")\n",
    "    print(\"The RECALL is\",numerator_recall,\"/\",denominator_recall,\"=\",numerator_recall/denominator_recall)\n",
    "    if numerator_precision == 0:\n",
    "        print(\"The PRECISION IS NA\")\n",
    "    else:\n",
    "        print(\"The PRECISION is\",numerator_precision,\"/\",denominator_precision,\"=\",numerator_precision/denominator_precision,\"\\n\")\n",
    "    if (numerator_precision/denominator_precision)+(numerator_recall/denominator_recall) != 0:\n",
    "        print(\"The F-SCORE is\", (2*(numerator_precision/denominator_precision)*(numerator_recall/denominator_recall)/((numerator_precision/denominator_precision)+(numerator_recall/denominator_recall))),\"\\n\")\n",
    "    else:\n",
    "        print(\"No F-SCORE, can't divide by 0\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'28355': 0.2075471698113207, '69419': 0.16273584905660377, '57460': 0.1320754716981132, '114587': 0.0990566037735849, '42071': 0.09669811320754716, '35267': 0.08254716981132075, '51647': 0.08254716981132075, '64448': 0.05424528301886792, '45980': 0.042452830188679236, '53826': 0.04009433962264151}, 1: {'79223': 0.17260273972602735, '92927': 0.1561643835616438, '46574': 0.10410958904109587, '67660': 0.10410958904109587, '103085': 0.09863013698630134, '86112': 0.08219178082191778, '101982': 0.07945205479452053, '75808': 0.07123287671232874, '68539': 0.06575342465753423, '54607': 0.06575342465753423}, 2: {'62258': 0.19607843137254902, '64586': 0.12745098039215685, '58271': 0.11764705882352941, 'nlsj86871': 0.10049019607843138, '101851': 0.09558823529411764, 'nlsj183': 0.0906862745098039, '75653': 0.07598039215686274, 'nlsj59923': 0.07352941176470587, '70105': 0.06372549019607843, '82758': 0.058823529411764705}, 3: {'102000': 0.14893617021276598, '70495': 0.14893617021276598, '7182': 0.12340425531914895, '70958': 0.1021276595744681, '49589': 0.08936170212765958, '83174': 0.08936170212765958, '106676': 0.08085106382978724, '22209': 0.07659574468085106, '93388': 0.0723404255319149, '16132': 0.06808510638297872}, 4: {'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228, '12620': 0.11278195488721805, '69419': 0.10150375939849625, '104421': 0.10150375939849625, '91085': 0.09022556390977446, '71308': 0.08270676691729324, '115748': 0.07894736842105265, '19641': 0.07518796992481204, '5390': 0.07142857142857144}}\n"
     ]
    }
   ],
   "source": [
    "print(k_words_with_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mus-4', 'mus-2', 'mus-1']\n"
     ]
    }
   ],
   "source": [
    "print(expert_senses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in expert_senses:\n",
    "    #print(i,dict_of_words[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qx: Model(s) comparison againstannotated subcorpus (sense importance evolution + sense emergence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode\n",
    "\n",
    "1. Parse senses_target.txt to get:\n",
    "\n",
    "    1.1 the date\n",
    "    \n",
    "    1.2 the number of senses at that date\n",
    "    \n",
    "    1.3 the number of uses of each sense at that date\n",
    "    \n",
    "    \n",
    "2. Using the numbers found in 1.3, plot the emergence of new senses and the distribution of others\n",
    "\n",
    "\n",
    "confidence interval!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new earliest sense: -430\n",
      "new latest sense: -430\n",
      "new latest sense: -425\n",
      "new latest sense: -422\n",
      "new latest sense: -420\n",
      "new latest sense: -415\n",
      "new latest sense: -370\n",
      "new latest sense: -362\n",
      "new latest sense: -355\n",
      "new latest sense: -350\n",
      "new latest sense: -335\n",
      "new latest sense: -300\n",
      "new latest sense: -270\n",
      "new latest sense: -250\n",
      "new latest sense: -150\n",
      "new latest sense: -35\n",
      "new latest sense: -7\n",
      "new latest sense: 90\n",
      "new latest sense: 93\n",
      "new latest sense: 95\n",
      "new latest sense: 100\n",
      "new latest sense: 108\n",
      "new latest sense: 150\n",
      "new latest sense: 170\n",
      "new latest sense: 175\n",
      "new latest sense: 176\n",
      "new latest sense: 180\n",
      "new latest sense: 185\n",
      "new latest sense: 195\n",
      "new latest sense: 200\n",
      "new latest sense: 220\n",
      "new latest sense: 228\n",
      "new latest sense: 230\n",
      "new latest sense: 238\n",
      "new latest sense: 359\n"
     ]
    }
   ],
   "source": [
    "# this is for the expert senses (gold standard truth of sense predominance in our corpus)\n",
    "\n",
    "\n",
    "expert_senses_chart = list() # list where we store all sense ids provided by expert\n",
    "sense_year = dict()\n",
    "earliest_sense = 10000 #just to be safe\n",
    "latest_sense = -10000\n",
    "\n",
    "for s in file_senses:\n",
    "    s = s.split(\"\\t\")\n",
    "    sense = s[11] # The sense ID is after the 10th tab\n",
    "    \n",
    "    \n",
    "    try: \n",
    "        sense_year[sense].append(int(s[0])) # The date is the first tab\n",
    "    except KeyError:\n",
    "        sense_year[sense] = list()\n",
    "        sense_year[sense].append(int(s[0])) # The date is the first tab\n",
    "    \n",
    "    expert_senses_chart.append(sense)\n",
    "\n",
    "    if int(s[0]) < earliest_sense:\n",
    "        earliest_sense = int(s[0])\n",
    "        print(\"new earliest sense:\",earliest_sense)\n",
    "        \n",
    "    if int(s[0]) > latest_sense:\n",
    "        latest_sense = int(s[0])\n",
    "        print(\"new latest sense:\",latest_sense)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating the slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.71428571428571\n",
      "-430 359\n"
     ]
    }
   ],
   "source": [
    "number_of_slices = 7 # that's what the model outputs now\n",
    "slice_duration = (latest_sense - earliest_sense)/number_of_slices\n",
    "print(slice_duration)\n",
    "print(earliest_sense,latest_sense)\n",
    "slice_years = dict()\n",
    "\n",
    "for period in range(0,number_of_slices):\n",
    "    slice_years[period] = list()\n",
    "    \n",
    "    for i in range(earliest_sense,latest_sense):\n",
    "        if i > int(period*slice_duration) + earliest_sense:\n",
    "            if i < int((period+1)*slice_duration) + earliest_sense:\n",
    "                slice_years[period].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "-430 359\n"
     ]
    }
   ],
   "source": [
    "number_of_slices = 7 # that's what the model outputs now\n",
    "slice_duration = 100 # thats what we've decided\n",
    "print(slice_duration)\n",
    "print(earliest_sense,latest_sense)\n",
    "slice_years = dict()\n",
    "\n",
    "for period in range(0,number_of_slices):\n",
    "    slice_years[period] = list()\n",
    "    \n",
    "    if period == number_of_slices-1:\n",
    "        for i in range(latest_i,latest_sense):\n",
    "            slice_years[period].append(i)  \n",
    "    \n",
    "    if period != number_of_slices-1:\n",
    "        for i in range(earliest_sense,latest_sense):\n",
    "        \n",
    "            if i > int(period*slice_duration) + earliest_sense:\n",
    "                if i < int((period+1)*slice_duration) + earliest_sense:\n",
    "                    slice_years[period].append(i)\n",
    "                    latest_i = i\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [-429, -428, -427, -426, -425, -424, -423, -422, -421, -420, -419, -418, -417, -416, -415, -414, -413, -412, -411, -410, -409, -408, -407, -406, -405, -404, -403, -402, -401, -400, -399, -398, -397, -396, -395, -394, -393, -392, -391, -390, -389, -388, -387, -386, -385, -384, -383, -382, -381, -380, -379, -378, -377, -376, -375, -374, -373, -372, -371, -370, -369, -368, -367, -366, -365, -364, -363, -362, -361, -360, -359, -358, -357, -356, -355, -354, -353, -352, -351, -350, -349, -348, -347, -346, -345, -344, -343, -342, -341, -340, -339, -338, -337, -336, -335, -334, -333, -332, -331] \n",
      "\n",
      "1 [-329, -328, -327, -326, -325, -324, -323, -322, -321, -320, -319, -318, -317, -316, -315, -314, -313, -312, -311, -310, -309, -308, -307, -306, -305, -304, -303, -302, -301, -300, -299, -298, -297, -296, -295, -294, -293, -292, -291, -290, -289, -288, -287, -286, -285, -284, -283, -282, -281, -280, -279, -278, -277, -276, -275, -274, -273, -272, -271, -270, -269, -268, -267, -266, -265, -264, -263, -262, -261, -260, -259, -258, -257, -256, -255, -254, -253, -252, -251, -250, -249, -248, -247, -246, -245, -244, -243, -242, -241, -240, -239, -238, -237, -236, -235, -234, -233, -232, -231] \n",
      "\n",
      "2 [-229, -228, -227, -226, -225, -224, -223, -222, -221, -220, -219, -218, -217, -216, -215, -214, -213, -212, -211, -210, -209, -208, -207, -206, -205, -204, -203, -202, -201, -200, -199, -198, -197, -196, -195, -194, -193, -192, -191, -190, -189, -188, -187, -186, -185, -184, -183, -182, -181, -180, -179, -178, -177, -176, -175, -174, -173, -172, -171, -170, -169, -168, -167, -166, -165, -164, -163, -162, -161, -160, -159, -158, -157, -156, -155, -154, -153, -152, -151, -150, -149, -148, -147, -146, -145, -144, -143, -142, -141, -140, -139, -138, -137, -136, -135, -134, -133, -132, -131] \n",
      "\n",
      "3 [-129, -128, -127, -126, -125, -124, -123, -122, -121, -120, -119, -118, -117, -116, -115, -114, -113, -112, -111, -110, -109, -108, -107, -106, -105, -104, -103, -102, -101, -100, -99, -98, -97, -96, -95, -94, -93, -92, -91, -90, -89, -88, -87, -86, -85, -84, -83, -82, -81, -80, -79, -78, -77, -76, -75, -74, -73, -72, -71, -70, -69, -68, -67, -66, -65, -64, -63, -62, -61, -60, -59, -58, -57, -56, -55, -54, -53, -52, -51, -50, -49, -48, -47, -46, -45, -44, -43, -42, -41, -40, -39, -38, -37, -36, -35, -34, -33, -32, -31] \n",
      "\n",
      "4 [-29, -28, -27, -26, -25, -24, -23, -22, -21, -20, -19, -18, -17, -16, -15, -14, -13, -12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69] \n",
      "\n",
      "5 [71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169] \n",
      "\n",
      "6 [169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in slice_years.keys():\n",
    "    print(key,slice_years[key],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting the number of hits per sense per period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sense: mus-1\n",
      "mus-1 106\n",
      "Sense: mus-4\n",
      "mus-4 35\n",
      "Sense: w\n",
      "w 7\n",
      "Sense: mus-2\n",
      "mus-2 24\n",
      "{('mus-1', 0): 10, ('mus-1', 1): 13, ('mus-1', 2): 17, ('mus-1', 3): 20, ('mus-1', 4): 28, ('mus-1', 5): 55, ('mus-1', 6): 106, ('mus-4', 0): 22, ('mus-4', 1): 23, ('mus-4', 2): 23, ('mus-4', 3): 25, ('mus-4', 4): 25, ('mus-4', 5): 26, ('mus-4', 6): 35, ('w', 0): 2, ('w', 1): 2, ('w', 2): 2, ('w', 3): 2, ('w', 4): 2, ('w', 5): 4, ('w', 6): 7, ('mus-2', 0): 0, ('mus-2', 1): 0, ('mus-2', 2): 0, ('mus-2', 3): 0, ('mus-2', 4): 0, ('mus-2', 5): 0, ('mus-2', 6): 24}\n"
     ]
    }
   ],
   "source": [
    "sense_date_amount = dict()\n",
    "\n",
    "for sense in sense_year.keys():\n",
    "   \n",
    "    print(\"Sense:\",sense)\n",
    "    counter = 0\n",
    "    for i in range(0,number_of_slices):\n",
    "        #print(\"period\",i,\"years for that sense in that period\",sense_year[sense])\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(sense_year[sense])\n",
    "        for year in sense_year[sense]:\n",
    "        \n",
    "            if year in slice_years[i]:\n",
    "                counter += 1\n",
    "                #print(sense_year[sense][i])\n",
    "                \n",
    "        sense_date_amount[sense,i] = counter           \n",
    "    print(sense,counter)\n",
    "    \n",
    "print(sense_date_amount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting the relative number of hits per sense per period\n",
    "(for plotting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total period {0: 32, 1: 36, 2: 40, 3: 45, 4: 53, 5: 81, 6: 165}\n",
      "sense date amount {('mus-1', 0): 10, ('mus-1', 1): 13, ('mus-1', 2): 17, ('mus-1', 3): 20, ('mus-1', 4): 28, ('mus-1', 5): 55, ('mus-1', 6): 106, ('mus-4', 0): 22, ('mus-4', 1): 23, ('mus-4', 2): 23, ('mus-4', 3): 25, ('mus-4', 4): 25, ('mus-4', 5): 26, ('mus-4', 6): 35, ('w', 0): 2, ('w', 1): 2, ('w', 2): 2, ('w', 3): 2, ('w', 4): 2, ('w', 5): 4, ('w', 6): 7, ('mus-2', 0): 0, ('mus-2', 1): 0, ('mus-2', 2): 0, ('mus-2', 3): 0, ('mus-2', 4): 0, ('mus-2', 5): 0, ('mus-2', 6): 24}\n",
      "('mus-1', 0) total for this sense at this period 10 total period 32\n",
      "relative 0.3125\n",
      "('mus-1', 1) total for this sense at this period 13 total period 36\n",
      "relative 0.3611111111111111\n",
      "('mus-1', 2) total for this sense at this period 17 total period 40\n",
      "relative 0.425\n",
      "('mus-1', 3) total for this sense at this period 20 total period 45\n",
      "relative 0.4444444444444444\n",
      "('mus-1', 4) total for this sense at this period 28 total period 53\n",
      "relative 0.5283018867924528\n",
      "('mus-1', 5) total for this sense at this period 55 total period 81\n",
      "relative 0.6790123456790124\n",
      "('mus-1', 6) total for this sense at this period 106 total period 165\n",
      "relative 0.6424242424242425\n",
      "('mus-4', 0) total for this sense at this period 22 total period 32\n",
      "relative 0.6875\n",
      "('mus-4', 1) total for this sense at this period 23 total period 36\n",
      "relative 0.6388888888888888\n",
      "('mus-4', 2) total for this sense at this period 23 total period 40\n",
      "relative 0.575\n",
      "('mus-4', 3) total for this sense at this period 25 total period 45\n",
      "relative 0.5555555555555556\n",
      "('mus-4', 4) total for this sense at this period 25 total period 53\n",
      "relative 0.4716981132075472\n",
      "('mus-4', 5) total for this sense at this period 26 total period 81\n",
      "relative 0.32098765432098764\n",
      "('mus-4', 6) total for this sense at this period 35 total period 165\n",
      "relative 0.21212121212121213\n",
      "('w', 0) total for this sense at this period 2 total period 32\n",
      "relative 0.0625\n",
      "('w', 1) total for this sense at this period 2 total period 36\n",
      "relative 0.05555555555555555\n",
      "('w', 2) total for this sense at this period 2 total period 40\n",
      "relative 0.05\n",
      "('w', 3) total for this sense at this period 2 total period 45\n",
      "relative 0.044444444444444446\n",
      "('w', 4) total for this sense at this period 2 total period 53\n",
      "relative 0.03773584905660377\n",
      "('w', 5) total for this sense at this period 4 total period 81\n",
      "relative 0.04938271604938271\n",
      "('w', 6) total for this sense at this period 7 total period 165\n",
      "relative 0.04242424242424243\n",
      "('mus-2', 0) total for this sense at this period 0 total period 32\n",
      "relative 0.0\n",
      "('mus-2', 1) total for this sense at this period 0 total period 36\n",
      "relative 0.0\n",
      "('mus-2', 2) total for this sense at this period 0 total period 40\n",
      "relative 0.0\n",
      "('mus-2', 3) total for this sense at this period 0 total period 45\n",
      "relative 0.0\n",
      "('mus-2', 4) total for this sense at this period 0 total period 53\n",
      "relative 0.0\n",
      "('mus-2', 5) total for this sense at this period 0 total period 81\n",
      "relative 0.0\n",
      "('mus-2', 6) total for this sense at this period 24 total period 165\n",
      "relative 0.14545454545454545\n",
      "{('mus-1', 0): 0.3125, ('mus-1', 1): 0.3611111111111111, ('mus-1', 2): 0.425, ('mus-1', 3): 0.4444444444444444, ('mus-1', 4): 0.5283018867924528, ('mus-1', 5): 0.6790123456790124, ('mus-1', 6): 0.6424242424242425, ('mus-4', 0): 0.6875, ('mus-4', 1): 0.6388888888888888, ('mus-4', 2): 0.575, ('mus-4', 3): 0.5555555555555556, ('mus-4', 4): 0.4716981132075472, ('mus-4', 5): 0.32098765432098764, ('mus-4', 6): 0.21212121212121213, ('w', 0): 0.0625, ('w', 1): 0.05555555555555555, ('w', 2): 0.05, ('w', 3): 0.044444444444444446, ('w', 4): 0.03773584905660377, ('w', 5): 0.04938271604938271, ('w', 6): 0.04242424242424243, ('mus-2', 0): 0.0, ('mus-2', 1): 0.0, ('mus-2', 2): 0.0, ('mus-2', 3): 0.0, ('mus-2', 4): 0.0, ('mus-2', 5): 0.0, ('mus-2', 6): 0.14545454545454545}\n",
      "mus-4 0 0.6875\n",
      "mus-2 0 0.0\n",
      "mus-1 0 0.3125\n",
      "mus-4 1 0.6388888888888888\n",
      "mus-2 1 0.0\n",
      "mus-1 1 0.3611111111111111\n",
      "mus-4 2 0.575\n",
      "mus-2 2 0.0\n",
      "mus-1 2 0.425\n",
      "mus-4 3 0.5555555555555556\n",
      "mus-2 3 0.0\n",
      "mus-1 3 0.4444444444444444\n",
      "mus-4 4 0.4716981132075472\n",
      "mus-2 4 0.0\n",
      "mus-1 4 0.5283018867924528\n",
      "mus-4 5 0.32098765432098764\n",
      "mus-2 5 0.0\n",
      "mus-1 5 0.6790123456790124\n",
      "mus-4 6 0.21212121212121213\n",
      "mus-2 6 0.14545454545454545\n",
      "mus-1 6 0.6424242424242425\n",
      "{0: [0.6875, 0.0, 0.3125], 1: [0.6388888888888888, 0.0, 0.3611111111111111], 2: [0.575, 0.0, 0.425], 3: [0.5555555555555556, 0.0, 0.4444444444444444], 4: [0.4716981132075472, 0.0, 0.5283018867924528], 5: [0.32098765432098764, 0.0, 0.6790123456790124], 6: [0.21212121212121213, 0.14545454545454545, 0.6424242424242425]}\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "total_period = dict()\n",
    "N = number_of_slices\n",
    "\n",
    "sense_period_relative = dict()\n",
    "\n",
    "for i in range(0,number_of_slices):\n",
    "    for entry in expert_senses:\n",
    "        \n",
    "# for period i we store for each sense the number of times the sense is seen\n",
    "        \n",
    "        try:\n",
    "            total_period[i] += sense_date_amount[entry,i]\n",
    "        except KeyError:\n",
    "            total_period[i] = 0\n",
    "            total_period[i] += sense_date_amount[entry,i]\n",
    "            \n",
    "        #print(i,entry,\"+\",sense_date_amount[entry,i],\"=\",total_period[i])\n",
    "        \n",
    "        \n",
    "print(\"total period\",total_period)\n",
    "print(\"sense date amount\",sense_date_amount)\n",
    "        \n",
    "for key in sense_date_amount:\n",
    "    \n",
    "    # for each (sense,period) pair we divide the number by the total number of words at that period\n",
    "    \n",
    "    print(key,\"total for this sense at this period\",sense_date_amount[key],\"total period\",total_period[key[1]])\n",
    "    \n",
    "    sense_period_relative[key] = float(sense_date_amount[key]/total_period[key[1]])\n",
    "    print(\"relative\",sense_period_relative[key])\n",
    "  \n",
    "print(sense_period_relative)        \n",
    "\n",
    "period_relative = dict()\n",
    "temp_list = list()\n",
    "\n",
    "for i in range(0,number_of_slices):\n",
    "    temp_list = list()\n",
    "    for entry in expert_senses:\n",
    "        if len(temp_list) < len(expert_senses):\n",
    "            temp_list.append(sense_period_relative[entry,i])\n",
    "            print(entry,i,sense_period_relative[entry,i])\n",
    "        \n",
    "    period_relative[i] = temp_list\n",
    "        \n",
    "        \n",
    "print(period_relative)\n",
    "print(number_of_slices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting expert annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [68, 0, 31]\n",
      "1 [63, 0, 36]\n",
      "2 [57, 0, 42]\n",
      "3 [55, 0, 44]\n",
      "4 [47, 0, 52]\n",
      "5 [32, 0, 67]\n",
      "6 [21, 14, 64]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "valeurs = period_relative\n",
    "colours = ['b','g','r','c','m','y','k'] #to make sure colours remain the same throughout all slices\n",
    "\n",
    "valeurs2 = dict()\n",
    "\n",
    "for key in valeurs.keys():\n",
    "    #print(key)\n",
    "    list_temp = list()\n",
    "    for item in valeurs[key]:\n",
    "        list_temp.append(int(item*100))   # let's have percentages and not .xx\n",
    "    valeurs2[key] = list_temp\n",
    "\n",
    "    #for value in valeurs\n",
    "    \n",
    "\n",
    "for key,vals in valeurs2.items():\n",
    "    print(key,vals)\n",
    "    \n",
    "    for i in range(0,len(vals)):        \n",
    "        if i == 0:\n",
    "            previous = 0\n",
    "            plt.bar(x=key, height=vals[i],bottom=previous,color=colours[i])\n",
    "            \n",
    "        else:         \n",
    "            previous = vals[i-1] + previous\n",
    "            plt.bar(x=key, height=vals[i],bottom=previous,color=colours[i])\n",
    "            \n",
    "plt.xticks(range(len(valeurs2)), valeurs2.keys())\n",
    "\n",
    "expert_image = s_senses.name.split(\"/\")[-1]\n",
    "\n",
    "#plt.figure(figsize=(20,10))\n",
    "image = plt.gcf()\n",
    "image.savefig(dir_out+\"/\"+expert_image+\".png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "liste_number_year = list() # creating a list because matplotlib wants a tuple\n",
    "for key in sense_date_amount.keys():\n",
    "    #print(key)\n",
    "    liste_number_year.append(sense_date_amount[key])\n",
    "    \n",
    "tuple_number_year = tuple(liste_number_year)\n",
    "#print(tuple_number_year)\n",
    "\n",
    "period_number = dict()\n",
    "\n",
    "for key in sense_date_amount.keys():\n",
    "    compteur = 0\n",
    "    if key[1] in range(0,number_of_slices):\n",
    "        print(key,sense_date_amount[key[0],key[1]])\n",
    "        compteur += sense_date_amount[key[0],key[1]]\n",
    "        \n",
    "        try :\n",
    "            period_number[key[1]] += compteur\n",
    "        except KeyError:\n",
    "            period_number[key[1]] = 0\n",
    "            period_number[key[1]] += compteur\n",
    "            \n",
    "        \n",
    "for entry in period_number:\n",
    "    print(\"priode\",entry,\"number of uses\",period_number[entry])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading model output for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_output_plot = output_senses.split(\"===============  per time  ===============\")[1].split(\"\\n\")\n",
    "period_relative_model = dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 []\n",
      "0 [0.04083955068731151]\n",
      "0 [0.04083955068731151, 0.008486796949771783]\n",
      "0 [0.04083955068731151, 0.008486796949771783, 0.00597187964362235]\n",
      "0 [0.04083955068731151, 0.008486796949771783, 0.00597187964362235, 0.7697746391951782]\n",
      "0 [0.04083955068731151, 0.008486796949771783, 0.00597187964362235, 0.7697746391951782, 0.17492713352411612]\n",
      "1 []\n",
      "1 [0.10907371060119478]\n",
      "1 [0.10907371060119478, 0.014298730684103153]\n",
      "1 [0.10907371060119478, 0.014298730684103153, 0.03384641373162034]\n",
      "1 [0.10907371060119478, 0.014298730684103153, 0.03384641373162034, 0.5064813344844331]\n",
      "1 [0.10907371060119478, 0.014298730684103153, 0.03384641373162034, 0.5064813344844331, 0.3362998104986487]\n",
      "2 []\n",
      "2 [0.08757407290535785]\n",
      "2 [0.08757407290535785, 0.015051254608967434]\n",
      "2 [0.08757407290535785, 0.015051254608967434, 0.024163835560131568]\n",
      "2 [0.08757407290535785, 0.015051254608967434, 0.024163835560131568, 0.6984925663043847]\n",
      "2 [0.08757407290535785, 0.015051254608967434, 0.024163835560131568, 0.6984925663043847, 0.17471827062115847]\n",
      "3 []\n",
      "3 [0.07407106626171724]\n",
      "3 [0.07407106626171724, 0.03404926797763053]\n",
      "3 [0.07407106626171724, 0.03404926797763053, 0.04501432410891889]\n",
      "3 [0.07407106626171724, 0.03404926797763053, 0.04501432410891889, 0.6654202704667155]\n",
      "3 [0.07407106626171724, 0.03404926797763053, 0.04501432410891889, 0.6654202704667155, 0.18144507118501788]\n",
      "4 []\n",
      "4 [0.1803754795434305]\n",
      "4 [0.1803754795434305, 0.04005687932522156]\n",
      "4 [0.1803754795434305, 0.04005687932522156, 0.10369804638106733]\n",
      "4 [0.1803754795434305, 0.04005687932522156, 0.10369804638106733, 0.43394041107060427]\n",
      "4 [0.1803754795434305, 0.04005687932522156, 0.10369804638106733, 0.43394041107060427, 0.24192918367967622]\n",
      "5 []\n",
      "5 [0.17698941573659951]\n",
      "5 [0.17698941573659951, 0.03704265374613105]\n",
      "5 [0.17698941573659951, 0.03704265374613105, 0.14841983538917458]\n",
      "5 [0.17698941573659951, 0.03704265374613105, 0.14841983538917458, 0.41587711391800924]\n",
      "5 [0.17698941573659951, 0.03704265374613105, 0.14841983538917458, 0.41587711391800924, 0.2216709812100856]\n",
      "6 []\n",
      "6 [0.20603546151577476]\n",
      "6 [0.20603546151577476, 0.0777841384954993]\n",
      "6 [0.20603546151577476, 0.0777841384954993, 0.2523433469634035]\n",
      "6 [0.20603546151577476, 0.0777841384954993, 0.2523433469634035, 0.3203623104634722]\n",
      "6 [0.20603546151577476, 0.0777841384954993, 0.2523433469634035, 0.3203623104634722, 0.14347474256185028]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(lines_output_plot)):\n",
    "    if lines_output_plot[i][0:5] == \"Time=\":  # if a line starts with \"time\" we take it into account\n",
    "        for x in range(i,i+number_of_the_k+1): # for every \"number of  the k\" lines that follow\n",
    "            #print(lines_output_plot[x])\n",
    "            if lines_output_plot[x][0:5] == \"Time=\": # if a line starts with \"time\" we take the value for the slice\n",
    "                period = lines_output_plot[x][5:6]\n",
    "                templist = list()\n",
    "                \n",
    "            if lines_output_plot[x][0:5] != \"Time=\":  # if a line doesn't start with \"time\" but is considered(cf line3)\n",
    "                ligne = re.split(\"\\s{3,}\",lines_output_plot[x]) # we take the first part of the line (importance of that K)\n",
    "                templist.append(float(ligne[0]))\n",
    "            print(period,templist)\n",
    "            \n",
    "        period_relative_model[str(period)] = templist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': [0.04083955068731151, 0.008486796949771783, 0.00597187964362235, 0.7697746391951782, 0.17492713352411612], '1': [0.10907371060119478, 0.014298730684103153, 0.03384641373162034, 0.5064813344844331, 0.3362998104986487], '2': [0.08757407290535785, 0.015051254608967434, 0.024163835560131568, 0.6984925663043847, 0.17471827062115847], '3': [0.07407106626171724, 0.03404926797763053, 0.04501432410891889, 0.6654202704667155, 0.18144507118501788], '4': [0.1803754795434305, 0.04005687932522156, 0.10369804638106733, 0.43394041107060427, 0.24192918367967622], '5': [0.17698941573659951, 0.03704265374613105, 0.14841983538917458, 0.41587711391800924, 0.2216709812100856], '6': [0.20603546151577476, 0.0777841384954993, 0.2523433469634035, 0.3203623104634722, 0.14347474256185028]}\n"
     ]
    }
   ],
   "source": [
    "print(period_relative_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting model output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.04083955068731151, 0.008486796949771783, 0.00597187964362235, 0.7697746391951782, 0.17492713352411612]\n",
      "<class 'str'> <class 'list'>\n",
      "1 [0.10907371060119478, 0.014298730684103153, 0.03384641373162034, 0.5064813344844331, 0.3362998104986487]\n",
      "<class 'str'> <class 'list'>\n",
      "2 [0.08757407290535785, 0.015051254608967434, 0.024163835560131568, 0.6984925663043847, 0.17471827062115847]\n",
      "<class 'str'> <class 'list'>\n",
      "3 [0.07407106626171724, 0.03404926797763053, 0.04501432410891889, 0.6654202704667155, 0.18144507118501788]\n",
      "<class 'str'> <class 'list'>\n",
      "4 [0.1803754795434305, 0.04005687932522156, 0.10369804638106733, 0.43394041107060427, 0.24192918367967622]\n",
      "<class 'str'> <class 'list'>\n",
      "5 [0.17698941573659951, 0.03704265374613105, 0.14841983538917458, 0.41587711391800924, 0.2216709812100856]\n",
      "<class 'str'> <class 'list'>\n",
      "6 [0.20603546151577476, 0.0777841384954993, 0.2523433469634035, 0.3203623104634722, 0.14347474256185028]\n",
      "<class 'str'> <class 'list'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "valeurs = period_relative_model\n",
    "colours = ['b','g','r','c','m','y','k'] #to make sure colours remain the same throughout all slices\n",
    "    \n",
    "\n",
    "for key,vals in valeurs.items():\n",
    "    print(key,vals)\n",
    "    print(type(key),type(vals))\n",
    "    \n",
    "    for i in range(0,len(vals)):        \n",
    "        if i == 0:\n",
    "            previous = 0\n",
    "            plt.bar(x=key, height=vals[i],bottom=previous,color=colours[i])\n",
    "            \n",
    "        else:         \n",
    "            previous = vals[i-1] + previous\n",
    "            plt.bar(x=key, height=vals[i],bottom=previous,color=colours[i])\n",
    "            \n",
    "plt.xticks(range(len(valeurs)), valeurs.keys())\n",
    "\n",
    "#plt.figure(figsize=(20,10))\n",
    "\n",
    "model_image = k_senses.name.split(\"/\")[-1]\n",
    "\n",
    "image = plt.gcf()\n",
    "image.savefig(dir_out+\"/\"+model_image+\".png\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "1. Choose best (k,s) pair\n",
    "1. Match k and s in the plots\n",
    "2. Label the plots (senses, slices)\n",
    "2. ~~Remove the \"w\" sense~~\n",
    "3. Confidence interval (less important for now)\n",
    "4. Fix the recall calculation cfr email Valerio 28/03\n",
    "5. CHECK PROBABILITIES (conf)\n",
    "6. Write output to file + sync github\n",
    "7. ~~Fix length of time interval (100 vs 113)_~~ ||| earliest date [from parameter_file] vs time interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best pair: the one with the maximum above a certain threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
