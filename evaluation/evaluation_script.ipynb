{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a work-in-progress notebook\n",
    "\n",
    "We wish to know this:\n",
    "\n",
    "1. How well does the model identify the correct number of senses for the target word?\n",
    "2. **How well does the model identify the correct senses for the target word?**\n",
    "3. **How well does the model assign the right words to a given sense of the target word?**\n",
    "4. How well does the model assign the senses to the time intervals for the target word?\n",
    "\n",
    "The script will evaluate **Q2** and **Q3**. Q4 will follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic variables and imports:\n",
    "\n",
    "import codecs, csv, os, time, re, io\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "# directories\n",
    "dir_in = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), \"evaluation\", \"evaluation_input\"))\n",
    "dir_out = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), \"evaluation\", \"evaluation_output\"))\n",
    "\n",
    "\n",
    "s_senses = io.open(dir_in+\"/senses_69419.txt\",\"r\")\n",
    "k_senses = io.open(dir_in+\"/mus.dat\",\"r\")\n",
    "\n",
    "# DEBUG:\n",
    "#s_senses = io.open(dir_in+\"/senses_69419_debug.txt\",\"r\")\n",
    "#k_senses = io.open(dir_in+\"/mus_debug.dat\",\"r\")\n",
    "# k0 = mus4\n",
    "# k1 = mus3\n",
    "# k2 = mus2\n",
    "# k3 = mus1\n",
    "# k4 = nothing\n",
    "\n",
    "file_senses = s_senses.readlines()[1:]\n",
    "output_senses = k_senses.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- ~~create the notebook~~\n",
    "- ~~organise the notebook~~\n",
    "- ~~write \"general idea\" pseudocode for the evaluation~~\n",
    "- ~~get input files~~\n",
    "- ~~figure out data structures to store the variables~~\n",
    "- ~~write actual code~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: How well does the model identify the correct senses for the target word?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-704-82fa54601c9a>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-704-82fa54601c9a>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    for each k:\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# For each target word, we have a list of senses  s (given by the expert)\n",
    "# For each target word, we have a list of senses k (given by the model)\n",
    "# This Q consists in matching s and k, and doing so in a confident way --> confidence score\n",
    "\n",
    "for each k:\n",
    "    for each s:\n",
    "        create conf(k,s)\n",
    "\n",
    "# What is conf(k,s)?\n",
    "        conf(k,s) = (p1*match(w1,s)+p2*match(w1,s)+px(wx,s))/10 WHERE\n",
    "    \n",
    "            px = probability of word wx \n",
    "                \n",
    "                and\n",
    "            \n",
    "            match(wx,s) =   1/number_of_senses_assigned_to_wx if s_is_one_of_them \n",
    "            \n",
    "                    or \n",
    "                            0 if w_is_not_associated_to_s\n",
    "                \n",
    "# Once we have gone through all s for one k, we have to choose the best k for s. How? (TBD, cfr Valerio and Barbara)\n",
    "\n",
    "# Once all ks have been assigned to all ss (or NA), we can calculate a general confidence score for the model.\n",
    "# One easy way to do that: \n",
    "\n",
    "conf_score_model = number_of_non_NA/k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real code\n",
    "\n",
    "Steps:\n",
    "\n",
    "- extract all senses from the file\n",
    "- use those senses as keys for a dictionary, `dict_of_words`\n",
    "- fill the dictionary: for each key, we store a list of words pertaining to that sense\n",
    "- transform the lists as sets so as to remove duplicates within the same sense\n",
    "- create a dictionary with a word as a key and its weight as a value, depending on how many senses it appears\n",
    "- parse the model output and get the probability weights for each word\n",
    "- do not take into account the first line\n",
    "- take care of empty lines\n",
    "\n",
    "Todo:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of senses: 4\n",
      "mus-1 {'16430', '54812', '18937', '7944', '5521', '21812', '12641', '112472', '76335', '42890', '81343', '101716', '24003', '59708', '48867', '4670', '31562', '21431', '66746', 'nlsj47984', '23690', '3986', '34855', '50093', '38547', '104872', '23468', '95853', '74127', '105344', '94483', '71262', '5009', '21802', '68970', '12349', '113060', '75910', '19641', '403', '34603', '26197', 'nlsj80462', '108536', '11072', '107167', '25007', 'nlsj68228', '37851', '62205', '63814', '65565', '2224', '66173', '55997', '678', '17730', '75948', '92662', '84725', '80557', '21487', '79595', '5112', '52332', '42842', '97285', '37776', '24801', '27629', '31948', '53956', '106502', '74735', '80042', '17381', 'nlsj9526', '7498', '116059', '104355', '28360', '80664', '40545', '13427', '6174', '13039', 'nlsj98815', 'nlsj8979', '19268', '79223', '92406', '31491', '10056', '103972', '54113', '95074', '46176', '93449', '41502', '70105', '2219', '260', '15047', '26943', '42887', '51241', 'nlsj80958', '6449', '6684', '104311', '65983', '116244', '7561', '64956', '72610', '113560', '49331', 'nlsj4012', 'nlsj801', '66639', '65097', '65552', '45513', '41619', '4068', '63212', 'nlsj114103', '97147', '46123', '88464', '102989', '2767', '77928', '35708', '53942', '58478', '2749', '62383', '7159', '62303', '38732', '104860', '50616', '19788', '86352', '103975', '61925', '34366', '14050', '1083', '22502', '72627', 'nlsj78558', '8665', 'nlsj96345', '1377', 'nlsj60710', '5607', '41705', '83774', 'nlsj77405', '116470', 'nlsj106628', 'nlsj10580', '103221', '7779', '17007', '29624', '33241', '78716', '108882', '2671', '60402', '98241', '82959', '26114', '72928', '89807', '34848', '80327', '21830', '15618', '47735', '20195', '104289', '24261', '55499', '100774', '93022', '28569', '4778', '95654', '105550', 'nlsj5437', '29828', '97386', '26136', '49589', '93388', '72716', '76703', '7133', '70550', '61176', '102669', 'nlsj4784', '28355', '5131', '11583', 'nlsj57918', '27209', '60225', '82954', '114731', '8505', '74819', '16400', '83756', '85306', '23678', 'nlsj1499', '43977', '46966', '7612', '95258', '84094', '72202', '41357', '11305', '114615', '75954', '72287', 'nlsj32160', '23628', '436', '54399', '47665', 'nlsj40132', '3241', 'nlsj96033', '48704', '31964', '74631', '112347', '85953', '18166', '80239', '78746', 'nlsj69856', '27415', 'nlsj17074', '7832', '112816', '102847', '22209', '26499', '61040', 'nlsj61205', '98031', '70958', '110114', '20728', '83783', '103942', '41032', '26447', 'nlsj5904', '22036', '66777', '51647', '84422', '29962', '2845', '45996', '70482', '37488', '66262', '39313', '2731', '21901', '53695', '47617', '18128', 'nlsj105676', '2583', '104538', '33671', '70477', '95432', '19346', '51815', '52460', '116050', '53161', '69419', '51369', '116058', '15121', '99638', 'nlsj7783', '100524', '15893', '90329', '22100', '103637', '76431', '50451', '11058', '65975', '18334', '2340', '10933', '91085', '7182', '82665', '105070', '36165', '115193', '71673', 'nlsj4320', '65628', '66494', '112720', '108537', '28566', '19452', '85672', '103922', '49875', '63845', '25870', 'nlsj59923', '103012', '1692', '102869', '39847', '83869', '20179', '36390', '69714', '18271', '104639', '75263', '56991', '73707', '28002', '61291', '22997', '39190', '59588', '72275', '93536', '44888', '31709', '3464', '56874', '51376', '941', '68791', '7062', '13579', '1564', '71314', '51727', '20674', '47447', '8586', '113741', '113711', '11206', '26233', '42686', '94098', '84234', '26684', '48498', '2834', 'nlsj11198', '18331', '71308', '64914', '31556', '56406', 'nlsj40053', '4927', '101445', '65697', '110302', '61056', '46474', '94474', '48670', '95221', '40323', 'nlsj71743', '83718', '86305', '98723', '46195', '96089', '112467', '86219', '110655', '93812', '79947', '114587', 'nlsj48763', '49955', '33074', 'nlsj86496', '80555', '72321', '85807', '47917', '18706', '4335', '34372', '6325', '105816', '83665', '111895', 'nlsj43224', 'nlsj11066', '22882', '114847', '92162', '100965', '55898', 'nlsj106503', '15162', '69052', '80761', '91055', '75352', '110606', 'nlsj5738', '114842', '31996', '113823', '27005', '41918', '110484', '13317', '45656', '60308', '84534', '83113', '83434', '110089', '63352', '76765', '51300', '26032', '56810', '20945', '72273', '51819', '86307', '24226', '72268', '63827', '107959', '28036', 'nlsj4113', '59005', '39195', '115810', '3128', '93417', '49437', '57321', '48770', '47964', 'nlsj32167', '103871', '83928', '112934', 'nlsj100912', '46074', '70708', 'nlsj8671', '111207', '31161', '61177', '23242', '48341', '61855', '64586', '110639', '56003', 'nlsj10130', '103993', '32686', '50644', '54971', '3398', '33647', '43124', '65757', '49227', '8909', '41536', '13608', '71559', '67104', '77698', '95522', '69863', '33160', '3327', '23794', '104225', '19972', 'nlsj7583', '30911', '30700', '83310', '92077', '61265', '95523', '16051', '113379', '67762', '68641', '83760', '68174', '69036', '98234', '80107', '4906', '66294', '110598', '4493', '67250', '96979', '112769', '67619', '106974', '43206', '41633', '34476', '45917', '63713', '4536', '15571', '59124', 'nlsj75598', 'nlsj68688', '2764', '19282', '14362', '51259', '33494', '64757', '9757', '109403', '26207', '49506', '102381', '74602', '67815', '107905', '76564', '11970', '74571', '46646', '80043', 'nlsj9035', '31565', '103701', '75552', '110027', '95368', '63143', '47513', '46216', '27764', '23658', '109730', '40001', '14181', '19546', '54592', '83251', '42659', '41082', '25018', '70768', '52035', '37616', '49933', '21783', '51256', '35710', '94713', '4274', '3237', 'nlsj10876', '72357', '80011', '26048', 'nlsj105619', '116293', '38488', '113556', '41538', 'nlsj4579', '62528', '49886', '62816', '29883', '101853', '112943', '73972', '111416', '74523', '37726', '67485', '112833', '108780', '113251', '50679', '84248', '7712', '2061', '86429', 'nlsj33192', '84494', '79103', '47745', '12035', '64448', '80357', '51358', '29974', '42827', '43305', '103957', 'nlsj2729', '24856', '83253', '16052', '15763', '15858', '71118', '37711', '95819', '463', '12176', '76184', '35092', '16171', '11197', '35267', '19711', '84552', '21920', '96565', 'nlsj5118', '71065', '62093', '91944', '48479', '73128', '82758', '15380', '45980', '38743', '102000', '105176', '50073', '7529', '6384', '92010', '21335', '4845', '19536', '92171', '110456', 'nlsj2610', '112351', '116416', '12409', '91800', '94316', '13002', '104421', '73277', '59501', '99622', '17197', '24523', '53442', '40161', '115845', '42830', 'nlsj52509', '79261', '35570', '2873', '54946', '69252', '33770', '4378', '38966', '25402', '109687', '61856', 'nlsj8970', '25228', '43089', 'nlsj28819', '69469', '22739', '55532', '33956', '98312', '36790', '110284', '41481', '65089', '106114', '75316', '51849', '94097', '32657', '91516', '74126', '57057', '51373', '88716', '104690', '112169', '19123', '82756', '84263', '89366', '76530', '23283', '76910', '73064', '48095', '112070', '12973', '20621', '55137', '3800', '64316', '106064', '42581', '100906', '16609', '79697', '77699', 'nlsj12527', '31236', 'nlsj7856', '75306', '39125', '76157', '114816', '62204', '24444', '100693', '48291', '11229', '55139', '18788', '19891', '112559', '50824', '91351', '40156', '102474', '106566', '52571', '84150', '7177', '114972', '104655', '114706', '55149', '28592', '37274', '4548', '50252', 'nlsj1611', '55498', '59276', '61885', '67868', '21589', '5132', '4587', '90504', '46804', '15378', '37095', '89309', '17962', '67974', '12485', '22316', '57262', '85420', '84475', '55815', '7724', '61791', '47876', '114548', '24292', '114688', '88498', '63314', '12948', '3289', '73221', '85417', '13098', '43060', '45671', '37244', '34071', '98173', '45285', '103152', 'nlsj114757', '36571', '1984', 'nlsj6617', '93796', '83266', '51661', '84434', '52095'}\n",
      "\n",
      "\n",
      "\n",
      "mus-4 {'74378', '57457', '42071', '48436', '38472', '37262', '83665', '88855', '31562', '112307', '27092', '55391', '49629', '21651', '97605', '98584', '106222', '85665', '58142', '18790', '3630', '43616', '84534', '74752', '76765', '84421', '83834', '66761', 'nlsj40134', '70495', '95970', '13942', '50521', '6339', '114553', '4470', '60840', '70364', '99647', '70556', '50908', '61946', 'nlsj1147', '22425', '63873', '114753', '100964', 'nlsj112819', '77640', '106502', '53015', '74735', '19528', '111764', '24340', '104355', '96999', '48504', '26039', '108652', '2898', '56189', '6174', '35166', '110639', '94677', '13039', '57466', '36216', '91634', '96503', '70558', '85876', '8909', '105548', '39180', '28349', '35417', '115374', '101714', '19972', '621', '115212', '15609', '61128', '51241', '16051', '37405', '67762', '114387', '113560', '68641', '72610', '35244', '69532', '98234', '96560', '65552', '103085', '4493', '55497', '71585', '115102', '79069', '109729', '66342', '94365', '24184', '67352', '97606', '28588', '6616', 'nlsj99416', '34366', '75989', '33238', '90334', '65703', 'nlsj177', '46646', '87413', '13852', '34522', '39238', '83605', '14266', '55981', '75552', '97241', '71984', '22660', '116218', '33426', '34982', '65191', 'nlsj33345', '106263', '67555', '44660', '10223', '67009', '20836', '83468', '47735', '22210', '82451', '98869', '65966', '64703', '101034', '31609', '28569', '62528', '36289', '93388', '13315', '39543', '29883', '115748', '13985', '61176', '73672', '100495', '70521', '37726', '11583', '108780', '39016', '24946', '93341', '112703', '30317', '97963', '51358', '2392', '91583', '20830', '45153', '51785', '16804', '93379', 'nlsj86473', '85306', '67526', '37075', '76028', '104624', '72202', '26226', '114615', '93332', '36938', '90166', '113477', '48704', '74621', '112794', '1199', '71373', '97252', '71065', 'nlsj79694', '85953', '63772', '58972', '15786', '115887', '102000', '95961', '22209', '14269', '76513', '110114', '4845', '33969', '7608', '116416', '26034', '114339', '59516', '31607', '3364', '13376', '66295', '90347', '45245', '84034', '37961', '45996', 'nlsj82141', '93435', '35570', '60646', '42214', '33770', '100379', '106206', '18062', '55313', '61231', '89405', '19503', '61245', '50236', '63550', '69419', 'nlsj82490', '109733', '51369', '41570', '13715', '75005', '41271', '116408', '32657', '68996', '15893', '83732', '37720', '105458', '11058', '82800', '18334', '104690', '10933', '20885', '98417', '3325', '71404', '20621', '64316', '67507', '106676', '58429', '75595', '24289', '114653', '66494', '112720', '112367', 'nlsj35116', '26002', '83869', '92926', '101851', '66750', '102795', '36180', '6898', '56991', '104639', '62204', '48291', '83650', '96981', '61553', '13813', '43975', '112559', '33486', '85099', '52571', '4474', '83825', '76516', '82190', 'nlsj42558', '7793', '102676', 'nlsj61122', '109918', '96698', '39078', '15011', '90504', '58319', '111722', '84234', '114083', '86108', '66389', '60703', '116216', '63928', '55383', '57460', '96853', '24132', 'nlsj71743', '26046', '83174', '74463', '57474', '39299', '93812', '66755', '47442'}\n",
      "\n",
      "\n",
      "\n",
      "w {'114624', '35642', '92927', '11058', '293', 'nlsj5856', '98712', '2219', '260', '12625', '16132', '46178', '73064', '31562', '84410', '61016', '58444', '31537', '71492', '16400', '85306', '30790', '65552', '63845', '41357', '64985', '31236', '38714', '72287', '86112', 'nlsj32160', 'nlsj5105', '104374', '23903', '48549', '62204', '21920', '98541', '112350', '113060', '56607', '61708', '31333', '63772', 'nlsj4603', '96193', '11072', '19282', '1379', '12210', '96106', '39186', '1083', '90410', '104655', '70958', '71976', '28592', '4548', '1093', '71312', '24436', '8665', '98747', '26447', '1377', '34522', '116416', '35541', '103701', '75552', '104429', '104421', '46804', '65747', 'nlsj7765', '109757', '45996', '114753', '23658', '115213', '33631', 'nlsj56179', '43260', '71308', '4958', '76801', '27847', '61073', '42214', '24340', '23799', '53826', '86305', '67132', '28569', '69419', '5390', '46195', '79223', '44552', '114558', 'nlsj7011', '35581', '1984', '49589', '65295', '3398', '54356', '78297', '26886', '1989', '32657', '12252'}\n",
      "\n",
      "\n",
      "\n",
      "mus-2 {'110127', '95144', '74378', '26818', '95095', '20508', '75652', '76335', 'nlsj86871', '18762', '53254', '49444', 'nlsj33192', '53222', '47745', '68539', '214', '100673', '74675', '4670', 'nlsj2729', 'nlsj68042', '92475', '74165', '15763', '19623', '60425', 'nlsj112110', '17228', '104624', '62535', '49439', '106023', '93880', '54990', '80525', '37343', '41918', '62274', '104637', '6184', '49182', '94967', '42678', '21920', '56954', '93624', '19641', '7362', '82758', '45980', '49176', '18561', '101790', '63032', '60317', '65565', '82623', '20413', '16440', '95540', '4562', '1904', '21490', '56223', 'nlsj10103', '98955', '51737', '114381', '74610', '106267', '23861', '19789', '53286', '65544', '45996', '113881', '45182', '112733', 'nlsj32819', '4763', '103038', '67422', 'nlsj100389', '110060', '116273', 'nlsj61217', '106206', '15198', '58527', '91198', '4698', '53538', 'nlsj7772', '69751', '65934', '46452', '69419', '110639', '29134', '97266', '75992', '98114', '60878', '69220', '115638', '38744', '30564', '102930', '59626', '45526', '93842', 'nlsj3264', '103637', '74126', '91541', '83212', '101257', '5591', '114347', '29390', '20469', 'nlsj8425', 'nlsj5634', '84089', '101982', '19236', '76910', '104416', 'nlsj75531', '64642', '83458', '65983', 'nlsj4805', '58429', '103667', '67762', '64483', '45525', 'nlsj61519', '66494', '103648', 'nlsj12526', '59121', '68706', 'nlsj4249', '85672', 'nlsj114797', 'nlsj10202', 'nlsj3072', '65552', '49875', '20270', '62337', '60316', '89321', 'nlsj76740', '92553', '105557', '33457', '60113', '62258', '21168', '75539', '58271', '114437', '105691', '66342', '26328', '16986', 'nlsj29538', '35633', 'nlsj99416', '102474', '61185', '61622', '52571', '67360', '57356', '94164', '83209', '112512', '32263', '20674', '64983', '45757', '67364', '2586', 'nlsj6085', '74225', '75653', '80188', '22316', '16884', '90350', '22166', '103773', '86213', '57262', '34243', '68185', '64559', '58262', 'nlsj3887', '21205', '47735', '30569', 'nlsj36416', '70768', '22389', 'nlsj3191', '90303', '112811', '58133', '93840', 'nlsj114757', '68144', '93388', '92051', '112724', '94291', 'nlsj70384', '113163', '73672', '33822'}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expert_senses = list() # list where we store all sense ids provided by expert\n",
    "for s in file_senses: # 60 for testing purposes\n",
    "    s = s.split(\"\\t\")\n",
    "    sense = s[11] # The sense ID is after the 10th tab\n",
    "    expert_senses.append(sense)\n",
    "    \n",
    "#print(len(expert_senses),expert_senses,len(set(expert_senses)))\n",
    "\n",
    "expert_senses = list(set(expert_senses)) # we only keep the unique senses\n",
    "number_of_s = len(expert_senses)  # we create a variable that stores the number of unique senses\n",
    "print(\"Number of senses:\",number_of_s)\n",
    "\n",
    "# This dictionary has a sense as a key, and a list of words as a value. \n",
    "dict_of_words = dict()\n",
    "# This list stores all words\n",
    "list_of_all_words = list()\n",
    "# This dictionary stores all words as keys and their weight as value\n",
    "word_weight = dict()\n",
    "\n",
    "for i in range(0,number_of_s): # for each sense, we create a dictionary entry which has a list as value\n",
    "    dict_of_words[expert_senses[i]] = list()\n",
    "\n",
    "    for s in file_senses: # we go back in the file\n",
    "        s = s.split(\"\\t\") # splitting on tabs\n",
    "        \n",
    "        sentence_of_ids = s[8] # 8 is for IDs, 9 is for words\n",
    "        list_of_ids = sentence_of_ids.split(\" \")  # splitting on spaces\n",
    "        for word_id in list_of_ids:\n",
    "            if s[11] == expert_senses[i]:      # we store all words for one sense \n",
    "                dict_of_words[expert_senses[i]].append(word_id)\n",
    "            list_of_all_words.append(word_id) # we store all words, we'll iterate over that for scores\n",
    "        \n",
    "\n",
    "    # Here, we remove duplicates\n",
    "    #dict_of_words[expert_senses[i]].append(\"79223\") #testing\n",
    "    dict_of_words[expert_senses[i]] = set(dict_of_words[expert_senses[i]]) \n",
    "      \n",
    "    print(expert_senses[i],set(dict_of_words[expert_senses[i]]))\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every word in the list of words that we have\n",
    "# we count the number of senses it appears in\n",
    "# we use that number to divide its importance: 1 sense = 1 importance; 2 senses = 0.5 importance\n",
    "# this can be finetuned\n",
    "\n",
    "for word in list_of_all_words:\n",
    "    x = 0\n",
    "    for i in range(0,number_of_s):\n",
    "        if word in dict_of_words[expert_senses[i]]:\n",
    "            x += 1 \n",
    "        if x != 0:\n",
    "            word_weight[word] = 1/x\n",
    "        \n",
    "    \n",
    "    #print(word,word_weight[word])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parsing output.dat\n",
    "- split on \"===============  per time  ===============\" and keep first part\n",
    "- transform that into a list, then\n",
    "- get lines that start with \"p(w|s)\"\n",
    "- count those, k = that number\n",
    "- split the line on \":\", keep the second part\n",
    "- split the rest on \";\", it's [ID] = prob_from_this_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word id 28355 ; probability 0.088\n",
      "word id 69419 ; probability 0.069\n",
      "word id 57460 ; probability 0.056\n",
      "word id 114587 ; probability 0.042\n",
      "word id 42071 ; probability 0.041\n",
      "word id 35267 ; probability 0.035\n",
      "word id 51647 ; probability 0.035\n",
      "word id 64448 ; probability 0.023\n",
      "word id 45980 ; probability 0.018\n",
      "word id 53826 ; probability 0.017\n",
      "{'28355': 0.2075471698113207}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377, '57460': 0.1320754716981132}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377, '57460': 0.1320754716981132, '114587': 0.0990566037735849}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377, '57460': 0.1320754716981132, '114587': 0.0990566037735849, '42071': 0.09669811320754716}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377, '57460': 0.1320754716981132, '114587': 0.0990566037735849, '42071': 0.09669811320754716, '35267': 0.08254716981132075}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377, '57460': 0.1320754716981132, '114587': 0.0990566037735849, '42071': 0.09669811320754716, '35267': 0.08254716981132075, '51647': 0.08254716981132075}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377, '57460': 0.1320754716981132, '114587': 0.0990566037735849, '42071': 0.09669811320754716, '35267': 0.08254716981132075, '51647': 0.08254716981132075, '64448': 0.05424528301886792}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377, '57460': 0.1320754716981132, '114587': 0.0990566037735849, '42071': 0.09669811320754716, '35267': 0.08254716981132075, '51647': 0.08254716981132075, '64448': 0.05424528301886792, '45980': 0.042452830188679236}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377, '57460': 0.1320754716981132, '114587': 0.0990566037735849, '42071': 0.09669811320754716, '35267': 0.08254716981132075, '51647': 0.08254716981132075, '64448': 0.05424528301886792, '45980': 0.042452830188679236, '53826': 0.04009433962264151}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377, '57460': 0.1320754716981132, '114587': 0.0990566037735849, '42071': 0.09669811320754716, '35267': 0.08254716981132075, '51647': 0.08254716981132075, '64448': 0.05424528301886792, '45980': 0.042452830188679236, '53826': 0.04009433962264151}\n",
      "word id 79223 ; probability 0.063\n",
      "word id 92927 ; probability 0.057\n",
      "word id 46574 ; probability 0.038\n",
      "word id 67660 ; probability 0.038\n",
      "word id 103085 ; probability 0.036\n",
      "word id 86112 ; probability 0.030\n",
      "word id 101982 ; probability 0.029\n",
      "word id 75808 ; probability 0.026\n",
      "word id 68539 ; probability 0.024\n",
      "word id 54607 ; probability 0.024\n",
      "{'79223': 0.17260273972602735}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438, '46574': 0.10410958904109587}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438, '46574': 0.10410958904109587, '67660': 0.10410958904109587}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438, '46574': 0.10410958904109587, '67660': 0.10410958904109587, '103085': 0.09863013698630134}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438, '46574': 0.10410958904109587, '67660': 0.10410958904109587, '103085': 0.09863013698630134, '86112': 0.08219178082191778}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438, '46574': 0.10410958904109587, '67660': 0.10410958904109587, '103085': 0.09863013698630134, '86112': 0.08219178082191778, '101982': 0.07945205479452053}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438, '46574': 0.10410958904109587, '67660': 0.10410958904109587, '103085': 0.09863013698630134, '86112': 0.08219178082191778, '101982': 0.07945205479452053, '75808': 0.07123287671232874}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438, '46574': 0.10410958904109587, '67660': 0.10410958904109587, '103085': 0.09863013698630134, '86112': 0.08219178082191778, '101982': 0.07945205479452053, '75808': 0.07123287671232874, '68539': 0.06575342465753423}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438, '46574': 0.10410958904109587, '67660': 0.10410958904109587, '103085': 0.09863013698630134, '86112': 0.08219178082191778, '101982': 0.07945205479452053, '75808': 0.07123287671232874, '68539': 0.06575342465753423, '54607': 0.06575342465753423}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438, '46574': 0.10410958904109587, '67660': 0.10410958904109587, '103085': 0.09863013698630134, '86112': 0.08219178082191778, '101982': 0.07945205479452053, '75808': 0.07123287671232874, '68539': 0.06575342465753423, '54607': 0.06575342465753423}\n",
      "word id 62258 ; probability 0.080\n",
      "word id 64586 ; probability 0.052\n",
      "word id 58271 ; probability 0.048\n",
      "word id nlsj86871 ; probability 0.041\n",
      "word id 101851 ; probability 0.039\n",
      "word id nlsj183 ; probability 0.037\n",
      "word id 75653 ; probability 0.031\n",
      "word id nlsj59923 ; probability 0.030\n",
      "word id 70105 ; probability 0.026\n",
      "word id 82758 ; probability 0.024\n",
      "{'62258': 0.19607843137254902}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685, '58271': 0.11764705882352941}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685, '58271': 0.11764705882352941, 'nlsj86871': 0.10049019607843138}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685, '58271': 0.11764705882352941, 'nlsj86871': 0.10049019607843138, '101851': 0.09558823529411764}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685, '58271': 0.11764705882352941, 'nlsj86871': 0.10049019607843138, '101851': 0.09558823529411764, 'nlsj183': 0.0906862745098039}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685, '58271': 0.11764705882352941, 'nlsj86871': 0.10049019607843138, '101851': 0.09558823529411764, 'nlsj183': 0.0906862745098039, '75653': 0.07598039215686274}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685, '58271': 0.11764705882352941, 'nlsj86871': 0.10049019607843138, '101851': 0.09558823529411764, 'nlsj183': 0.0906862745098039, '75653': 0.07598039215686274, 'nlsj59923': 0.07352941176470587}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685, '58271': 0.11764705882352941, 'nlsj86871': 0.10049019607843138, '101851': 0.09558823529411764, 'nlsj183': 0.0906862745098039, '75653': 0.07598039215686274, 'nlsj59923': 0.07352941176470587, '70105': 0.06372549019607843}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685, '58271': 0.11764705882352941, 'nlsj86871': 0.10049019607843138, '101851': 0.09558823529411764, 'nlsj183': 0.0906862745098039, '75653': 0.07598039215686274, 'nlsj59923': 0.07352941176470587, '70105': 0.06372549019607843, '82758': 0.058823529411764705}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685, '58271': 0.11764705882352941, 'nlsj86871': 0.10049019607843138, '101851': 0.09558823529411764, 'nlsj183': 0.0906862745098039, '75653': 0.07598039215686274, 'nlsj59923': 0.07352941176470587, '70105': 0.06372549019607843, '82758': 0.058823529411764705}\n",
      "word id 102000 ; probability 0.035\n",
      "word id 70495 ; probability 0.035\n",
      "word id 7182 ; probability 0.029\n",
      "word id 70958 ; probability 0.024\n",
      "word id 49589 ; probability 0.021\n",
      "word id 83174 ; probability 0.021\n",
      "word id 106676 ; probability 0.019\n",
      "word id 22209 ; probability 0.018\n",
      "word id 93388 ; probability 0.017\n",
      "word id 16132 ; probability 0.016\n",
      "{'102000': 0.14893617021276598}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598, '7182': 0.12340425531914895}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598, '7182': 0.12340425531914895, '70958': 0.1021276595744681}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598, '7182': 0.12340425531914895, '70958': 0.1021276595744681, '49589': 0.08936170212765958}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598, '7182': 0.12340425531914895, '70958': 0.1021276595744681, '49589': 0.08936170212765958, '83174': 0.08936170212765958}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598, '7182': 0.12340425531914895, '70958': 0.1021276595744681, '49589': 0.08936170212765958, '83174': 0.08936170212765958, '106676': 0.08085106382978724}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598, '7182': 0.12340425531914895, '70958': 0.1021276595744681, '49589': 0.08936170212765958, '83174': 0.08936170212765958, '106676': 0.08085106382978724, '22209': 0.07659574468085106}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598, '7182': 0.12340425531914895, '70958': 0.1021276595744681, '49589': 0.08936170212765958, '83174': 0.08936170212765958, '106676': 0.08085106382978724, '22209': 0.07659574468085106, '93388': 0.0723404255319149}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598, '7182': 0.12340425531914895, '70958': 0.1021276595744681, '49589': 0.08936170212765958, '83174': 0.08936170212765958, '106676': 0.08085106382978724, '22209': 0.07659574468085106, '93388': 0.0723404255319149, '16132': 0.06808510638297872}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598, '7182': 0.12340425531914895, '70958': 0.1021276595744681, '49589': 0.08936170212765958, '83174': 0.08936170212765958, '106676': 0.08085106382978724, '22209': 0.07659574468085106, '93388': 0.0723404255319149, '16132': 0.06808510638297872}\n",
      "word id nlsj5634 ; probability 0.039\n",
      "word id 61925 ; probability 0.037\n",
      "word id 12620 ; probability 0.030\n",
      "word id 69419 ; probability 0.027\n",
      "word id 104421 ; probability 0.027\n",
      "word id 91085 ; probability 0.024\n",
      "word id 71308 ; probability 0.022\n",
      "word id 115748 ; probability 0.021\n",
      "word id 19641 ; probability 0.020\n",
      "word id 5390 ; probability 0.019\n",
      "{'nlsj5634': 0.14661654135338348}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228, '12620': 0.11278195488721805}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228, '12620': 0.11278195488721805, '69419': 0.10150375939849625}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228, '12620': 0.11278195488721805, '69419': 0.10150375939849625, '104421': 0.10150375939849625}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228, '12620': 0.11278195488721805, '69419': 0.10150375939849625, '104421': 0.10150375939849625, '91085': 0.09022556390977446}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228, '12620': 0.11278195488721805, '69419': 0.10150375939849625, '104421': 0.10150375939849625, '91085': 0.09022556390977446, '71308': 0.08270676691729324}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228, '12620': 0.11278195488721805, '69419': 0.10150375939849625, '104421': 0.10150375939849625, '91085': 0.09022556390977446, '71308': 0.08270676691729324, '115748': 0.07894736842105265}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228, '12620': 0.11278195488721805, '69419': 0.10150375939849625, '104421': 0.10150375939849625, '91085': 0.09022556390977446, '71308': 0.08270676691729324, '115748': 0.07894736842105265, '19641': 0.07518796992481204}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228, '12620': 0.11278195488721805, '69419': 0.10150375939849625, '104421': 0.10150375939849625, '91085': 0.09022556390977446, '71308': 0.08270676691729324, '115748': 0.07894736842105265, '19641': 0.07518796992481204, '5390': 0.07142857142857144}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228, '12620': 0.11278195488721805, '69419': 0.10150375939849625, '104421': 0.10150375939849625, '91085': 0.09022556390977446, '71308': 0.08270676691729324, '115748': 0.07894736842105265, '19641': 0.07518796992481204, '5390': 0.07142857142857144}\n"
     ]
    }
   ],
   "source": [
    "lines_output = output_senses.split(\"===============  per time  ===============\")[0].split(\"\\n\")\n",
    "\n",
    "number_of_the_k = 0\n",
    "\n",
    "k_words_with_prob = dict()\n",
    "\n",
    "for line in lines_output:\n",
    "    if line[:6] == \"p(w|s)\":\n",
    "        line = line.split(\":\")[1]\n",
    "        line = line.split(\";\")\n",
    "        #print(number_of_the_k,line)\n",
    "        dico_word_prob = dict()\n",
    "        temp_dict = dict()\n",
    "        k_words_with_prob[number_of_the_k] = list()\n",
    "        \n",
    "        line = line[:-1] # last item of the list is empty\n",
    "        \n",
    "        total_probability = 0 # to have relative probs\n",
    "        for word_prob in line:\n",
    "            \n",
    "\n",
    "        \n",
    "            #word_prob = word_prob.split(\",\")\n",
    "            #for word in word_prob:\n",
    "            probability = re.findall(\"([\\d.\\w]*)\",word_prob)\n",
    "            if probability:\n",
    "                probability = list(filter(None,probability))\n",
    "                    \n",
    "            total_probability += float(probability[1])\n",
    "            print(\"word id\",probability[0],\"; probability\",probability[1])\n",
    "        \n",
    "            dico_word_prob[probability[0]] = float(probability[1])\n",
    "        #print(type(k_words_with_prob[number_of_the_k]))\n",
    "        \n",
    "        for i in dico_word_prob.keys():\n",
    "            \n",
    "            temp_dict[i] = float(dico_word_prob[i]/total_probability)\n",
    "            k_words_with_prob[number_of_the_k] = temp_dict\n",
    "            \n",
    "            print(k_words_with_prob[number_of_the_k])\n",
    "            \n",
    "        #k_words_with_prob[number_of_the_k] = [float(dico_word_prob[i]/total_probability) for i in dico_word_prob]\n",
    "        #print(k_words_with_prob[number_of_the_k])\n",
    "        print(temp_dict)\n",
    "        number_of_the_k += 1\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k_words_with_prob\n",
    "This dictionary has the sense number 'k' as keys and the a dictionary of [word] = probability as values.\n",
    "Example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "#print(\"Probability for word ID 5390 in sense k = 4:\",k_words_with_prob[4][\"5390\"])\n",
    "print(type(k_words_with_prob[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output sense 0\n",
      "\texpert sense number  0 mus-1\n",
      "\t\tword from annotation for sense 0 : 28355\n",
      "\t\t\tword  28355 is in output for sense 0 with probability: 0.2075471698113207 and weight: 1.0\n",
      "\t\tword from annotation for sense 0 : 69419\n",
      "\t\t\tword  69419 is in output for sense 0 with probability: 0.16273584905660377 and weight: 0.25\n",
      "\t\tword from annotation for sense 0 : 57460\n",
      "\t\tword from annotation for sense 0 : 114587\n",
      "\t\t\tword  114587 is in output for sense 0 with probability: 0.0990566037735849 and weight: 1.0\n",
      "\t\tword from annotation for sense 0 : 42071\n",
      "\t\tword from annotation for sense 0 : 35267\n",
      "\t\t\tword  35267 is in output for sense 0 with probability: 0.08254716981132075 and weight: 1.0\n",
      "\t\tword from annotation for sense 0 : 51647\n",
      "\t\t\tword  51647 is in output for sense 0 with probability: 0.08254716981132075 and weight: 1.0\n",
      "\t\tword from annotation for sense 0 : 64448\n",
      "\t\t\tword  64448 is in output for sense 0 with probability: 0.05424528301886792 and weight: 1.0\n",
      "\t\tword from annotation for sense 0 : 45980\n",
      "\t\t\tword  45980 is in output for sense 0 with probability: 0.042452830188679236 and weight: 0.5\n",
      "\t\tword from annotation for sense 0 : 53826\n",
      "\texpert sense number  1 mus-4\n",
      "\t\tword from annotation for sense 0 : 28355\n",
      "\t\tword from annotation for sense 0 : 69419\n",
      "\t\t\tword  69419 is in output for sense 0 with probability: 0.16273584905660377 and weight: 0.25\n",
      "\t\tword from annotation for sense 0 : 57460\n",
      "\t\t\tword  57460 is in output for sense 0 with probability: 0.1320754716981132 and weight: 1.0\n",
      "\t\tword from annotation for sense 0 : 114587\n",
      "\t\tword from annotation for sense 0 : 42071\n",
      "\t\t\tword  42071 is in output for sense 0 with probability: 0.09669811320754716 and weight: 1.0\n",
      "\t\tword from annotation for sense 0 : 35267\n",
      "\t\tword from annotation for sense 0 : 51647\n",
      "\t\tword from annotation for sense 0 : 64448\n",
      "\t\tword from annotation for sense 0 : 45980\n",
      "\t\tword from annotation for sense 0 : 53826\n",
      "\texpert sense number  2 w\n",
      "\t\tword from annotation for sense 0 : 28355\n",
      "\t\tword from annotation for sense 0 : 69419\n",
      "\t\t\tword  69419 is in output for sense 0 with probability: 0.16273584905660377 and weight: 0.25\n",
      "\t\tword from annotation for sense 0 : 57460\n",
      "\t\tword from annotation for sense 0 : 114587\n",
      "\t\tword from annotation for sense 0 : 42071\n",
      "\t\tword from annotation for sense 0 : 35267\n",
      "\t\tword from annotation for sense 0 : 51647\n",
      "\t\tword from annotation for sense 0 : 64448\n",
      "\t\tword from annotation for sense 0 : 45980\n",
      "\t\tword from annotation for sense 0 : 53826\n",
      "\t\t\tword  53826 is in output for sense 0 with probability: 0.04009433962264151 and weight: 1.0\n",
      "\texpert sense number  3 mus-2\n",
      "\t\tword from annotation for sense 0 : 28355\n",
      "\t\tword from annotation for sense 0 : 69419\n",
      "\t\t\tword  69419 is in output for sense 0 with probability: 0.16273584905660377 and weight: 0.25\n",
      "\t\tword from annotation for sense 0 : 57460\n",
      "\t\tword from annotation for sense 0 : 114587\n",
      "\t\tword from annotation for sense 0 : 42071\n",
      "\t\tword from annotation for sense 0 : 35267\n",
      "\t\tword from annotation for sense 0 : 51647\n",
      "\t\tword from annotation for sense 0 : 64448\n",
      "\t\tword from annotation for sense 0 : 45980\n",
      "\t\t\tword  45980 is in output for sense 0 with probability: 0.042452830188679236 and weight: 0.5\n",
      "\t\tword from annotation for sense 0 : 53826\n",
      "output sense 1\n",
      "\texpert sense number  0 mus-1\n",
      "\t\tword from annotation for sense 1 : 79223\n",
      "\t\t\tword  79223 is in output for sense 1 with probability: 0.17260273972602735 and weight: 0.5\n",
      "\t\tword from annotation for sense 1 : 92927\n",
      "\t\tword from annotation for sense 1 : 46574\n",
      "\t\tword from annotation for sense 1 : 67660\n",
      "\t\tword from annotation for sense 1 : 103085\n",
      "\t\tword from annotation for sense 1 : 86112\n",
      "\t\tword from annotation for sense 1 : 101982\n",
      "\t\tword from annotation for sense 1 : 75808\n",
      "\t\tword from annotation for sense 1 : 68539\n",
      "\t\tword from annotation for sense 1 : 54607\n",
      "\texpert sense number  1 mus-4\n",
      "\t\tword from annotation for sense 1 : 79223\n",
      "\t\tword from annotation for sense 1 : 92927\n",
      "\t\tword from annotation for sense 1 : 46574\n",
      "\t\tword from annotation for sense 1 : 67660\n",
      "\t\tword from annotation for sense 1 : 103085\n",
      "\t\t\tword  103085 is in output for sense 1 with probability: 0.09863013698630134 and weight: 1.0\n",
      "\t\tword from annotation for sense 1 : 86112\n",
      "\t\tword from annotation for sense 1 : 101982\n",
      "\t\tword from annotation for sense 1 : 75808\n",
      "\t\tword from annotation for sense 1 : 68539\n",
      "\t\tword from annotation for sense 1 : 54607\n",
      "\texpert sense number  2 w\n",
      "\t\tword from annotation for sense 1 : 79223\n",
      "\t\t\tword  79223 is in output for sense 1 with probability: 0.17260273972602735 and weight: 0.5\n",
      "\t\tword from annotation for sense 1 : 92927\n",
      "\t\t\tword  92927 is in output for sense 1 with probability: 0.1561643835616438 and weight: 1.0\n",
      "\t\tword from annotation for sense 1 : 46574\n",
      "\t\tword from annotation for sense 1 : 67660\n",
      "\t\tword from annotation for sense 1 : 103085\n",
      "\t\tword from annotation for sense 1 : 86112\n",
      "\t\t\tword  86112 is in output for sense 1 with probability: 0.08219178082191778 and weight: 1.0\n",
      "\t\tword from annotation for sense 1 : 101982\n",
      "\t\tword from annotation for sense 1 : 75808\n",
      "\t\tword from annotation for sense 1 : 68539\n",
      "\t\tword from annotation for sense 1 : 54607\n",
      "\texpert sense number  3 mus-2\n",
      "\t\tword from annotation for sense 1 : 79223\n",
      "\t\tword from annotation for sense 1 : 92927\n",
      "\t\tword from annotation for sense 1 : 46574\n",
      "\t\tword from annotation for sense 1 : 67660\n",
      "\t\tword from annotation for sense 1 : 103085\n",
      "\t\tword from annotation for sense 1 : 86112\n",
      "\t\tword from annotation for sense 1 : 101982\n",
      "\t\t\tword  101982 is in output for sense 1 with probability: 0.07945205479452053 and weight: 1.0\n",
      "\t\tword from annotation for sense 1 : 75808\n",
      "\t\tword from annotation for sense 1 : 68539\n",
      "\t\t\tword  68539 is in output for sense 1 with probability: 0.06575342465753423 and weight: 1.0\n",
      "\t\tword from annotation for sense 1 : 54607\n",
      "output sense 2\n",
      "\texpert sense number  0 mus-1\n",
      "\t\tword from annotation for sense 2 : 62258\n",
      "\t\tword from annotation for sense 2 : 64586\n",
      "\t\t\tword  64586 is in output for sense 2 with probability: 0.12745098039215685 and weight: 1.0\n",
      "\t\tword from annotation for sense 2 : 58271\n",
      "\t\tword from annotation for sense 2 : nlsj86871\n",
      "\t\tword from annotation for sense 2 : 101851\n",
      "\t\tword from annotation for sense 2 : nlsj183\n",
      "\t\tword from annotation for sense 2 : 75653\n",
      "\t\tword from annotation for sense 2 : nlsj59923\n",
      "\t\t\tword  nlsj59923 is in output for sense 2 with probability: 0.07352941176470587 and weight: 1.0\n",
      "\t\tword from annotation for sense 2 : 70105\n",
      "\t\t\tword  70105 is in output for sense 2 with probability: 0.06372549019607843 and weight: 1.0\n",
      "\t\tword from annotation for sense 2 : 82758\n",
      "\t\t\tword  82758 is in output for sense 2 with probability: 0.058823529411764705 and weight: 0.5\n",
      "\texpert sense number  1 mus-4\n",
      "\t\tword from annotation for sense 2 : 62258\n",
      "\t\tword from annotation for sense 2 : 64586\n",
      "\t\tword from annotation for sense 2 : 58271\n",
      "\t\tword from annotation for sense 2 : nlsj86871\n",
      "\t\tword from annotation for sense 2 : 101851\n",
      "\t\t\tword  101851 is in output for sense 2 with probability: 0.09558823529411764 and weight: 1.0\n",
      "\t\tword from annotation for sense 2 : nlsj183\n",
      "\t\tword from annotation for sense 2 : 75653\n",
      "\t\tword from annotation for sense 2 : nlsj59923\n",
      "\t\tword from annotation for sense 2 : 70105\n",
      "\t\tword from annotation for sense 2 : 82758\n",
      "\texpert sense number  2 w\n",
      "\t\tword from annotation for sense 2 : 62258\n",
      "\t\tword from annotation for sense 2 : 64586\n",
      "\t\tword from annotation for sense 2 : 58271\n",
      "\t\tword from annotation for sense 2 : nlsj86871\n",
      "\t\tword from annotation for sense 2 : 101851\n",
      "\t\tword from annotation for sense 2 : nlsj183\n",
      "\t\tword from annotation for sense 2 : 75653\n",
      "\t\tword from annotation for sense 2 : nlsj59923\n",
      "\t\tword from annotation for sense 2 : 70105\n",
      "\t\tword from annotation for sense 2 : 82758\n",
      "\texpert sense number  3 mus-2\n",
      "\t\tword from annotation for sense 2 : 62258\n",
      "\t\t\tword  62258 is in output for sense 2 with probability: 0.19607843137254902 and weight: 1.0\n",
      "\t\tword from annotation for sense 2 : 64586\n",
      "\t\tword from annotation for sense 2 : 58271\n",
      "\t\t\tword  58271 is in output for sense 2 with probability: 0.11764705882352941 and weight: 1.0\n",
      "\t\tword from annotation for sense 2 : nlsj86871\n",
      "\t\t\tword  nlsj86871 is in output for sense 2 with probability: 0.10049019607843138 and weight: 1.0\n",
      "\t\tword from annotation for sense 2 : 101851\n",
      "\t\tword from annotation for sense 2 : nlsj183\n",
      "\t\tword from annotation for sense 2 : 75653\n",
      "\t\t\tword  75653 is in output for sense 2 with probability: 0.07598039215686274 and weight: 1.0\n",
      "\t\tword from annotation for sense 2 : nlsj59923\n",
      "\t\tword from annotation for sense 2 : 70105\n",
      "\t\tword from annotation for sense 2 : 82758\n",
      "\t\t\tword  82758 is in output for sense 2 with probability: 0.058823529411764705 and weight: 0.5\n",
      "output sense 3\n",
      "\texpert sense number  0 mus-1\n",
      "\t\tword from annotation for sense 3 : 102000\n",
      "\t\t\tword  102000 is in output for sense 3 with probability: 0.14893617021276598 and weight: 0.5\n",
      "\t\tword from annotation for sense 3 : 70495\n",
      "\t\tword from annotation for sense 3 : 7182\n",
      "\t\t\tword  7182 is in output for sense 3 with probability: 0.12340425531914895 and weight: 1.0\n",
      "\t\tword from annotation for sense 3 : 70958\n",
      "\t\t\tword  70958 is in output for sense 3 with probability: 0.1021276595744681 and weight: 0.5\n",
      "\t\tword from annotation for sense 3 : 49589\n",
      "\t\t\tword  49589 is in output for sense 3 with probability: 0.08936170212765958 and weight: 0.5\n",
      "\t\tword from annotation for sense 3 : 83174\n",
      "\t\tword from annotation for sense 3 : 106676\n",
      "\t\tword from annotation for sense 3 : 22209\n",
      "\t\t\tword  22209 is in output for sense 3 with probability: 0.07659574468085106 and weight: 0.5\n",
      "\t\tword from annotation for sense 3 : 93388\n",
      "\t\t\tword  93388 is in output for sense 3 with probability: 0.0723404255319149 and weight: 0.3333333333333333\n",
      "\t\tword from annotation for sense 3 : 16132\n",
      "\texpert sense number  1 mus-4\n",
      "\t\tword from annotation for sense 3 : 102000\n",
      "\t\t\tword  102000 is in output for sense 3 with probability: 0.14893617021276598 and weight: 0.5\n",
      "\t\tword from annotation for sense 3 : 70495\n",
      "\t\t\tword  70495 is in output for sense 3 with probability: 0.14893617021276598 and weight: 1.0\n",
      "\t\tword from annotation for sense 3 : 7182\n",
      "\t\tword from annotation for sense 3 : 70958\n",
      "\t\tword from annotation for sense 3 : 49589\n",
      "\t\tword from annotation for sense 3 : 83174\n",
      "\t\t\tword  83174 is in output for sense 3 with probability: 0.08936170212765958 and weight: 1.0\n",
      "\t\tword from annotation for sense 3 : 106676\n",
      "\t\t\tword  106676 is in output for sense 3 with probability: 0.08085106382978724 and weight: 1.0\n",
      "\t\tword from annotation for sense 3 : 22209\n",
      "\t\t\tword  22209 is in output for sense 3 with probability: 0.07659574468085106 and weight: 0.5\n",
      "\t\tword from annotation for sense 3 : 93388\n",
      "\t\t\tword  93388 is in output for sense 3 with probability: 0.0723404255319149 and weight: 0.3333333333333333\n",
      "\t\tword from annotation for sense 3 : 16132\n",
      "\texpert sense number  2 w\n",
      "\t\tword from annotation for sense 3 : 102000\n",
      "\t\tword from annotation for sense 3 : 70495\n",
      "\t\tword from annotation for sense 3 : 7182\n",
      "\t\tword from annotation for sense 3 : 70958\n",
      "\t\t\tword  70958 is in output for sense 3 with probability: 0.1021276595744681 and weight: 0.5\n",
      "\t\tword from annotation for sense 3 : 49589\n",
      "\t\t\tword  49589 is in output for sense 3 with probability: 0.08936170212765958 and weight: 0.5\n",
      "\t\tword from annotation for sense 3 : 83174\n",
      "\t\tword from annotation for sense 3 : 106676\n",
      "\t\tword from annotation for sense 3 : 22209\n",
      "\t\tword from annotation for sense 3 : 93388\n",
      "\t\tword from annotation for sense 3 : 16132\n",
      "\t\t\tword  16132 is in output for sense 3 with probability: 0.06808510638297872 and weight: 1.0\n",
      "\texpert sense number  3 mus-2\n",
      "\t\tword from annotation for sense 3 : 102000\n",
      "\t\tword from annotation for sense 3 : 70495\n",
      "\t\tword from annotation for sense 3 : 7182\n",
      "\t\tword from annotation for sense 3 : 70958\n",
      "\t\tword from annotation for sense 3 : 49589\n",
      "\t\tword from annotation for sense 3 : 83174\n",
      "\t\tword from annotation for sense 3 : 106676\n",
      "\t\tword from annotation for sense 3 : 22209\n",
      "\t\tword from annotation for sense 3 : 93388\n",
      "\t\t\tword  93388 is in output for sense 3 with probability: 0.0723404255319149 and weight: 0.3333333333333333\n",
      "\t\tword from annotation for sense 3 : 16132\n",
      "output sense 4\n",
      "\texpert sense number  0 mus-1\n",
      "\t\tword from annotation for sense 4 : nlsj5634\n",
      "\t\tword from annotation for sense 4 : 61925\n",
      "\t\t\tword  61925 is in output for sense 4 with probability: 0.13909774436090228 and weight: 1.0\n",
      "\t\tword from annotation for sense 4 : 12620\n",
      "\t\tword from annotation for sense 4 : 69419\n",
      "\t\t\tword  69419 is in output for sense 4 with probability: 0.10150375939849625 and weight: 0.25\n",
      "\t\tword from annotation for sense 4 : 104421\n",
      "\t\t\tword  104421 is in output for sense 4 with probability: 0.10150375939849625 and weight: 0.5\n",
      "\t\tword from annotation for sense 4 : 91085\n",
      "\t\t\tword  91085 is in output for sense 4 with probability: 0.09022556390977446 and weight: 1.0\n",
      "\t\tword from annotation for sense 4 : 71308\n",
      "\t\t\tword  71308 is in output for sense 4 with probability: 0.08270676691729324 and weight: 0.5\n",
      "\t\tword from annotation for sense 4 : 115748\n",
      "\t\tword from annotation for sense 4 : 19641\n",
      "\t\t\tword  19641 is in output for sense 4 with probability: 0.07518796992481204 and weight: 0.5\n",
      "\t\tword from annotation for sense 4 : 5390\n",
      "\texpert sense number  1 mus-4\n",
      "\t\tword from annotation for sense 4 : nlsj5634\n",
      "\t\tword from annotation for sense 4 : 61925\n",
      "\t\tword from annotation for sense 4 : 12620\n",
      "\t\tword from annotation for sense 4 : 69419\n",
      "\t\t\tword  69419 is in output for sense 4 with probability: 0.10150375939849625 and weight: 0.25\n",
      "\t\tword from annotation for sense 4 : 104421\n",
      "\t\tword from annotation for sense 4 : 91085\n",
      "\t\tword from annotation for sense 4 : 71308\n",
      "\t\tword from annotation for sense 4 : 115748\n",
      "\t\t\tword  115748 is in output for sense 4 with probability: 0.07894736842105265 and weight: 1.0\n",
      "\t\tword from annotation for sense 4 : 19641\n",
      "\t\tword from annotation for sense 4 : 5390\n",
      "\texpert sense number  2 w\n",
      "\t\tword from annotation for sense 4 : nlsj5634\n",
      "\t\tword from annotation for sense 4 : 61925\n",
      "\t\tword from annotation for sense 4 : 12620\n",
      "\t\tword from annotation for sense 4 : 69419\n",
      "\t\t\tword  69419 is in output for sense 4 with probability: 0.10150375939849625 and weight: 0.25\n",
      "\t\tword from annotation for sense 4 : 104421\n",
      "\t\t\tword  104421 is in output for sense 4 with probability: 0.10150375939849625 and weight: 0.5\n",
      "\t\tword from annotation for sense 4 : 91085\n",
      "\t\tword from annotation for sense 4 : 71308\n",
      "\t\t\tword  71308 is in output for sense 4 with probability: 0.08270676691729324 and weight: 0.5\n",
      "\t\tword from annotation for sense 4 : 115748\n",
      "\t\tword from annotation for sense 4 : 19641\n",
      "\t\tword from annotation for sense 4 : 5390\n",
      "\t\t\tword  5390 is in output for sense 4 with probability: 0.07142857142857144 and weight: 1.0\n",
      "\texpert sense number  3 mus-2\n",
      "\t\tword from annotation for sense 4 : nlsj5634\n",
      "\t\t\tword  nlsj5634 is in output for sense 4 with probability: 0.14661654135338348 and weight: 1.0\n",
      "\t\tword from annotation for sense 4 : 61925\n",
      "\t\tword from annotation for sense 4 : 12620\n",
      "\t\tword from annotation for sense 4 : 69419\n",
      "\t\t\tword  69419 is in output for sense 4 with probability: 0.10150375939849625 and weight: 0.25\n",
      "\t\tword from annotation for sense 4 : 104421\n",
      "\t\tword from annotation for sense 4 : 91085\n",
      "\t\tword from annotation for sense 4 : 71308\n",
      "\t\tword from annotation for sense 4 : 115748\n",
      "\t\tword from annotation for sense 4 : 19641\n",
      "\t\t\tword  19641 is in output for sense 4 with probability: 0.07518796992481204 and weight: 0.5\n",
      "\t\tword from annotation for sense 4 : 5390\n"
     ]
    }
   ],
   "source": [
    "for key in k_words_with_prob.keys():\n",
    "    print(\"output sense\",key)\n",
    "    for i in range(0,number_of_s):\n",
    "        print(\"\\texpert sense number \", i, expert_senses[i])\n",
    "        for second_key in k_words_with_prob[key].keys(): # Barbara's note: shouldn't it be k_words_with_prob[i] here?\n",
    "            print(\"\\t\\tword from annotation for sense\", key, \":\", second_key)\n",
    "            if second_key in dict_of_words[expert_senses[i]]:\n",
    "                print(\"\\t\\t\\tword \", second_key, \"is in output for sense\", key, \"with probability:\", k_words_with_prob[key][second_key], \"and weight:\", word_weight[second_key])\n",
    "\n",
    "                \n",
    "# Here we get all the senses and for each sense we do a matching between the k words and s words and get the probability\n",
    "# For some reason the first word for each sense arrives several times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of expert senses s: 4\n",
      "number of model output senses k: 5\n",
      "\n",
      "\n",
      "Choose best match for k = 0\n",
      "k = 0 \t s = 0 (= expert sense mus-1 )\t conf[k,s] = 8.360259433962197\n",
      "k = 0 \t s = 1 (= expert sense mus-4 )\t conf[k,s] = 8.100825471698055\n",
      "k = 0 \t s = 2 (= expert sense w )\t conf[k,s] = 7.647995283018815\n",
      "k = 0 \t s = 3 (= expert sense mus-2 )\t conf[k,s] = 7.671580188679194\n",
      "\n",
      "\n",
      "Choose best match for k = 1\n",
      "k = 1 \t s = 0 (= expert sense mus-1 )\t conf[k,s] = 0.3452054794520548\n",
      "k = 1 \t s = 1 (= expert sense mus-4 )\t conf[k,s] = 0.19726027397260265\n",
      "k = 1 \t s = 2 (= expert sense w )\t conf[k,s] = 0.5835616438356163\n",
      "k = 1 \t s = 3 (= expert sense mus-2 )\t conf[k,s] = 0.6219178082191781\n",
      "\n",
      "\n",
      "Choose best match for k = 2\n",
      "k = 2 \t s = 0 (= expert sense mus-1 )\t conf[k,s] = 0.4509803921568625\n",
      "k = 2 \t s = 1 (= expert sense mus-4 )\t conf[k,s] = 0.4779411764705884\n",
      "k = 2 \t s = 2 (= expert sense w )\t conf[k,s] = 0\n",
      "k = 2 \t s = 3 (= expert sense mus-2 )\t conf[k,s] = 2.012254901960786\n",
      "\n",
      "\n",
      "Choose best match for k = 3\n",
      "k = 3 \t s = 0 (= expert sense mus-1 )\t conf[k,s] = 1.9390070921985874\n",
      "k = 3 \t s = 1 (= expert sense mus-4 )\t conf[k,s] = 3.5539007092198607\n",
      "k = 3 \t s = 2 (= expert sense w )\t conf[k,s] = 0.457446808510638\n",
      "k = 3 \t s = 3 (= expert sense mus-2 )\t conf[k,s] = 0.09645390070921984\n",
      "\n",
      "\n",
      "Choose best match for k = 4\n",
      "k = 4 \t s = 0 (= expert sense mus-1 )\t conf[k,s] = 5.87312030075191\n",
      "k = 4 \t s = 1 (= expert sense mus-4 )\t conf[k,s] = 4.982142857142889\n",
      "k = 4 \t s = 2 (= expert sense w )\t conf[k,s] = 5.452067669172961\n",
      "k = 4 \t s = 3 (= expert sense mus-2 )\t conf[k,s] = 5.1550751879699614\n"
     ]
    }
   ],
   "source": [
    "## Calculating confidence score for each (words_of_k,words_of_s) pair\n",
    "\n",
    "# conf(k,s) = (p1*match(w1,s)+p2*match(w1,s)+px(wx,s))/10\n",
    "        # match(wx,s) =   1/number_of_senses_assigned_to_wx if s_is_one_of_them \n",
    "\n",
    "##### TODO: for now conf[k,s] is multiplied by the number of expert senses --- FIX\n",
    "    \n",
    "print(\"number of expert senses s:\",number_of_s)\n",
    "print(\"number of model output senses k:\",len(k_words_with_prob.keys()))\n",
    "compteur = 0\n",
    "\n",
    "match = dict()\n",
    "conf = dict()\n",
    "for k in k_words_with_prob.keys():  # for each output sense, we go through...\n",
    "    print(\"\\n\")\n",
    "    print(\"Choose best match for k =\",k)\n",
    "    for s in range(0,number_of_s):       # each expert sense\n",
    "        \n",
    "        conf[k,s] = 0 \n",
    "        \n",
    "        #print(\"expert sense\",s)\n",
    "        for mot in k_words_with_prob[k]:      # for each word within output by the model for the output sense\n",
    "            #print(k,mot)\n",
    "            #print(expert_senses[s])\n",
    "            \n",
    "            if mot in dict_of_words[expert_senses[s]]:  # if that word exists in the list of expert words for that sense\n",
    "                \n",
    "                #print(s,dict_of_words[expert_senses[s]])\n",
    "                #print(k_words_with_prob[k][mot])\n",
    "                \n",
    "                for word in list_of_all_words:  # this help getting a key for a dictionary later on\n",
    "                    if mot == word:\n",
    "                        match_weighted = float((k_words_with_prob[k][mot]))*word_weight[word] #this dictionary cfr comment on line 24\n",
    "                        # word_weight[word] is already \"1/number_of_expert_senses_assigned_to_this_word\"\n",
    "                        \n",
    "                        #print(\"sense\",expert_senses[s],\"word\",word,\"match_weighted\",match_weighted)\n",
    "                        \n",
    "                        #print(k,s,conf[k,s])\n",
    "\n",
    "                        \n",
    "                        # To fix? \n",
    "                        # The way the code works is that all matches happen number_of_s times\n",
    "                        # (number_of_s = number of expert senses)\n",
    "                        # easy fix is to divide the match score by number_of_s\n",
    "                        \n",
    "                        conf[k,s] = conf[k,s] + match_weighted/4\n",
    "                        \n",
    "                    #else: \n",
    "                        #print(word,\"has no match for sense\",expert_senses[s])\n",
    "                        #print(word,word_weight[word],\"match\",k_words_with_prob[k][mot],\"match weighted\",match_weighted)\n",
    "                    #print(\"test1\")\n",
    "                #print(\"test2\")\n",
    "                \n",
    "                    #compteur += 1\n",
    "                \n",
    "        if (k,s) in conf.keys():\n",
    "        \n",
    "            conf[k,s] = conf[k,s] \n",
    "            print(\"k =\",k,\"\\t s =\",s,\"(= expert sense\",expert_senses[s],\")\\t conf[k,s] =\",conf[k,s])\n",
    "            \n",
    "            #print(compteur)\n",
    "            \n",
    "    #print(k_words_with_prob[k])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Barbara's note: \"key\" and \"i\" may be different senses; for example, for key=0, this is the first sense in the output, and i=0 is the first sense annotated by the expert. I think what we want here is to try matching all key values with all i values; for each (key, i) pair, we get the conf(key, i), as in pages 5 ff. of the Goals and plan.docx document.\n",
    "\n",
    "Then, when possible, we can pick the best \"i\" for each \"key\"; we haven't yet decided how, but it will probably have to do with the maximum conf value.\n",
    "\n",
    "Once we have a key-->i mapping, we can calculate precision and recall.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: How well does the model assign the right words to a given sense of the target word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-711-f315d04cacf9>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-711-f315d04cacf9>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    for each k:\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# For each k, we use the words given by the expert as unquestionable truth.\n",
    "# Judging the model's assignment of words to a given sense becomes a question of precision and recall.\n",
    "\n",
    "# precision is all correct w weighted by their respective probabilities / all w weighted by their probabilities\n",
    "\n",
    "for each k:\n",
    "    for each w:\n",
    "        if w in expert_list:\n",
    "            w_weight = p*1\n",
    "            numerator += w_weight\n",
    "        w_weight = p*1\n",
    "        denominator += w_weight\n",
    "    precision = numerator/denominator\n",
    "    \n",
    "# recall is all correct w weighted by their respective probabilities / all w assigned to the sense by the expert\n",
    "for each k:\n",
    "    for each w:\n",
    "        if w in expert_list:\n",
    "            w_weight = p*1\n",
    "            numerator += w_weight\n",
    "    denominator = len(expert_list)\n",
    "    recall = numerator/denominator\n",
    "    \n",
    "# f-score can be used as well\n",
    "\n",
    "for each k:\n",
    "    f_score = 2 * precision * recall / (precision+recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we have prob + word for each k,s pair we can calculate precision and recall \n",
    "\n",
    "list_with_ks = list()  # this list stores the k,s matches found above\n",
    "list_with_ks = [\"0,0\",\"0,1\",\"0,2\",\"0,3\",\"1,0\",\"1,1\",\"1,2\",\"1,3\",\"2,0\",\"2,1\",\"2,2\",\"2,3\",\"3,0\",\"3,1\",\"3,2\",\"3,3\",\"4,0\",\"4,1\",\"4,2\",\"4,3\"]\n",
    "\n",
    "\n",
    "for key in list_with_ks:\n",
    "    numerator_recall = 0\n",
    "    denominator_precision = 0\n",
    "    numerator_precision = 0\n",
    "    for word in k_words_with_prob[int(key[0])]: \n",
    "        w_weight_precision = k_words_with_prob[int(key[0])][word] * 1\n",
    "        if word in dict_of_words[expert_senses[int(key[2])]]:   \n",
    "            w_weight_recall = k_words_with_prob[int(key[0])][word] * 1\n",
    "            numerator_recall += float(w_weight_recall)\n",
    "            #w_weight_precision = k_words_with_prob[int(key[0])][word] * 1  this was moved above the if\n",
    "            # cfr Valerio's email from March 28\n",
    "            numerator_precision += float(w_weight_precision)\n",
    "    \n",
    "        \n",
    "        denominator_precision += float(w_weight_precision)\n",
    "    denominator_recall = len(dict_of_words[expert_senses[int(key[2])]])\n",
    "    numerator_recall = numerator_recall*10\n",
    "            \n",
    "    print(\"For pair ks\",key,\":\")\n",
    "    print(\"The RECALL is\",numerator_recall,\"/\",denominator_recall,\"=\",numerator_recall/denominator_recall)\n",
    "    if numerator_precision == 0:\n",
    "        print(\"The PRECISION IS NA\")\n",
    "    else:\n",
    "        print(\"The PRECISION is\",numerator_precision,\"/\",denominator_precision,\"=\",numerator_precision/denominator_precision,\"\\n\")\n",
    "    if (numerator_precision/denominator_precision)+(numerator_recall/denominator_recall) != 0:\n",
    "        print(\"The F-SCORE is\", (2*(numerator_precision/denominator_precision)*(numerator_recall/denominator_recall)/((numerator_precision/denominator_precision)+(numerator_recall/denominator_recall))),\"\\n\")\n",
    "    else:\n",
    "        print(\"No F-SCORE, can't divide by 0\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'28355': 0.2075471698113207, '69419': 0.16273584905660377, '57460': 0.1320754716981132, '114587': 0.0990566037735849, '42071': 0.09669811320754716, '35267': 0.08254716981132075, '51647': 0.08254716981132075, '64448': 0.05424528301886792, '45980': 0.042452830188679236, '53826': 0.04009433962264151}, 1: {'79223': 0.17260273972602735, '92927': 0.1561643835616438, '46574': 0.10410958904109587, '67660': 0.10410958904109587, '103085': 0.09863013698630134, '86112': 0.08219178082191778, '101982': 0.07945205479452053, '75808': 0.07123287671232874, '68539': 0.06575342465753423, '54607': 0.06575342465753423}, 2: {'62258': 0.19607843137254902, '64586': 0.12745098039215685, '58271': 0.11764705882352941, 'nlsj86871': 0.10049019607843138, '101851': 0.09558823529411764, 'nlsj183': 0.0906862745098039, '75653': 0.07598039215686274, 'nlsj59923': 0.07352941176470587, '70105': 0.06372549019607843, '82758': 0.058823529411764705}, 3: {'102000': 0.14893617021276598, '70495': 0.14893617021276598, '7182': 0.12340425531914895, '70958': 0.1021276595744681, '49589': 0.08936170212765958, '83174': 0.08936170212765958, '106676': 0.08085106382978724, '22209': 0.07659574468085106, '93388': 0.0723404255319149, '16132': 0.06808510638297872}, 4: {'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228, '12620': 0.11278195488721805, '69419': 0.10150375939849625, '104421': 0.10150375939849625, '91085': 0.09022556390977446, '71308': 0.08270676691729324, '115748': 0.07894736842105265, '19641': 0.07518796992481204, '5390': 0.07142857142857144}}\n"
     ]
    }
   ],
   "source": [
    "print(k_words_with_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mus-1', 'mus-4', 'w', 'mus-2']\n"
     ]
    }
   ],
   "source": [
    "print(expert_senses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-714-d6839a6dbd08>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-714-d6839a6dbd08>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    #print(i,dict_of_words[i])\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "for i in expert_senses:\n",
    "    #print(i,dict_of_words[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qx: Model(s) comparison againstannotated subcorpus (sense importance evolution + sense emergence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode\n",
    "\n",
    "1. Parse senses_target.txt to get:\n",
    "\n",
    "    1.1 the date\n",
    "    \n",
    "    1.2 the number of senses at that date\n",
    "    \n",
    "    1.3 the number of uses of each sense at that date\n",
    "    \n",
    "    \n",
    "2. Using the numbers found in 1.3, plot the emergence of new senses and the distribution of others\n",
    "\n",
    "\n",
    "confidence interval!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new earliest sense: -430\n",
      "new latest sense: -430\n",
      "new latest sense: -425\n",
      "new latest sense: -422\n",
      "new latest sense: -420\n",
      "new latest sense: -415\n",
      "new latest sense: -370\n",
      "new latest sense: -362\n",
      "new latest sense: -355\n",
      "new latest sense: -350\n",
      "new latest sense: -335\n",
      "new latest sense: -300\n",
      "new latest sense: -270\n",
      "new latest sense: -250\n",
      "new latest sense: -150\n",
      "new latest sense: -35\n",
      "new latest sense: -7\n",
      "new latest sense: 90\n",
      "new latest sense: 93\n",
      "new latest sense: 95\n",
      "new latest sense: 100\n",
      "new latest sense: 108\n",
      "new latest sense: 150\n",
      "new latest sense: 170\n",
      "new latest sense: 175\n",
      "new latest sense: 176\n",
      "new latest sense: 180\n",
      "new latest sense: 185\n",
      "new latest sense: 195\n",
      "new latest sense: 200\n",
      "new latest sense: 220\n",
      "new latest sense: 228\n",
      "new latest sense: 230\n",
      "new latest sense: 238\n",
      "new latest sense: 359\n"
     ]
    }
   ],
   "source": [
    "# this is for the expert senses (gold standard truth of sense predominance in our corpus)\n",
    "\n",
    "\n",
    "expert_senses_chart = list() # list where we store all sense ids provided by expert\n",
    "sense_year = dict()\n",
    "earliest_sense = 10000 #just to be safe\n",
    "latest_sense = -10000\n",
    "\n",
    "for s in file_senses:\n",
    "    s = s.split(\"\\t\")\n",
    "    sense = s[11] # The sense ID is after the 10th tab\n",
    "    \n",
    "    \n",
    "    try: \n",
    "        sense_year[sense].append(int(s[0])) # The date is the first tab\n",
    "    except KeyError:\n",
    "        sense_year[sense] = list()\n",
    "        sense_year[sense].append(int(s[0])) # The date is the first tab\n",
    "    \n",
    "    expert_senses_chart.append(sense)\n",
    "\n",
    "    if int(s[0]) < earliest_sense:\n",
    "        earliest_sense = int(s[0])\n",
    "        print(\"new earliest sense:\",earliest_sense)\n",
    "        \n",
    "    if int(s[0]) > latest_sense:\n",
    "        latest_sense = int(s[0])\n",
    "        print(\"new latest sense:\",latest_sense)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## this is wrong and needs to be fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.71428571428571\n",
      "-430 359\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "number_of_slices = 7 # that's what the model outputs now\n",
    "slice_duration = (latest_sense - earliest_sense)/number_of_slices\n",
    "print(slice_duration)\n",
    "print(earliest_sense,latest_sense)\n",
    "\n",
    "slice_years = dict()\n",
    "\n",
    "threshold = int(earliest_sense + slice_duration)\n",
    "blabla = earliest_sense\n",
    "\n",
    "print(type(threshold))\n",
    "\n",
    "for period in range(0,number_of_slices):\n",
    "    slice_years[period] = list()\n",
    "    \n",
    "    #for i in range(threshold,int(threshold+slice_duration)):\n",
    "    \n",
    "    \n",
    "    for i in range(earliest_sense,latest_sense):\n",
    "\n",
    "        if i < threshold:\n",
    "            \n",
    "            slice_years[period].append(i)\n",
    "            \n",
    "        elif i > threshold:\n",
    "            threshold += int(slice_duration)\n",
    "\n",
    "#print(slice_years)\n",
    "\n",
    "# slice_years[period] contains, for each period ('total of number_of_slices' periods), all possible years in that time slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [-430, -429, -428, -427, -426, -425, -424, -423, -422, -421, -420, -419, -418, -417, -416, -415, -414, -413, -412, -411, -410, -409, -408, -407, -406, -405, -404, -403, -402, -401, -400, -399, -398, -397, -396, -395, -394, -393, -392, -391, -390, -389, -388, -387, -386, -385, -384, -383, -382, -381, -380, -379, -378, -377, -376, -375, -374, -373, -372, -371, -370, -369, -368, -367, -366, -365, -364, -363, -362, -361, -360, -359, -358, -357, -356, -355, -354, -353, -352, -351, -350, -349, -348, -347, -346, -345, -344, -343, -342, -341, -340, -339, -338, -337, -336, -335, -334, -333, -332, -331, -330, -329, -328, -327, -326, -325, -324, -323, -322, -321, -320, -319, -318, -315, -314, -313, -312, -311, -310, -309, -308, -307, -306, -305, -304, -303, -302, -301, -300, -299, -298, -297, -296, -295, -294, -293, -292, -291, -290, -289, -288, -287, -286, -285, -284, -283, -282, -281, -280, -279, -278, -277, -276, -275, -274, -273, -272, -271, -270, -269, -268, -267, -266, -265, -264, -263, -262, -261, -260, -259, -258, -257, -256, -255, -254, -253, -252, -251, -250, -249, -248, -247, -246, -245, -244, -243, -242, -241, -240, -239, -238, -237, -236, -235, -234, -233, -232, -231, -230, -229, -228, -227, -226, -225, -224, -223, -222, -221, -220, -219, -218, -217, -216, -215, -214, -213, -212, -211, -210, -209, -208, -207, -206, -203, -202, -201, -200, -199, -198, -197, -196, -195, -194, -193, -192, -191, -190, -189, -188, -187, -186, -185, -184, -183, -182, -181, -180, -179, -178, -177, -176, -175, -174, -173, -172, -171, -170, -169, -168, -167, -166, -165, -164, -163, -162, -161, -160, -159, -158, -157, -156, -155, -154, -153, -152, -151, -150, -149, -148, -147, -146, -145, -144, -143, -142, -141, -140, -139, -138, -137, -136, -135, -134, -133, -132, -131, -130, -129, -128, -127, -126, -125, -124, -123, -122, -121, -120, -119, -118, -117, -116, -115, -114, -113, -112, -111, -110, -109, -108, -107, -106, -105, -104, -103, -102, -101, -100, -99, -98, -97, -96, -95, -94, -91, -90, -89, -88, -87, -86, -85, -84, -83, -82, -81, -80, -79, -78, -77, -76, -75, -74, -73, -72, -71, -70, -69, -68, -67, -66, -65, -64, -63, -62, -61, -60, -59, -58, -57, -56, -55, -54, -53, -52, -51, -50, -49, -48, -47, -46, -45, -44, -43, -42, -41, -40, -39, -38, -37, -36, -35, -34, -33, -32, -31, -30, -29, -28, -27, -26, -25, -24, -23, -22, -21, -20, -19, -18, -17, -16, -15, -14, -13, -12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 357, 358] \n",
      "\n",
      "1 [-430, -429, -428, -427, -426, -425, -424, -423, -422, -421, -420, -419, -418, -417, -416, -415, -414, -413, -412, -411, -410, -409, -408, -407, -406, -405, -404, -403, -402, -401, -400, -399, -398, -397, -396, -395, -394, -393, -392, -391, -390, -389, -388, -387, -386, -385, -384, -383, -382, -381, -380, -379, -378, -377, -376, -375, -374, -373, -372, -371, -370, -369, -368, -367, -366, -365, -364, -363, -362, -361, -360, -359, -358, -357, -356, -355, -354, -353, -352, -351, -350, -349, -348, -347, -346, -345, -344, -343, -342, -341, -340, -339, -338, -337, -336, -335, -334, -333, -332, -331, -330, -329, -328, -327, -326, -325, -324, -323, -322, -321, -320, -319, -318, -317, -316, -315, -314, -313, -312, -311, -310, -309, -308, -307, -306, -305, -304, -303, -302, -301, -300, -299, -298, -297, -296, -295, -294, -293, -292, -291, -290, -289, -288, -287, -286, -285, -284, -283, -282, -281, -280, -279, -278, -277, -276, -275, -274, -273, -272, -271, -270, -269, -268, -267, -266, -265, -264, -263, -262, -261, -260, -259, -258, -257, -256, -255, -254, -253, -252, -251, -250, -249, -248, -247, -246, -245, -244, -243, -242, -241, -240, -239, -238, -237, -236, -235, -234, -233, -232, -231, -230, -229, -228, -227, -226, -225, -224, -223, -222, -221, -220, -219, -218, -217, -216, -215, -214, -213, -212, -211, -210, -209, -208, -207, -206, -205, -204, -203, -202, -201, -200, -199, -198, -197, -196, -195, -194, -193, -192, -191, -190, -189, -188, -187, -186, -185, -184, -183, -182, -181, -180, -179, -178, -177, -176, -175, -174, -173, -172, -171, -170, -169, -168, -167, -166, -165, -164, -163, -162, -161, -160, -159, -158, -157, -156, -155, -154, -153, -152, -151, -150, -149, -148, -147, -146, -145, -144, -143, -142, -141, -140, -139, -138, -137, -136, -135, -134, -133, -132, -131, -130, -129, -128, -127, -126, -125, -124, -123, -122, -121, -120, -119, -118, -117, -116, -115, -114, -113, -112, -111, -110, -109, -108, -107, -106, -105, -104, -103, -102, -101, -100, -99, -98, -97, -96, -95, -94, -93, -92, -91, -90, -89, -88, -87, -86, -85, -84, -83, -82, -81, -80, -79, -78, -77, -76, -75, -74, -73, -72, -71, -70, -69, -68, -67, -66, -65, -64, -63, -62, -61, -60, -59, -58, -57, -56, -55, -54, -53, -52, -51, -50, -49, -48, -47, -46, -45, -44, -43, -42, -41, -40, -39, -38, -37, -36, -35, -34, -33, -32, -31, -30, -29, -28, -27, -26, -25, -24, -23, -22, -21, -20, -19, -18, -17, -16, -15, -14, -13, -12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358] \n",
      "\n",
      "2 [-430, -429, -428, -427, -426, -425, -424, -423, -422, -421, -420, -419, -418, -417, -416, -415, -414, -413, -412, -411, -410, -409, -408, -407, -406, -405, -404, -403, -402, -401, -400, -399, -398, -397, -396, -395, -394, -393, -392, -391, -390, -389, -388, -387, -386, -385, -384, -383, -382, -381, -380, -379, -378, -377, -376, -375, -374, -373, -372, -371, -370, -369, -368, -367, -366, -365, -364, -363, -362, -361, -360, -359, -358, -357, -356, -355, -354, -353, -352, -351, -350, -349, -348, -347, -346, -345, -344, -343, -342, -341, -340, -339, -338, -337, -336, -335, -334, -333, -332, -331, -330, -329, -328, -327, -326, -325, -324, -323, -322, -321, -320, -319, -318, -317, -316, -315, -314, -313, -312, -311, -310, -309, -308, -307, -306, -305, -304, -303, -302, -301, -300, -299, -298, -297, -296, -295, -294, -293, -292, -291, -290, -289, -288, -287, -286, -285, -284, -283, -282, -281, -280, -279, -278, -277, -276, -275, -274, -273, -272, -271, -270, -269, -268, -267, -266, -265, -264, -263, -262, -261, -260, -259, -258, -257, -256, -255, -254, -253, -252, -251, -250, -249, -248, -247, -246, -245, -244, -243, -242, -241, -240, -239, -238, -237, -236, -235, -234, -233, -232, -231, -230, -229, -228, -227, -226, -225, -224, -223, -222, -221, -220, -219, -218, -217, -216, -215, -214, -213, -212, -211, -210, -209, -208, -207, -206, -205, -204, -203, -202, -201, -200, -199, -198, -197, -196, -195, -194, -193, -192, -191, -190, -189, -188, -187, -186, -185, -184, -183, -182, -181, -180, -179, -178, -177, -176, -175, -174, -173, -172, -171, -170, -169, -168, -167, -166, -165, -164, -163, -162, -161, -160, -159, -158, -157, -156, -155, -154, -153, -152, -151, -150, -149, -148, -147, -146, -145, -144, -143, -142, -141, -140, -139, -138, -137, -136, -135, -134, -133, -132, -131, -130, -129, -128, -127, -126, -125, -124, -123, -122, -121, -120, -119, -118, -117, -116, -115, -114, -113, -112, -111, -110, -109, -108, -107, -106, -105, -104, -103, -102, -101, -100, -99, -98, -97, -96, -95, -94, -93, -92, -91, -90, -89, -88, -87, -86, -85, -84, -83, -82, -81, -80, -79, -78, -77, -76, -75, -74, -73, -72, -71, -70, -69, -68, -67, -66, -65, -64, -63, -62, -61, -60, -59, -58, -57, -56, -55, -54, -53, -52, -51, -50, -49, -48, -47, -46, -45, -44, -43, -42, -41, -40, -39, -38, -37, -36, -35, -34, -33, -32, -31, -30, -29, -28, -27, -26, -25, -24, -23, -22, -21, -20, -19, -18, -17, -16, -15, -14, -13, -12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358] \n",
      "\n",
      "3 [-430, -429, -428, -427, -426, -425, -424, -423, -422, -421, -420, -419, -418, -417, -416, -415, -414, -413, -412, -411, -410, -409, -408, -407, -406, -405, -404, -403, -402, -401, -400, -399, -398, -397, -396, -395, -394, -393, -392, -391, -390, -389, -388, -387, -386, -385, -384, -383, -382, -381, -380, -379, -378, -377, -376, -375, -374, -373, -372, -371, -370, -369, -368, -367, -366, -365, -364, -363, -362, -361, -360, -359, -358, -357, -356, -355, -354, -353, -352, -351, -350, -349, -348, -347, -346, -345, -344, -343, -342, -341, -340, -339, -338, -337, -336, -335, -334, -333, -332, -331, -330, -329, -328, -327, -326, -325, -324, -323, -322, -321, -320, -319, -318, -317, -316, -315, -314, -313, -312, -311, -310, -309, -308, -307, -306, -305, -304, -303, -302, -301, -300, -299, -298, -297, -296, -295, -294, -293, -292, -291, -290, -289, -288, -287, -286, -285, -284, -283, -282, -281, -280, -279, -278, -277, -276, -275, -274, -273, -272, -271, -270, -269, -268, -267, -266, -265, -264, -263, -262, -261, -260, -259, -258, -257, -256, -255, -254, -253, -252, -251, -250, -249, -248, -247, -246, -245, -244, -243, -242, -241, -240, -239, -238, -237, -236, -235, -234, -233, -232, -231, -230, -229, -228, -227, -226, -225, -224, -223, -222, -221, -220, -219, -218, -217, -216, -215, -214, -213, -212, -211, -210, -209, -208, -207, -206, -205, -204, -203, -202, -201, -200, -199, -198, -197, -196, -195, -194, -193, -192, -191, -190, -189, -188, -187, -186, -185, -184, -183, -182, -181, -180, -179, -178, -177, -176, -175, -174, -173, -172, -171, -170, -169, -168, -167, -166, -165, -164, -163, -162, -161, -160, -159, -158, -157, -156, -155, -154, -153, -152, -151, -150, -149, -148, -147, -146, -145, -144, -143, -142, -141, -140, -139, -138, -137, -136, -135, -134, -133, -132, -131, -130, -129, -128, -127, -126, -125, -124, -123, -122, -121, -120, -119, -118, -117, -116, -115, -114, -113, -112, -111, -110, -109, -108, -107, -106, -105, -104, -103, -102, -101, -100, -99, -98, -97, -96, -95, -94, -93, -92, -91, -90, -89, -88, -87, -86, -85, -84, -83, -82, -81, -80, -79, -78, -77, -76, -75, -74, -73, -72, -71, -70, -69, -68, -67, -66, -65, -64, -63, -62, -61, -60, -59, -58, -57, -56, -55, -54, -53, -52, -51, -50, -49, -48, -47, -46, -45, -44, -43, -42, -41, -40, -39, -38, -37, -36, -35, -34, -33, -32, -31, -30, -29, -28, -27, -26, -25, -24, -23, -22, -21, -20, -19, -18, -17, -16, -15, -14, -13, -12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358] \n",
      "\n",
      "4 [-430, -429, -428, -427, -426, -425, -424, -423, -422, -421, -420, -419, -418, -417, -416, -415, -414, -413, -412, -411, -410, -409, -408, -407, -406, -405, -404, -403, -402, -401, -400, -399, -398, -397, -396, -395, -394, -393, -392, -391, -390, -389, -388, -387, -386, -385, -384, -383, -382, -381, -380, -379, -378, -377, -376, -375, -374, -373, -372, -371, -370, -369, -368, -367, -366, -365, -364, -363, -362, -361, -360, -359, -358, -357, -356, -355, -354, -353, -352, -351, -350, -349, -348, -347, -346, -345, -344, -343, -342, -341, -340, -339, -338, -337, -336, -335, -334, -333, -332, -331, -330, -329, -328, -327, -326, -325, -324, -323, -322, -321, -320, -319, -318, -317, -316, -315, -314, -313, -312, -311, -310, -309, -308, -307, -306, -305, -304, -303, -302, -301, -300, -299, -298, -297, -296, -295, -294, -293, -292, -291, -290, -289, -288, -287, -286, -285, -284, -283, -282, -281, -280, -279, -278, -277, -276, -275, -274, -273, -272, -271, -270, -269, -268, -267, -266, -265, -264, -263, -262, -261, -260, -259, -258, -257, -256, -255, -254, -253, -252, -251, -250, -249, -248, -247, -246, -245, -244, -243, -242, -241, -240, -239, -238, -237, -236, -235, -234, -233, -232, -231, -230, -229, -228, -227, -226, -225, -224, -223, -222, -221, -220, -219, -218, -217, -216, -215, -214, -213, -212, -211, -210, -209, -208, -207, -206, -205, -204, -203, -202, -201, -200, -199, -198, -197, -196, -195, -194, -193, -192, -191, -190, -189, -188, -187, -186, -185, -184, -183, -182, -181, -180, -179, -178, -177, -176, -175, -174, -173, -172, -171, -170, -169, -168, -167, -166, -165, -164, -163, -162, -161, -160, -159, -158, -157, -156, -155, -154, -153, -152, -151, -150, -149, -148, -147, -146, -145, -144, -143, -142, -141, -140, -139, -138, -137, -136, -135, -134, -133, -132, -131, -130, -129, -128, -127, -126, -125, -124, -123, -122, -121, -120, -119, -118, -117, -116, -115, -114, -113, -112, -111, -110, -109, -108, -107, -106, -105, -104, -103, -102, -101, -100, -99, -98, -97, -96, -95, -94, -93, -92, -91, -90, -89, -88, -87, -86, -85, -84, -83, -82, -81, -80, -79, -78, -77, -76, -75, -74, -73, -72, -71, -70, -69, -68, -67, -66, -65, -64, -63, -62, -61, -60, -59, -58, -57, -56, -55, -54, -53, -52, -51, -50, -49, -48, -47, -46, -45, -44, -43, -42, -41, -40, -39, -38, -37, -36, -35, -34, -33, -32, -31, -30, -29, -28, -27, -26, -25, -24, -23, -22, -21, -20, -19, -18, -17, -16, -15, -14, -13, -12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358] \n",
      "\n",
      "5 [-430, -429, -428, -427, -426, -425, -424, -423, -422, -421, -420, -419, -418, -417, -416, -415, -414, -413, -412, -411, -410, -409, -408, -407, -406, -405, -404, -403, -402, -401, -400, -399, -398, -397, -396, -395, -394, -393, -392, -391, -390, -389, -388, -387, -386, -385, -384, -383, -382, -381, -380, -379, -378, -377, -376, -375, -374, -373, -372, -371, -370, -369, -368, -367, -366, -365, -364, -363, -362, -361, -360, -359, -358, -357, -356, -355, -354, -353, -352, -351, -350, -349, -348, -347, -346, -345, -344, -343, -342, -341, -340, -339, -338, -337, -336, -335, -334, -333, -332, -331, -330, -329, -328, -327, -326, -325, -324, -323, -322, -321, -320, -319, -318, -317, -316, -315, -314, -313, -312, -311, -310, -309, -308, -307, -306, -305, -304, -303, -302, -301, -300, -299, -298, -297, -296, -295, -294, -293, -292, -291, -290, -289, -288, -287, -286, -285, -284, -283, -282, -281, -280, -279, -278, -277, -276, -275, -274, -273, -272, -271, -270, -269, -268, -267, -266, -265, -264, -263, -262, -261, -260, -259, -258, -257, -256, -255, -254, -253, -252, -251, -250, -249, -248, -247, -246, -245, -244, -243, -242, -241, -240, -239, -238, -237, -236, -235, -234, -233, -232, -231, -230, -229, -228, -227, -226, -225, -224, -223, -222, -221, -220, -219, -218, -217, -216, -215, -214, -213, -212, -211, -210, -209, -208, -207, -206, -205, -204, -203, -202, -201, -200, -199, -198, -197, -196, -195, -194, -193, -192, -191, -190, -189, -188, -187, -186, -185, -184, -183, -182, -181, -180, -179, -178, -177, -176, -175, -174, -173, -172, -171, -170, -169, -168, -167, -166, -165, -164, -163, -162, -161, -160, -159, -158, -157, -156, -155, -154, -153, -152, -151, -150, -149, -148, -147, -146, -145, -144, -143, -142, -141, -140, -139, -138, -137, -136, -135, -134, -133, -132, -131, -130, -129, -128, -127, -126, -125, -124, -123, -122, -121, -120, -119, -118, -117, -116, -115, -114, -113, -112, -111, -110, -109, -108, -107, -106, -105, -104, -103, -102, -101, -100, -99, -98, -97, -96, -95, -94, -93, -92, -91, -90, -89, -88, -87, -86, -85, -84, -83, -82, -81, -80, -79, -78, -77, -76, -75, -74, -73, -72, -71, -70, -69, -68, -67, -66, -65, -64, -63, -62, -61, -60, -59, -58, -57, -56, -55, -54, -53, -52, -51, -50, -49, -48, -47, -46, -45, -44, -43, -42, -41, -40, -39, -38, -37, -36, -35, -34, -33, -32, -31, -30, -29, -28, -27, -26, -25, -24, -23, -22, -21, -20, -19, -18, -17, -16, -15, -14, -13, -12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358] \n",
      "\n",
      "6 [-430, -429, -428, -427, -426, -425, -424, -423, -422, -421, -420, -419, -418, -417, -416, -415, -414, -413, -412, -411, -410, -409, -408, -407, -406, -405, -404, -403, -402, -401, -400, -399, -398, -397, -396, -395, -394, -393, -392, -391, -390, -389, -388, -387, -386, -385, -384, -383, -382, -381, -380, -379, -378, -377, -376, -375, -374, -373, -372, -371, -370, -369, -368, -367, -366, -365, -364, -363, -362, -361, -360, -359, -358, -357, -356, -355, -354, -353, -352, -351, -350, -349, -348, -347, -346, -345, -344, -343, -342, -341, -340, -339, -338, -337, -336, -335, -334, -333, -332, -331, -330, -329, -328, -327, -326, -325, -324, -323, -322, -321, -320, -319, -318, -317, -316, -315, -314, -313, -312, -311, -310, -309, -308, -307, -306, -305, -304, -303, -302, -301, -300, -299, -298, -297, -296, -295, -294, -293, -292, -291, -290, -289, -288, -287, -286, -285, -284, -283, -282, -281, -280, -279, -278, -277, -276, -275, -274, -273, -272, -271, -270, -269, -268, -267, -266, -265, -264, -263, -262, -261, -260, -259, -258, -257, -256, -255, -254, -253, -252, -251, -250, -249, -248, -247, -246, -245, -244, -243, -242, -241, -240, -239, -238, -237, -236, -235, -234, -233, -232, -231, -230, -229, -228, -227, -226, -225, -224, -223, -222, -221, -220, -219, -218, -217, -216, -215, -214, -213, -212, -211, -210, -209, -208, -207, -206, -205, -204, -203, -202, -201, -200, -199, -198, -197, -196, -195, -194, -193, -192, -191, -190, -189, -188, -187, -186, -185, -184, -183, -182, -181, -180, -179, -178, -177, -176, -175, -174, -173, -172, -171, -170, -169, -168, -167, -166, -165, -164, -163, -162, -161, -160, -159, -158, -157, -156, -155, -154, -153, -152, -151, -150, -149, -148, -147, -146, -145, -144, -143, -142, -141, -140, -139, -138, -137, -136, -135, -134, -133, -132, -131, -130, -129, -128, -127, -126, -125, -124, -123, -122, -121, -120, -119, -118, -117, -116, -115, -114, -113, -112, -111, -110, -109, -108, -107, -106, -105, -104, -103, -102, -101, -100, -99, -98, -97, -96, -95, -94, -93, -92, -91, -90, -89, -88, -87, -86, -85, -84, -83, -82, -81, -80, -79, -78, -77, -76, -75, -74, -73, -72, -71, -70, -69, -68, -67, -66, -65, -64, -63, -62, -61, -60, -59, -58, -57, -56, -55, -54, -53, -52, -51, -50, -49, -48, -47, -46, -45, -44, -43, -42, -41, -40, -39, -38, -37, -36, -35, -34, -33, -32, -31, -30, -29, -28, -27, -26, -25, -24, -23, -22, -21, -20, -19, -18, -17, -16, -15, -14, -13, -12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in slice_years.keys():\n",
    "    print(key,slice_years[key],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## need to check and properly comment this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total period {0: 4, 1: 8, 2: 21, 3: 50, 4: 138, 5: 138, 6: 139}\n",
      "sense date amount {('mus-1', 0): 3, ('mus-1', 1): 7, ('mus-1', 2): 18, ('mus-1', 3): 45, ('mus-1', 4): 96, ('mus-1', 5): 96, ('mus-1', 6): 97, ('mus-4', 0): 1, ('mus-4', 1): 1, ('mus-4', 2): 3, ('mus-4', 3): 4, ('mus-4', 4): 13, ('mus-4', 5): 13, ('mus-4', 6): 13, ('w', 0): 0, ('w', 1): 0, ('w', 2): 0, ('w', 3): 1, ('w', 4): 5, ('w', 5): 5, ('w', 6): 5, ('mus-2', 0): 0, ('mus-2', 1): 0, ('mus-2', 2): 0, ('mus-2', 3): 0, ('mus-2', 4): 24, ('mus-2', 5): 24, ('mus-2', 6): 24}\n",
      "('mus-1', 0) total for this sense at this period 3 total period 4\n",
      "relative 0.75\n",
      "('mus-1', 1) total for this sense at this period 7 total period 8\n",
      "relative 0.875\n",
      "('mus-1', 2) total for this sense at this period 18 total period 21\n",
      "relative 0.8571428571428571\n",
      "('mus-1', 3) total for this sense at this period 45 total period 50\n",
      "relative 0.9\n",
      "('mus-1', 4) total for this sense at this period 96 total period 138\n",
      "relative 0.6956521739130435\n",
      "('mus-1', 5) total for this sense at this period 96 total period 138\n",
      "relative 0.6956521739130435\n",
      "('mus-1', 6) total for this sense at this period 97 total period 139\n",
      "relative 0.697841726618705\n",
      "('mus-4', 0) total for this sense at this period 1 total period 4\n",
      "relative 0.25\n",
      "('mus-4', 1) total for this sense at this period 1 total period 8\n",
      "relative 0.125\n",
      "('mus-4', 2) total for this sense at this period 3 total period 21\n",
      "relative 0.14285714285714285\n",
      "('mus-4', 3) total for this sense at this period 4 total period 50\n",
      "relative 0.08\n",
      "('mus-4', 4) total for this sense at this period 13 total period 138\n",
      "relative 0.09420289855072464\n",
      "('mus-4', 5) total for this sense at this period 13 total period 138\n",
      "relative 0.09420289855072464\n",
      "('mus-4', 6) total for this sense at this period 13 total period 139\n",
      "relative 0.09352517985611511\n",
      "('w', 0) total for this sense at this period 0 total period 4\n",
      "relative 0.0\n",
      "('w', 1) total for this sense at this period 0 total period 8\n",
      "relative 0.0\n",
      "('w', 2) total for this sense at this period 0 total period 21\n",
      "relative 0.0\n",
      "('w', 3) total for this sense at this period 1 total period 50\n",
      "relative 0.02\n",
      "('w', 4) total for this sense at this period 5 total period 138\n",
      "relative 0.036231884057971016\n",
      "('w', 5) total for this sense at this period 5 total period 138\n",
      "relative 0.036231884057971016\n",
      "('w', 6) total for this sense at this period 5 total period 139\n",
      "relative 0.03597122302158273\n",
      "('mus-2', 0) total for this sense at this period 0 total period 4\n",
      "relative 0.0\n",
      "('mus-2', 1) total for this sense at this period 0 total period 8\n",
      "relative 0.0\n",
      "('mus-2', 2) total for this sense at this period 0 total period 21\n",
      "relative 0.0\n",
      "('mus-2', 3) total for this sense at this period 0 total period 50\n",
      "relative 0.0\n",
      "('mus-2', 4) total for this sense at this period 24 total period 138\n",
      "relative 0.17391304347826086\n",
      "('mus-2', 5) total for this sense at this period 24 total period 138\n",
      "relative 0.17391304347826086\n",
      "('mus-2', 6) total for this sense at this period 24 total period 139\n",
      "relative 0.17266187050359713\n",
      "{('mus-1', 0): 0.75, ('mus-1', 1): 0.875, ('mus-1', 2): 0.8571428571428571, ('mus-1', 3): 0.9, ('mus-1', 4): 0.6956521739130435, ('mus-1', 5): 0.6956521739130435, ('mus-1', 6): 0.697841726618705, ('mus-4', 0): 0.25, ('mus-4', 1): 0.125, ('mus-4', 2): 0.14285714285714285, ('mus-4', 3): 0.08, ('mus-4', 4): 0.09420289855072464, ('mus-4', 5): 0.09420289855072464, ('mus-4', 6): 0.09352517985611511, ('w', 0): 0.0, ('w', 1): 0.0, ('w', 2): 0.0, ('w', 3): 0.02, ('w', 4): 0.036231884057971016, ('w', 5): 0.036231884057971016, ('w', 6): 0.03597122302158273, ('mus-2', 0): 0.0, ('mus-2', 1): 0.0, ('mus-2', 2): 0.0, ('mus-2', 3): 0.0, ('mus-2', 4): 0.17391304347826086, ('mus-2', 5): 0.17391304347826086, ('mus-2', 6): 0.17266187050359713}\n",
      "mus-1 0 0.75\n",
      "mus-4 0 0.25\n",
      "w 0 0.0\n",
      "mus-2 0 0.0\n",
      "mus-1 1 0.875\n",
      "mus-4 1 0.125\n",
      "w 1 0.0\n",
      "mus-2 1 0.0\n",
      "mus-1 2 0.8571428571428571\n",
      "mus-4 2 0.14285714285714285\n",
      "w 2 0.0\n",
      "mus-2 2 0.0\n",
      "mus-1 3 0.9\n",
      "mus-4 3 0.08\n",
      "w 3 0.02\n",
      "mus-2 3 0.0\n",
      "mus-1 4 0.6956521739130435\n",
      "mus-4 4 0.09420289855072464\n",
      "w 4 0.036231884057971016\n",
      "mus-2 4 0.17391304347826086\n",
      "mus-1 5 0.6956521739130435\n",
      "mus-4 5 0.09420289855072464\n",
      "w 5 0.036231884057971016\n",
      "mus-2 5 0.17391304347826086\n",
      "mus-1 6 0.697841726618705\n",
      "mus-4 6 0.09352517985611511\n",
      "w 6 0.03597122302158273\n",
      "mus-2 6 0.17266187050359713\n",
      "{0: [0.75, 0.25, 0.0, 0.0], 1: [0.875, 0.125, 0.0, 0.0], 2: [0.8571428571428571, 0.14285714285714285, 0.0, 0.0], 3: [0.9, 0.08, 0.02, 0.0], 4: [0.6956521739130435, 0.09420289855072464, 0.036231884057971016, 0.17391304347826086], 5: [0.6956521739130435, 0.09420289855072464, 0.036231884057971016, 0.17391304347826086], 6: [0.697841726618705, 0.09352517985611511, 0.03597122302158273, 0.17266187050359713]}\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "total_period = dict()\n",
    "N = number_of_slices\n",
    "\n",
    "sense_period_relative = dict()\n",
    "\n",
    "for i in range(0,number_of_slices):\n",
    "    for entry in expert_senses:\n",
    "        \n",
    "# for period i we store for each sense the number of times the sense is seen\n",
    "        \n",
    "        try:\n",
    "            total_period[i] += sense_date_amount[entry,i]\n",
    "        except KeyError:\n",
    "            total_period[i] = 0\n",
    "            total_period[i] += sense_date_amount[entry,i]\n",
    "            \n",
    "        #print(i,entry,\"+\",sense_date_amount[entry,i],\"=\",total_period[i])\n",
    "        \n",
    "        \n",
    "print(\"total period\",total_period)\n",
    "print(\"sense date amount\",sense_date_amount)\n",
    "        \n",
    "for key in sense_date_amount:\n",
    "    \n",
    "    # for each (sense,period) pair we divide the number by the total number of words at that period\n",
    "    \n",
    "    print(key,\"total for this sense at this period\",sense_date_amount[key],\"total period\",total_period[key[1]])\n",
    "    \n",
    "    sense_period_relative[key] = float(sense_date_amount[key]/total_period[key[1]])\n",
    "    print(\"relative\",sense_period_relative[key])\n",
    "  \n",
    "print(sense_period_relative)        \n",
    "\n",
    "period_relative = dict()\n",
    "temp_list = list()\n",
    "\n",
    "for i in range(0,number_of_slices):\n",
    "    temp_list = list()\n",
    "    for entry in expert_senses:\n",
    "        if len(temp_list) < len(expert_senses):\n",
    "            temp_list.append(sense_period_relative[entry,i])\n",
    "            print(entry,i,sense_period_relative[entry,i])\n",
    "        \n",
    "    period_relative[i] = temp_list\n",
    "        \n",
    "        \n",
    "print(period_relative)\n",
    "print(number_of_slices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [75, 25, 0, 0]\n",
      "1 [87, 12, 0, 0]\n",
      "2 [85, 14, 0, 0]\n",
      "3 [90, 8, 2, 0]\n",
      "4 [69, 9, 3, 17]\n",
      "5 [69, 9, 3, 17]\n",
      "6 [69, 9, 3, 17]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvFvnyVgAADZNJREFUeJzt3X+o3fV9x/Hnq0axpmuj8xLSRGZgUpHBpl5si0Os2Ya1peaPIspWggj5x3Z2HXS2/8TC/uhg9MdgOIKxS5nTSrQoXekmqdL1j2a9V938ETuDqzVZNLd0trUbONf3/rhfu9ss5px7vufknPvx+YBwzvd7vt973obL0+/93u/5JlWFJKldb5n2AJKkyTL0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktS4gaFPcmeSY0meXLHunCQPJXm2ezy7W58kf5HkUJJ/SXLJJIeXJA02zBH9XwNXH7fuVmB/VV0A7O+WAd4PXND92QncPp4xJUmjGhj6qvoW8KPjVl8L7O2e7wW2r1j/5Vr2HWBDkk3jGlaStHrrRtxvY1Ud7Z6/CGzsnm8GXlix3eFu3VGOk2Qny0f9rF+//tILL7xwpEEW/31xpP0m5dJ3XjrtEfS6xRn63rjU7wuN3+Li4g+ram7QdqOG/heqqpKs+j4KVbUb2A0wPz9fCwsLI71/PpOR9puUhV2j/XdoAjJD3xsjfn9LJ5Pk+WG2G/Wqm5dePyXTPR7r1h8Bzlux3ZZunSRpSkY9on8Q2AF8tnt8YMX6jya5B3g38OMVp3jUmbWfQmrX4B/I1uLMkpYNDH2Su4ErgXOTHAZ2sRz4e5PcBDwPXNdt/nXgGuAQ8J/AjROYWZK0CgNDX1U3vMFL206wbQE39x1KGofcNu0J/s8wP3/kkUcmPcaq1JVXDtzGmfsbZua+/GSsJDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS4wy9JDXO0EtS43r/4+DSzLpthv65wV3THkBvZh7RS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNc7LK6dhli77Ay/9kxrnEb0kNc7QS1LjDL0kNc5z9BqOv1eQ1iyP6CWpcYZekhpn6CWpcYZekhpn6CWpcYZekhpn6CWpcb1Cn+SPkjyV5Mkkdyc5M8nWJAeSHErylSRnjGtYSdLqjfyBqSSbgT8ELqqq/0pyL3A9cA3w+aq6J8lfATcBt49lWqlh9b73TXuEX1aDPyTnzGMwxMx99T11sw54a5J1wFnAUeAqYF/3+l5ge8/3kCT1MHLoq+oI8OfAD1gO/I+BReDlqnqt2+wwsPlE+yfZmWQhycLS0tKoY0iSBhg59EnOBq4FtgLvBNYDVw+7f1Xtrqr5qpqfm5sbdQxJ0gB9Tt38DvBvVbVUVf8N3A9cDmzoTuUAbAGO9JxRktRDn9D/AHhPkrOSBNgGPA08DHy422YH8EC/ESVJfYx81U1VHUiyD3gUeA14DNgN/B1wT5I/7dbtGcegUuty27Qn+GXDXAvizP2dihuA97offVXt4v/fGfw54LI+X1eSND5+MlaSGmfoJalxhl6SGmfoJalxhl6SGtfrqpuZcNupuDhpFY6/BkmSpswjeklqnKGXpMYZeklqnKGXpMYZeklqnKGXpMYZeklqnKGXpMYZeklqnKGXpMYZeklqnKGXpMalavo3BZufn6+FhYWR9k3GPExPw/x1OnN/a23mtTYvOPOp0ifBSRaran7Qdh7RS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjeoU+yYYk+5I8k+RgkvcmOSfJQ0me7R7PHtewkqTV63tE/0XgG1V1IfCbwEHgVmB/VV0A7O+WJUlTMnLok7wDuALYA1BVr1bVy8C1wN5us73A9r5DSpJG1+eIfiuwBHwpyWNJ7kiyHthYVUe7bV4ENp5o5yQ7kywkWVhaWuoxhiTpZPqEfh1wCXB7VV0M/IzjTtPU8j9Ie8J/EbGqdlfVfFXNz83N9RhDknQyfUJ/GDhcVQe65X0sh/+lJJsAusdj/UaUJPUxcuir6kXghSTv6lZtA54GHgR2dOt2AA/0mlCS1Mu6nvt/DLgryRnAc8CNLP/P494kNwHPA9f1fA9JUg+9Ql9VjwPzJ3hpW5+vK0kaHz8ZK0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mNM/SS1DhDL0mN6x36JKcleSzJ17rlrUkOJDmU5CtJzug/piRpVOM4or8FOLhi+c+Az1fVrwP/Adw0hveQJI2oV+iTbAE+ANzRLQe4CtjXbbIX2N7nPSRJ/fQ9ov8C8Eng593yrwIvV9Vr3fJhYPOJdkyyM8lCkoWlpaWeY0iS3sjIoU/yQeBYVS2Osn9V7a6q+aqan5ubG3UMSdIA63rseznwoSTXAGcCbwe+CGxIsq47qt8CHOk/piRpVCMf0VfVp6pqS1WdD1wPfLOqfh94GPhwt9kO4IHeU0qSRjaJ6+j/BPhEkkMsn7PfM4H3kCQNqc+pm1+oqkeAR7rnzwGXjePrSpL685OxktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjTP0ktQ4Qy9JjRs59EnOS/JwkqeTPJXklm79OUkeSvJs93j2+MaVJK1WnyP614A/rqqLgPcANye5CLgV2F9VFwD7u2VJ0pSMHPqqOlpVj3bPfwocBDYD1wJ7u832Atv7DilJGt1YztEnOR+4GDgAbKyqo91LLwIb32CfnUkWkiwsLS2NYwxJ0gn0Dn2StwH3AR+vqp+sfK2qCqgT7VdVu6tqvqrm5+bm+o4hSXoDvUKf5HSWI39XVd3frX4pyabu9U3AsX4jSpL66HPVTYA9wMGq+tyKlx4EdnTPdwAPjD6eJKmvdT32vRz4CPBEkse7dZ8GPgvcm+Qm4Hngun4jSpL6GDn0VfVtIG/w8rZRv64kabz8ZKwkNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNc7QS1LjDL0kNW4ioU9ydZLvJTmU5NZJvIckaThjD32S04C/BN4PXATckOSicb+PJGk4kziivww4VFXPVdWrwD3AtRN4H0nSENZN4GtuBl5YsXwYePfxGyXZCezsFl9J8r0JzLIa5wI/7PtFkjFMMjxnnry1Ni8486kyCzP/2jAbTSL0Q6mq3cDuab3/8ZIsVNX8tOdYDWeevLU2LzjzqbKWZp7EqZsjwHkrlrd06yRJUzCJ0H8XuCDJ1iRnANcDD07gfSRJQxj7qZuqei3JR4G/B04D7qyqp8b9PhMwM6eRVsGZJ2+tzQvOfKqsmZlTVdOeQZI0QX4yVpIaZ+glqXGGnrV3y4YkdyY5luTJac8yjCTnJXk4ydNJnkpyy7RnGiTJmUn+Kck/dzN/ZtozDSvJaUkeS/K1ac8yjCTfT/JEkseTLEx7nkGSbEiyL8kzSQ4mee+0ZxrkTX+Ovrtlw78Cv8vyh7u+C9xQVU9PdbCTSHIF8Arw5ar6jWnPM0iSTcCmqno0ya8Ai8D2Gf87DrC+ql5JcjrwbeCWqvrOlEcbKMkngHng7VX1wWnPM0iS7wPzVdX7w0enQpK9wD9W1R3dlYVnVdXL057rZDyiX4O3bKiqbwE/mvYcw6qqo1X1aPf8p8BBlj9BPbNq2Svd4undn5k/KkqyBfgAcMe0Z2lRkncAVwB7AKrq1VmPPBh6OPEtG2Y6QmtZkvOBi4ED051ksO4UyOPAMeChqpr5mYEvAJ8Efj7tQVahgH9IstjdGmWWbQWWgC91p8fuSLJ+2kMNYuh1yiR5G3Af8PGq+sm05xmkqv6nqn6L5U93X5Zkpk+TJfkgcKyqFqc9yyr9dlVdwvIdb2/uTk3OqnXAJcDtVXUx8DNg5n+vZ+i9ZcMp0Z3nvg+4q6run/Y8q9H9aP4wcPW0ZxngcuBD3Tnve4CrkvzNdEcarKqOdI/HgK+yfDp1Vh0GDq/46W4fy+GfaYbeWzZMXPeLzT3Awar63LTnGUaSuSQbuudvZfmX9c9Md6qTq6pPVdWWqjqf5e/jb1bVH0x5rJNKsr77BT3dKZDfA2b2arKqehF4Icm7ulXbgJm9qOB1U7t75axYi7dsSHI3cCVwbpLDwK6q2jPdqU7qcuAjwBPdOW+AT1fV16c40yCbgL3dVVlvAe6tqjVxueIasxH46vKxAOuAv62qb0x3pIE+BtzVHRg+B9w45XkGetNfXilJrfPUjSQ1ztBLUuMMvSQ1ztBLUuMMvSQ1ztBLUuMMvSQ17n8BkINDbFvzKXQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#valeurs = {\"p1\":[0.4, 0.55, 0.05, 0.0], \"p2\":[0.2, 0.3, 0.5, 0.0], \"p3\":[0.4, 0.2, 0.2, 0.2], \"p4\":[0.2, 0.2, 0.2, 0.4], \"p5\":[0.4, 0.55, 0.05, 0.0], \"p6\":[0.4, 0.55, 0.05, 0.0], \"p7\":[0.4, 0.55, 0.05, 0.0]}\n",
    "valeurs = period_relative\n",
    "colours = ['b','g','r','c','m','y','k']\n",
    "\n",
    "valeurs2 = dict()\n",
    "\n",
    "for key in valeurs.keys():\n",
    "    #print(key)\n",
    "    list_temp = list()\n",
    "    for item in valeurs[key]:\n",
    "        list_temp.append(int(item*100))\n",
    "    valeurs2[key] = list_temp\n",
    "\n",
    "    #for value in valeurs\n",
    "    \n",
    "\n",
    "for key,vals in valeurs2.items():\n",
    "    print(key,vals)\n",
    "    \n",
    "    for i in range(0,len(vals)):        \n",
    "        if i == 0:\n",
    "            previous = 0\n",
    "            plt.bar(x=key, height=vals[i],bottom=previous,color=colours[i])\n",
    "            \n",
    "        else:         \n",
    "            previous = vals[i-1] + previous\n",
    "            plt.bar(x=key, height=vals[i],bottom=previous,color=colours[i])\n",
    "            \n",
    "plt.xticks(range(len(valeurs2)), valeurs2.keys())\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sense: mus-1\n",
      "mus-1 97\n",
      "Sense: mus-4\n",
      "mus-4 13\n",
      "Sense: w\n",
      "w 5\n",
      "Sense: mus-2\n",
      "mus-2 24\n",
      "{('mus-1', 0): 3, ('mus-1', 1): 7, ('mus-1', 2): 18, ('mus-1', 3): 45, ('mus-1', 4): 96, ('mus-1', 5): 96, ('mus-1', 6): 97, ('mus-4', 0): 1, ('mus-4', 1): 1, ('mus-4', 2): 3, ('mus-4', 3): 4, ('mus-4', 4): 13, ('mus-4', 5): 13, ('mus-4', 6): 13, ('w', 0): 0, ('w', 1): 0, ('w', 2): 0, ('w', 3): 1, ('w', 4): 5, ('w', 5): 5, ('w', 6): 5, ('mus-2', 0): 0, ('mus-2', 1): 0, ('mus-2', 2): 0, ('mus-2', 3): 0, ('mus-2', 4): 24, ('mus-2', 5): 24, ('mus-2', 6): 24}\n"
     ]
    }
   ],
   "source": [
    "sense_date_amount = dict()\n",
    "\n",
    "\n",
    "\n",
    "for sense in sense_year.keys():\n",
    "   \n",
    "    print(\"Sense:\",sense)\n",
    "    counter = 0\n",
    "    for i in range(0,number_of_slices):\n",
    "        #print(\"period\",i,\"years for that sense in that period\",sense_year[sense])\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(sense_year[sense])\n",
    "        for year in sense_year[sense]:\n",
    "        \n",
    "            if year in slice_years[i]:\n",
    "                counter += 1\n",
    "                #print(sense_year[sense][i])\n",
    "                \n",
    "        sense_date_amount[sense,i] = counter           \n",
    "    print(sense,counter)\n",
    "    \n",
    "print(sense_date_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mus-1', 0) 3\n",
      "('mus-1', 1) 7\n",
      "('mus-1', 2) 18\n",
      "('mus-1', 3) 45\n",
      "('mus-1', 4) 96\n",
      "('mus-1', 5) 96\n",
      "('mus-1', 6) 97\n",
      "('mus-4', 0) 1\n",
      "('mus-4', 1) 1\n",
      "('mus-4', 2) 3\n",
      "('mus-4', 3) 4\n",
      "('mus-4', 4) 13\n",
      "('mus-4', 5) 13\n",
      "('mus-4', 6) 13\n",
      "('w', 0) 0\n",
      "('w', 1) 0\n",
      "('w', 2) 0\n",
      "('w', 3) 1\n",
      "('w', 4) 5\n",
      "('w', 5) 5\n",
      "('w', 6) 5\n",
      "('mus-2', 0) 0\n",
      "('mus-2', 1) 0\n",
      "('mus-2', 2) 0\n",
      "('mus-2', 3) 0\n",
      "('mus-2', 4) 24\n",
      "('mus-2', 5) 24\n",
      "('mus-2', 6) 24\n",
      "priode 0 number of uses 4\n",
      "priode 1 number of uses 8\n",
      "priode 2 number of uses 21\n",
      "priode 3 number of uses 50\n",
      "priode 4 number of uses 138\n",
      "priode 5 number of uses 138\n",
      "priode 6 number of uses 139\n"
     ]
    }
   ],
   "source": [
    "liste_number_year = list() # creating a list because matplotlib wants a tuple\n",
    "for key in sense_date_amount.keys():\n",
    "    #print(key)\n",
    "    liste_number_year.append(sense_date_amount[key])\n",
    "    \n",
    "tuple_number_year = tuple(liste_number_year)\n",
    "#print(tuple_number_year)\n",
    "\n",
    "period_number = dict()\n",
    "\n",
    "for key in sense_date_amount.keys():\n",
    "    compteur = 0\n",
    "    if key[1] in range(0,number_of_slices):\n",
    "        print(key,sense_date_amount[key[0],key[1]])\n",
    "        compteur += sense_date_amount[key[0],key[1]]\n",
    "        \n",
    "        try :\n",
    "            period_number[key[1]] += compteur\n",
    "        except KeyError:\n",
    "            period_number[key[1]] = 0\n",
    "            period_number[key[1]] += compteur\n",
    "            \n",
    "        \n",
    "for entry in period_number:\n",
    "    print(\"priode\",entry,\"number of uses\",period_number[entry])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
