{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a work-in-progress notebook\n",
    "\n",
    "We wish to know this:\n",
    "\n",
    "1. How well does the model identify the correct number of senses for the target word?\n",
    "2. **How well does the model identify the correct senses for the target word?**\n",
    "3. **How well does the model assign the right words to a given sense of the target word?**\n",
    "4. How well does the model assign the senses to the time intervals for the target word?\n",
    "\n",
    "The script will evaluate **Q2** and **Q3**. Q4 will follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic variables and imports:\n",
    "\n",
    "import codecs, csv, os, time, re, io\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "# directories\n",
    "dir_in = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), \"evaluation\", \"evaluation_input\"))\n",
    "dir_out = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), \"evaluation\", \"evaluation_output\"))\n",
    "\n",
    "\n",
    "s_senses = io.open(dir_in+\"/senses_69419.txt\",\"r\")\n",
    "k_senses = io.open(dir_in+\"/mus.dat\",\"r\")\n",
    "\n",
    "# DEBUG:\n",
    "#s_senses = io.open(dir_in+\"/senses_69419_debug.txt\",\"r\")\n",
    "#k_senses = io.open(dir_in+\"/mus_debug.dat\",\"r\")\n",
    "# k0 = mus4\n",
    "# k1 = mus3\n",
    "# k2 = mus2\n",
    "# k3 = mus1\n",
    "# k4 = nothing\n",
    "\n",
    "file_senses = s_senses.readlines()[1:]\n",
    "output_senses = k_senses.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='/Users/hengchen/git/seed-semantic-change/evaluation/evaluation_input/senses_69419.txt' mode='r' encoding='UTF-8'>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- ~~create the notebook~~\n",
    "- ~~organise the notebook~~\n",
    "- ~~write \"general idea\" pseudocode for the evaluation~~\n",
    "- ~~get input files~~\n",
    "- ~~figure out data structures to store the variables~~\n",
    "- ~~write actual code~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: How well does the model identify the correct senses for the target word?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-82fa54601c9a>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-82fa54601c9a>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    for each k:\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# For each target word, we have a list of senses  s (given by the expert)\n",
    "# For each target word, we have a list of senses k (given by the model)\n",
    "# This Q consists in matching s and k, and doing so in a confident way --> confidence score\n",
    "\n",
    "for each k:\n",
    "    for each s:\n",
    "        create conf(k,s)\n",
    "\n",
    "# What is conf(k,s)?\n",
    "        conf(k,s) = (p1*match(w1,s)+p2*match(w1,s)+px(wx,s))/10 WHERE\n",
    "    \n",
    "            px = probability of word wx \n",
    "                \n",
    "                and\n",
    "            \n",
    "            match(wx,s) =   1/number_of_senses_assigned_to_wx if s_is_one_of_them \n",
    "            \n",
    "                    or \n",
    "                            0 if w_is_not_associated_to_s\n",
    "                \n",
    "# Once we have gone through all s for one k, we have to choose the best k for s. How? (TBD, cfr Valerio and Barbara)\n",
    "\n",
    "# Once all ks have been assigned to all ss (or NA), we can calculate a general confidence score for the model.\n",
    "# One easy way to do that: \n",
    "\n",
    "conf_score_model = number_of_non_NA/k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real code\n",
    "\n",
    "Steps:\n",
    "\n",
    "- extract all senses from the file\n",
    "- use those senses as keys for a dictionary, `dict_of_words`\n",
    "- fill the dictionary: for each key, we store a list of words pertaining to that sense\n",
    "- transform the lists as sets so as to remove duplicates within the same sense\n",
    "- create a dictionary with a word as a key and its weight as a value, depending on how many senses it appears\n",
    "- parse the model output and get the probability weights for each word\n",
    "- do not take into account the first line\n",
    "- take care of empty lines\n",
    "\n",
    "Todo:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of senses: 4\n",
      "mus-1 {'66173', '7561', '93796', '101853', '69419', '28355', '116058', '113823', '7944', '31709', '83756', '102847', '34372', '16171', '72357', '3464', '95819', '80557', '51815', '36571', '68791', '84094', '73128', '95853', '26233', '84725', '15162', '43089', '27209', '56874', '14181', '7498', 'nlsj47984', '62204', '34366', '106974', '82959', '104690', '49506', '18166', '113711', '106502', 'nlsj10876', '23242', '30911', '19123', '62205', '76431', '50824', '85417', '29974', '89807', '49331', '70105', '4587', '19711', '95654', '19972', '26136', '24801', '26207', '72268', '23658', '45285', '34603', '80239', '112347', '73972', '11206', '42887', '91516', '72275', '110114', '65089', '52460', '37851', '61885', 'nlsj4012', '26499', '109403', '46176', '38966', '46966', '5009', '110484', '4335', '47917', '51647', '98234', '3237', '45917', '7182', '14050', '46216', '103957', '33770', '53161', '53956', '68174', '63212', '70477', '2767', '112351', '4906', '52095', '70482', '62303', '50073', 'nlsj5118', '116416', '29624', '58478', '51358', '79223', '42581', 'nlsj7783', '43305', '100693', 'nlsj1499', '110089', '86219', '39190', '61177', '15618', '69036', '1377', '62528', '83251', '26114', '71314', '104225', '72716', '39195', '1083', '85420', '25402', '26032', '41082', '15763', '61855', '37616', '61791', '76910', '31161', '67250', '92171', '51241', '77698', '91055', '19282', '39125', '106114', '94713', '72928', '46123', '45980', 'nlsj32160', '42659', '28002', '61056', '24523', '15893', '4068', '95258', '74735', '83665', '941', '41502', '55499', '23628', '63352', '22739', 'nlsj68228', 'nlsj52509', '3327', '94483', '19891', '18331', 'nlsj5437', '59588', '35092', '52035', '83928', '22100', '51259', '39313', '15571', '18706', '107959', '48479', '64914', '73277', '47447', '33671', '31562', '36390', '62816', 'nlsj105676', '55137', '28360', '26943', '112833', '101716', '72321', '65757', '54113', '67485', '80011', '84263', '67762', '83869', '49437', '86429', '71559', '35267', '65628', '74631', '67619', '86307', '104872', '55815', 'nlsj80462', '92162', '463', '4927', '61856', '112934', '71065', 'nlsj11198', '28569', '18334', '70958', '5131', '54399', '11197', 'nlsj69856', '20945', '110027', '48770', '103942', '56991', '63814', 'nlsj7583', '48498', '63845', '47665', '103221', '72287', '16051', '32686', '33494', '26197', '94097', '75306', '110284', '72610', '40156', '90329', '53695', '3128', '29962', '83434', '80761', 'nlsj86496', '69052', '41705', '92010', '83718', '85306', '31556', '45671', '2061', '110456', 'nlsj10130', '108536', '114972', '107905', '21487', '46074', '12035', '74602', '6684', '104538', '48704', '96089', '12176', '65565', '29828', '70550', '83266', 'nlsj5738', '33956', '15121', '23283', '7612', '34071', '55898', 'nlsj8671', '60402', '103975', '17197', '86352', '94316', '95074', '74819', '114706', 'nlsj1611', '103993', '70768', '17007', '54592', '79103', '23678', 'nlsj17074', '11305', '94098', '49886', '4845', '93536', '10056', '13579', '112816', '7529', '51300', '21783', '24261', '6449', 'nlsj75598', '61925', '104289', '84552', '2671', '51256', '51369', '82758', '38488', '67974', '52332', '86305', '3289', '105816', '65552', '24856', 'nlsj96345', '72627', '3800', '33074', '57321', '67104', '49955', '63713', '6384', '41357', '81343', '67868', '80327', '93449', '10933', '33647', '104655', '50616', '98723', '41918', '50644', '19641', '34476', '13427', '19268', '79595', '56406', '5521', '43124', '66639', '8586', '77699', 'nlsj96033', '104639', '69863', '102669', '114615', '92077', '98031', '84248', '90504', '12349', '85953', '105344', '38732', '28566', '73064', '23468', '55139', '93388', '96979', '22316', '115845', '85672', '48095', '2583', 'nlsj4113', '31491', 'nlsj28819', '6325', '49933', '8665', 'nlsj8970', '31948', '72273', '95523', '2340', '113741', '60225', '114847', '99638', '68641', '103922', '98312', '65983', '2873', '27415', 'nlsj5904', '59124', '7062', '23690', '8909', '4493', '59708', '11072', '109687', '13317', '96565', '1984', '63827', '20728', '91085', '6174', '59005', '40161', '102474', '61265', '260', '28036', '22882', '11058', '101445', '116244', '103012', '50679', '7779', '69469', '15858', '105550', '65097', '45996', '74523', 'nlsj106628', '1692', '46195', '110639', 'nlsj43224', '2834', '37711', '64757', '12973', '98241', 'nlsj71743', '99622', '67815', '25007', '47735', '3241', '95432', '112559', '21335', '22209', 'nlsj77405', '32657', '80107', '64316', '40001', '116059', '74571', '114548', 'nlsj4320', 'nlsj9035', '22997', '51819', '48341', '42890', '40545', '65697', '108537', '114816', 'nlsj57918', '75948', '103637', '47513', '114842', '20179', '91800', '95368', '21812', '46646', '108882', '114731', '39847', '71118', '112472', '80043', 'nlsj114757', '19788', '97386', '22502', '17962', '103152', '88716', 'nlsj60710', '116050', '108780', '18788', '1564', '112467', '102869', 'nlsj9526', '26447', '75552', '24226', '76157', '21431', '29883', '79261', '85807', '91351', '74126', '105176', '73707', '2749', '105070', '78746', '33241', '95221', '111895', '36165', 'nlsj801', '33160', '37274', '19346', 'nlsj2610', '41536', '103871', '7832', '4548', '2219', '83253', '61040', '100774', '63314', '4778', '12409', '95522', '66746', '53442', '88498', '45656', 'nlsj61205', '12641', '55498', '57057', '7177', '38547', '9757', '7133', '50252', '110302', '13098', '115810', '28592', '43206', 'nlsj6617', '41481', '55532', '64448', '106566', '17381', '3398', '55997', 'nlsj4579', '100906', '2845', '11229', '31236', '20674', '4274', '71673', '62383', '75910', '44888', '8505', '80555', '16400', '46474', '51727', '70708', '54812', 'nlsj80958', '25228', '89309', '83760', '98173', '20195', 'nlsj7856', '60308', '64586', '84234', '41538', '13002', '42830', 'nlsj98815', '12948', '5132', '48670', '53942', '109730', '93417', '88464', '17730', '102989', '37776', '45513', '27764', '16430', '13039', '35570', '92406', '104421', '51849', '80357', '4670', '24292', '59501', '94474', '75352', '112169', '116470', '11970', '104355', '51373', '31964', '102381', '55149', '114587', '2731', '35708', '116293', '69252', '76703', '76335', '15380', '113560', '27629', 'nlsj2729', '102000', '92662', '84434', '38743', '52571', '114688', '57262', '66294', '12485', '84534', '41032', '49875', '113060', 'nlsj8979', '68970', '83113', '82665', '51661', '62093', '26048', '83783', '5112', '25870', '76184', '7712', '3986', '111207', '64956', '19546', '84494', 'nlsj40053', '4536', '18271', '42686', '103972', '21589', '51376', '18128', 'nlsj106503', '112070', '36790', '11583', '40323', '54946', '115193', '84475', '37726', '83310', '16609', '41633', '83774', '47617', '91944', '4378', '19452', '74127', '7724', '100965', '2224', '19536', '66777', '71262', '42842', '106064', '75263', '112720', '34855', '436', '84422', 'nlsj4784', '113556', '56810', '7159', '76564', '79947', '69714', '111416', '25018', '80664', '27005', '37488', '24444', '65975', '89366', '34848', '107167', 'nlsj48763', '30700', '22036', '113251', '61291', 'nlsj68688', '76530', '18937', '66494', '43977', 'nlsj12527', '76765', '37244', '110598', '113379', '48867', 'nlsj105619', '14362', '24003', '97147', '31565', '403', '66262', 'nlsj114103', '23794', '110655', '93022', 'nlsj78558', 'nlsj11066', '42827', '104860', '13608', '61176', '50451', '43060', '56003', '47964', '82954', '26684', '21802', '112769', 'nlsj33192', '71308', '73221', '54971', '80042', '63143', '35710', '110606', '77928', '84150', '21920', '47876', '37095', '20621', '75954', '93812', '78716', '104311', 'nlsj32167', '82756', '47745', 'nlsj10580', '41619', '97285', '49589', '48291', '75316', '2764', '50093', '15047', '49227', '46804', '100524', 'nlsj40132', '79697', '103701', '21901', '112943', '72202', '59276', '678', '31996', '15378', '5607', 'nlsj59923', '16052', 'nlsj100912', '21830'}\n",
      "\n",
      "\n",
      "\n",
      "mus-4 {'24289', '3364', '100964', '69419', '8909', '4493', '39016', '85099', '96560', '60646', '35166', '6174', '3325', '33969', '68996', '22660', '82451', '74378', '11058', '112307', '114339', '47442', '85665', '37262', '50236', '45996', '14269', '13852', '99647', '62204', '110639', '34366', '55497', '104690', '22425', '106502', '7793', 'nlsj71743', '67009', '106676', '75989', '1199', '108652', '47735', '36216', '65703', '30317', '112559', '22209', '32657', '112367', '44660', '114553', '19972', '64316', '26002', '83825', '88855', '57457', '61128', '19528', '105458', '110114', 'nlsj1147', '59516', '64703', '55981', '97963', '76516', '57474', '112794', '101034', '83834', '26034', '115212', '46646', '77640', '75595', '84034', '14266', 'nlsj86473', '98234', '108780', '37961', '33770', '36938', '33238', '75552', '29883', '20885', '97605', '19503', '4474', '116416', '51358', '38472', '61946', '58429', '92926', '76513', '69532', '62528', '115374', '85876', '43616', '83732', '114653', '70556', '98584', '57460', '114083', '621', '71984', '51241', '45153', '4470', '39299', '2898', '13376', '15893', '83174', '66342', 'nlsj177', '34982', '74735', '58972', '79069', '116218', '83665', '13985', '113477', '65966', '73672', '71373', '74621', '13813', '70495', '83605', '98417', '39238', '109918', '96981', '26046', '96698', '27092', 'nlsj99416', '33486', '18062', '42214', '58142', '10223', '22210', '96999', '66389', '84234', '82190', '115102', '90347', '93332', '31562', '93435', '28349', '89405', '36289', '97252', '24340', '13039', '102676', '35570', '86108', '101714', '93379', '74463', '60703', '13942', '67762', '104355', '83869', '63772', '13715', '53015', '31609', '3630', '50908', '102795', '94677', '70558', '113560', '95961', '102000', '66755', '71065', '28569', '52571', '105548', '18334', '109729', '114387', '37405', '2392', '84534', '13315', '90334', '75005', '49629', '56991', '15786', '74752', '37720', '18790', '34522', '20830', '103085', '16051', 'nlsj33345', '61231', '63550', '72610', '111722', '111764', '104624', '94365', '35417', '11583', '71585', '37726', '7608', '85306', '51785', '41271', '31607', '45245', '90166', '6898', '98869', '70521', '33426', '26226', '100495', '91583', '48704', '112703', '24946', '106206', '112720', '55313', '37075', '91634', '57466', '87413', '15011', '35244', '67555', '43975', '39180', '101851', 'nlsj82141', '66750', '106222', '106263', '61245', '66494', '4845', '76765', '66295', '97606', '71404', '36180', '67507', '48504', '97241', '39078', '70364', 'nlsj61122', '116408', '51369', '95970', '65191', '114753', 'nlsj42558', 'nlsj79694', '65552', '67526', '116216', '42071', '24132', '48436', '20836', 'nlsj82490', 'nlsj40134', '61176', '10933', 'nlsj35116', '55391', '58319', 'nlsj112819', '21651', '61553', '96503', '6616', '55383', '67352', '93341', '60840', '20621', '93812', '104639', '56189', '24184', '114615', '66761', '82800', '109733', '41570', '115748', '76028', '63928', '48291', '90504', '115887', '83650', '83468', '85953', '93388', '63873', '100379', '96853', '15609', '72202', '6339', '50521', '84421', '28588', '39543', '26039', '16804', '68641'}\n",
      "\n",
      "\n",
      "\n",
      "mus-2 {'103667', '58527', '83458', '63032', '69419', '68706', '4670', 'nlsj114797', '7362', 'nlsj8425', '45525', '29134', '64559', '110127', '75653', '67762', '95540', '102474', '90303', '74378', '19236', '76335', '214', '113881', 'nlsj3191', 'nlsj2729', '89321', '45996', '105557', '97266', '112724', '110639', '52571', '112512', '49182', '57262', '49444', '62535', 'nlsj61217', '18762', '49875', '21490', '47735', '106023', '45526', 'nlsj10202', '106267', 'nlsj5634', '57356', '56223', '94291', '62337', 'nlsj100389', '58262', '68185', '69220', '93624', '103038', '60113', '4763', '104624', '95095', '103648', 'nlsj3887', '68539', '19623', '45182', 'nlsj4805', '64983', '100673', 'nlsj3072', '49439', '82623', '38744', '83212', '115638', '42678', 'nlsj61519', '17228', '103637', '114347', '90350', '62274', '114437', '21205', 'nlsj112110', 'nlsj36416', '106206', '45757', '98114', '65565', 'nlsj114757', '105691', '60317', '22389', '51737', 'nlsj76740', '116273', '34243', '58133', '75992', '74225', '4698', '1904', '98955', '112811', '21168', '29390', '93880', '15198', '20508', '91541', '75539', '19789', '70768', '20413', '59121', '104416', '74126', '33822', 'nlsj7772', '102930', '53222', '66494', '61622', '16884', '84089', '30569', '65544', '92475', '94164', '58429', '60878', '80525', 'nlsj10103', '26818', '37343', '64642', '80188', '82758', 'nlsj75531', '5591', '46452', '65552', '62258', '67422', '74610', '69751', '2586', '30564', '22166', '15763', '103773', '76910', '16440', '35633', '56954', '112733', 'nlsj6085', '101257', '59626', '41918', '67364', '64483', '75652', '19641', '20469', 'nlsj33192', 'nlsj4249', '16986', '53286', '60316', '54990', '93842', 'nlsj70384', '45980', '26328', '104637', '21920', '68144', '6184', '94967', '66342', '101982', '91198', '74165', '67360', '53538', 'nlsj68042', '20674', '92051', '65934', '73672', '47745', 'nlsj32819', '61185', '86213', 'nlsj3264', '95144', 'nlsj29538', '18561', '74675', '92553', 'nlsj86871', '58271', '93840', '93388', '4562', '114381', '20270', '60425', '22316', '23861', 'nlsj12526', '85672', '49176', 'nlsj99416', '101790', '113163', '53254', '32263', '83209', '33457', '65983', '110060'}\n",
      "\n",
      "\n",
      "\n",
      "w {'31562', '115213', '65747', 'nlsj5856', '114624', '96106', '109757', '104374', '69419', '24340', '11072', '90410', '38714', '1984', '104421', '116416', 'nlsj4603', '58444', '35581', '79223', '16132', '63772', '260', '24436', '92927', '61073', '11058', '98712', '1093', '64985', '4548', '2219', '1377', '46178', '76801', '43260', '12625', '48549', '45996', '114753', '30790', '28569', '62204', '46195', '1083', '86305', '65552', '70958', '104429', '293', '53826', '113060', '35541', '39186', '1379', '41357', 'nlsj56179', '63845', '78297', '104655', '65295', '28592', '4958', '34522', '19282', '32657', '27847', '72287', '71308', '71976', '98541', '23658', 'nlsj32160', '3398', '31333', '21920', '71312', '85306', '67132', '12210', '23799', '96193', '5390', '31236', '61708', '98747', '1989', 'nlsj7011', '61016', '16400', '49589', '33631', '46804', '31537', '71492', '73064', '35642', '23903', '103701', '84410', '56607', '54356', '8665', '26886', '114558', '112350', '42214', '86112', 'nlsj7765', '12252', 'nlsj5105', '26447', '75552', '44552'}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expert_senses = list() # list where we store all sense ids provided by expert\n",
    "for s in file_senses: # 60 for testing purposes\n",
    "    s = s.split(\"\\t\")\n",
    "    sense = s[11] # The sense ID is after the 10th tab\n",
    "    expert_senses.append(sense)\n",
    "    \n",
    "#print(len(expert_senses),expert_senses,len(set(expert_senses)))\n",
    "\n",
    "expert_senses = list(set(expert_senses)) # we only keep the unique senses\n",
    "number_of_s = len(expert_senses)  # we create a variable that stores the number of unique senses\n",
    "print(\"Number of senses:\",number_of_s)\n",
    "\n",
    "# This dictionary has a sense as a key, and a list of words as a value. \n",
    "dict_of_words = dict()\n",
    "# This list stores all words\n",
    "list_of_all_words = list()\n",
    "# This dictionary stores all words as keys and their weight as value\n",
    "word_weight = dict()\n",
    "\n",
    "for i in range(0,number_of_s): # for each sense, we create a dictionary entry which has a list as value\n",
    "    dict_of_words[expert_senses[i]] = list()\n",
    "\n",
    "    for s in file_senses: # we go back in the file\n",
    "        s = s.split(\"\\t\") # splitting on tabs\n",
    "        \n",
    "        sentence_of_ids = s[8] # 8 is for IDs, 9 is for words\n",
    "        list_of_ids = sentence_of_ids.split(\" \")  # splitting on spaces\n",
    "        for word_id in list_of_ids:\n",
    "            if s[11] == expert_senses[i]:      # we store all words for one sense \n",
    "                dict_of_words[expert_senses[i]].append(word_id)\n",
    "            list_of_all_words.append(word_id) # we store all words, we'll iterate over that for scores\n",
    "        \n",
    "\n",
    "    # Here, we remove duplicates\n",
    "    #dict_of_words[expert_senses[i]].append(\"79223\") #testing\n",
    "    dict_of_words[expert_senses[i]] = set(dict_of_words[expert_senses[i]]) \n",
    "      \n",
    "    print(expert_senses[i],set(dict_of_words[expert_senses[i]]))\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every word in the list of words that we have\n",
    "# we count the number of senses it appears in\n",
    "# we use that number to divide its importance: 1 sense = 1 importance; 2 senses = 0.5 importance\n",
    "# this can be finetuned\n",
    "\n",
    "for word in list_of_all_words:\n",
    "    x = 0\n",
    "    for i in range(0,number_of_s):\n",
    "        if word in dict_of_words[expert_senses[i]]:\n",
    "            x += 1 \n",
    "        if x != 0:\n",
    "            word_weight[word] = 1/x\n",
    "        \n",
    "    \n",
    "    #print(word,word_weight[word])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parsing output.dat\n",
    "- split on \"===============  per time  ===============\" and keep first part\n",
    "- transform that into a list, then\n",
    "- get lines that start with \"p(w|s)\"\n",
    "- count those, k = that number\n",
    "- split the line on \":\", keep the second part\n",
    "- split the rest on \";\", it's [ID] = prob_from_this_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word id 28355 ; probability 0.088\n",
      "word id 69419 ; probability 0.069\n",
      "word id 57460 ; probability 0.056\n",
      "word id 114587 ; probability 0.042\n",
      "word id 42071 ; probability 0.041\n",
      "word id 35267 ; probability 0.035\n",
      "word id 51647 ; probability 0.035\n",
      "word id 64448 ; probability 0.023\n",
      "word id 45980 ; probability 0.018\n",
      "word id 53826 ; probability 0.017\n",
      "{'28355': 0.2075471698113207}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377, '57460': 0.1320754716981132}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377, '57460': 0.1320754716981132, '114587': 0.0990566037735849}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377, '57460': 0.1320754716981132, '114587': 0.0990566037735849, '42071': 0.09669811320754716}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377, '57460': 0.1320754716981132, '114587': 0.0990566037735849, '42071': 0.09669811320754716, '35267': 0.08254716981132075}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377, '57460': 0.1320754716981132, '114587': 0.0990566037735849, '42071': 0.09669811320754716, '35267': 0.08254716981132075, '51647': 0.08254716981132075}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377, '57460': 0.1320754716981132, '114587': 0.0990566037735849, '42071': 0.09669811320754716, '35267': 0.08254716981132075, '51647': 0.08254716981132075, '64448': 0.05424528301886792}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377, '57460': 0.1320754716981132, '114587': 0.0990566037735849, '42071': 0.09669811320754716, '35267': 0.08254716981132075, '51647': 0.08254716981132075, '64448': 0.05424528301886792, '45980': 0.042452830188679236}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377, '57460': 0.1320754716981132, '114587': 0.0990566037735849, '42071': 0.09669811320754716, '35267': 0.08254716981132075, '51647': 0.08254716981132075, '64448': 0.05424528301886792, '45980': 0.042452830188679236, '53826': 0.04009433962264151}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377, '57460': 0.1320754716981132, '114587': 0.0990566037735849, '42071': 0.09669811320754716, '35267': 0.08254716981132075, '51647': 0.08254716981132075, '64448': 0.05424528301886792, '45980': 0.042452830188679236, '53826': 0.04009433962264151}\n",
      "word id 79223 ; probability 0.063\n",
      "word id 92927 ; probability 0.057\n",
      "word id 46574 ; probability 0.038\n",
      "word id 67660 ; probability 0.038\n",
      "word id 103085 ; probability 0.036\n",
      "word id 86112 ; probability 0.030\n",
      "word id 101982 ; probability 0.029\n",
      "word id 75808 ; probability 0.026\n",
      "word id 68539 ; probability 0.024\n",
      "word id 54607 ; probability 0.024\n",
      "{'79223': 0.17260273972602735}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438, '46574': 0.10410958904109587}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438, '46574': 0.10410958904109587, '67660': 0.10410958904109587}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438, '46574': 0.10410958904109587, '67660': 0.10410958904109587, '103085': 0.09863013698630134}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438, '46574': 0.10410958904109587, '67660': 0.10410958904109587, '103085': 0.09863013698630134, '86112': 0.08219178082191778}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438, '46574': 0.10410958904109587, '67660': 0.10410958904109587, '103085': 0.09863013698630134, '86112': 0.08219178082191778, '101982': 0.07945205479452053}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438, '46574': 0.10410958904109587, '67660': 0.10410958904109587, '103085': 0.09863013698630134, '86112': 0.08219178082191778, '101982': 0.07945205479452053, '75808': 0.07123287671232874}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438, '46574': 0.10410958904109587, '67660': 0.10410958904109587, '103085': 0.09863013698630134, '86112': 0.08219178082191778, '101982': 0.07945205479452053, '75808': 0.07123287671232874, '68539': 0.06575342465753423}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438, '46574': 0.10410958904109587, '67660': 0.10410958904109587, '103085': 0.09863013698630134, '86112': 0.08219178082191778, '101982': 0.07945205479452053, '75808': 0.07123287671232874, '68539': 0.06575342465753423, '54607': 0.06575342465753423}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438, '46574': 0.10410958904109587, '67660': 0.10410958904109587, '103085': 0.09863013698630134, '86112': 0.08219178082191778, '101982': 0.07945205479452053, '75808': 0.07123287671232874, '68539': 0.06575342465753423, '54607': 0.06575342465753423}\n",
      "word id 62258 ; probability 0.080\n",
      "word id 64586 ; probability 0.052\n",
      "word id 58271 ; probability 0.048\n",
      "word id nlsj86871 ; probability 0.041\n",
      "word id 101851 ; probability 0.039\n",
      "word id nlsj183 ; probability 0.037\n",
      "word id 75653 ; probability 0.031\n",
      "word id nlsj59923 ; probability 0.030\n",
      "word id 70105 ; probability 0.026\n",
      "word id 82758 ; probability 0.024\n",
      "{'62258': 0.19607843137254902}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685, '58271': 0.11764705882352941}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685, '58271': 0.11764705882352941, 'nlsj86871': 0.10049019607843138}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685, '58271': 0.11764705882352941, 'nlsj86871': 0.10049019607843138, '101851': 0.09558823529411764}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685, '58271': 0.11764705882352941, 'nlsj86871': 0.10049019607843138, '101851': 0.09558823529411764, 'nlsj183': 0.0906862745098039}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685, '58271': 0.11764705882352941, 'nlsj86871': 0.10049019607843138, '101851': 0.09558823529411764, 'nlsj183': 0.0906862745098039, '75653': 0.07598039215686274}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685, '58271': 0.11764705882352941, 'nlsj86871': 0.10049019607843138, '101851': 0.09558823529411764, 'nlsj183': 0.0906862745098039, '75653': 0.07598039215686274, 'nlsj59923': 0.07352941176470587}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685, '58271': 0.11764705882352941, 'nlsj86871': 0.10049019607843138, '101851': 0.09558823529411764, 'nlsj183': 0.0906862745098039, '75653': 0.07598039215686274, 'nlsj59923': 0.07352941176470587, '70105': 0.06372549019607843}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685, '58271': 0.11764705882352941, 'nlsj86871': 0.10049019607843138, '101851': 0.09558823529411764, 'nlsj183': 0.0906862745098039, '75653': 0.07598039215686274, 'nlsj59923': 0.07352941176470587, '70105': 0.06372549019607843, '82758': 0.058823529411764705}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685, '58271': 0.11764705882352941, 'nlsj86871': 0.10049019607843138, '101851': 0.09558823529411764, 'nlsj183': 0.0906862745098039, '75653': 0.07598039215686274, 'nlsj59923': 0.07352941176470587, '70105': 0.06372549019607843, '82758': 0.058823529411764705}\n",
      "word id 102000 ; probability 0.035\n",
      "word id 70495 ; probability 0.035\n",
      "word id 7182 ; probability 0.029\n",
      "word id 70958 ; probability 0.024\n",
      "word id 49589 ; probability 0.021\n",
      "word id 83174 ; probability 0.021\n",
      "word id 106676 ; probability 0.019\n",
      "word id 22209 ; probability 0.018\n",
      "word id 93388 ; probability 0.017\n",
      "word id 16132 ; probability 0.016\n",
      "{'102000': 0.14893617021276598}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598, '7182': 0.12340425531914895}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598, '7182': 0.12340425531914895, '70958': 0.1021276595744681}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598, '7182': 0.12340425531914895, '70958': 0.1021276595744681, '49589': 0.08936170212765958}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598, '7182': 0.12340425531914895, '70958': 0.1021276595744681, '49589': 0.08936170212765958, '83174': 0.08936170212765958}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598, '7182': 0.12340425531914895, '70958': 0.1021276595744681, '49589': 0.08936170212765958, '83174': 0.08936170212765958, '106676': 0.08085106382978724}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598, '7182': 0.12340425531914895, '70958': 0.1021276595744681, '49589': 0.08936170212765958, '83174': 0.08936170212765958, '106676': 0.08085106382978724, '22209': 0.07659574468085106}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598, '7182': 0.12340425531914895, '70958': 0.1021276595744681, '49589': 0.08936170212765958, '83174': 0.08936170212765958, '106676': 0.08085106382978724, '22209': 0.07659574468085106, '93388': 0.0723404255319149}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598, '7182': 0.12340425531914895, '70958': 0.1021276595744681, '49589': 0.08936170212765958, '83174': 0.08936170212765958, '106676': 0.08085106382978724, '22209': 0.07659574468085106, '93388': 0.0723404255319149, '16132': 0.06808510638297872}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598, '7182': 0.12340425531914895, '70958': 0.1021276595744681, '49589': 0.08936170212765958, '83174': 0.08936170212765958, '106676': 0.08085106382978724, '22209': 0.07659574468085106, '93388': 0.0723404255319149, '16132': 0.06808510638297872}\n",
      "word id nlsj5634 ; probability 0.039\n",
      "word id 61925 ; probability 0.037\n",
      "word id 12620 ; probability 0.030\n",
      "word id 69419 ; probability 0.027\n",
      "word id 104421 ; probability 0.027\n",
      "word id 91085 ; probability 0.024\n",
      "word id 71308 ; probability 0.022\n",
      "word id 115748 ; probability 0.021\n",
      "word id 19641 ; probability 0.020\n",
      "word id 5390 ; probability 0.019\n",
      "{'nlsj5634': 0.14661654135338348}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228, '12620': 0.11278195488721805}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228, '12620': 0.11278195488721805, '69419': 0.10150375939849625}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228, '12620': 0.11278195488721805, '69419': 0.10150375939849625, '104421': 0.10150375939849625}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228, '12620': 0.11278195488721805, '69419': 0.10150375939849625, '104421': 0.10150375939849625, '91085': 0.09022556390977446}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228, '12620': 0.11278195488721805, '69419': 0.10150375939849625, '104421': 0.10150375939849625, '91085': 0.09022556390977446, '71308': 0.08270676691729324}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228, '12620': 0.11278195488721805, '69419': 0.10150375939849625, '104421': 0.10150375939849625, '91085': 0.09022556390977446, '71308': 0.08270676691729324, '115748': 0.07894736842105265}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228, '12620': 0.11278195488721805, '69419': 0.10150375939849625, '104421': 0.10150375939849625, '91085': 0.09022556390977446, '71308': 0.08270676691729324, '115748': 0.07894736842105265, '19641': 0.07518796992481204}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228, '12620': 0.11278195488721805, '69419': 0.10150375939849625, '104421': 0.10150375939849625, '91085': 0.09022556390977446, '71308': 0.08270676691729324, '115748': 0.07894736842105265, '19641': 0.07518796992481204, '5390': 0.07142857142857144}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228, '12620': 0.11278195488721805, '69419': 0.10150375939849625, '104421': 0.10150375939849625, '91085': 0.09022556390977446, '71308': 0.08270676691729324, '115748': 0.07894736842105265, '19641': 0.07518796992481204, '5390': 0.07142857142857144}\n"
     ]
    }
   ],
   "source": [
    "lines_output = output_senses.split(\"===============  per time  ===============\")[0].split(\"\\n\")\n",
    "\n",
    "number_of_the_k = 0\n",
    "\n",
    "k_words_with_prob = dict()\n",
    "\n",
    "for line in lines_output:\n",
    "    if line[:6] == \"p(w|s)\":\n",
    "        line = line.split(\":\")[1]\n",
    "        line = line.split(\";\")\n",
    "        #print(number_of_the_k,line)\n",
    "        dico_word_prob = dict()\n",
    "        temp_dict = dict()\n",
    "        k_words_with_prob[number_of_the_k] = list()\n",
    "        \n",
    "        line = line[:-1] # last item of the list is empty\n",
    "        \n",
    "        total_probability = 0 # to have relative probs\n",
    "        for word_prob in line:\n",
    "            \n",
    "\n",
    "        \n",
    "            #word_prob = word_prob.split(\",\")\n",
    "            #for word in word_prob:\n",
    "            probability = re.findall(\"([\\d.\\w]*)\",word_prob)\n",
    "            if probability:\n",
    "                probability = list(filter(None,probability))\n",
    "                    \n",
    "            total_probability += float(probability[1])\n",
    "            print(\"word id\",probability[0],\"; probability\",probability[1])\n",
    "        \n",
    "            dico_word_prob[probability[0]] = float(probability[1])\n",
    "        #print(type(k_words_with_prob[number_of_the_k]))\n",
    "        \n",
    "        for i in dico_word_prob.keys():\n",
    "            \n",
    "            temp_dict[i] = float(dico_word_prob[i]/total_probability)\n",
    "            k_words_with_prob[number_of_the_k] = temp_dict\n",
    "            \n",
    "            print(k_words_with_prob[number_of_the_k])\n",
    "            \n",
    "        #k_words_with_prob[number_of_the_k] = [float(dico_word_prob[i]/total_probability) for i in dico_word_prob]\n",
    "        #print(k_words_with_prob[number_of_the_k])\n",
    "        print(temp_dict)\n",
    "        number_of_the_k += 1\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k_words_with_prob\n",
    "This dictionary has the sense number 'k' as keys and the a dictionary of [word] = probability as values.\n",
    "Example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "#print(\"Probability for word ID 5390 in sense k = 4:\",k_words_with_prob[4][\"5390\"])\n",
    "print(type(k_words_with_prob[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output sense 0\n",
      "\texpert sense number  0 mus-1\n",
      "\t\tword from annotation for sense 0 : 28355\n",
      "\t\t\tword  28355 is in output for sense 0 with probability: 0.2075471698113207 and weight: 1.0\n",
      "\t\tword from annotation for sense 0 : 69419\n",
      "\t\t\tword  69419 is in output for sense 0 with probability: 0.16273584905660377 and weight: 0.25\n",
      "\t\tword from annotation for sense 0 : 57460\n",
      "\t\tword from annotation for sense 0 : 114587\n",
      "\t\t\tword  114587 is in output for sense 0 with probability: 0.0990566037735849 and weight: 1.0\n",
      "\t\tword from annotation for sense 0 : 42071\n",
      "\t\tword from annotation for sense 0 : 35267\n",
      "\t\t\tword  35267 is in output for sense 0 with probability: 0.08254716981132075 and weight: 1.0\n",
      "\t\tword from annotation for sense 0 : 51647\n",
      "\t\t\tword  51647 is in output for sense 0 with probability: 0.08254716981132075 and weight: 1.0\n",
      "\t\tword from annotation for sense 0 : 64448\n",
      "\t\t\tword  64448 is in output for sense 0 with probability: 0.05424528301886792 and weight: 1.0\n",
      "\t\tword from annotation for sense 0 : 45980\n",
      "\t\t\tword  45980 is in output for sense 0 with probability: 0.042452830188679236 and weight: 0.5\n",
      "\t\tword from annotation for sense 0 : 53826\n",
      "\texpert sense number  1 mus-4\n",
      "\t\tword from annotation for sense 0 : 28355\n",
      "\t\tword from annotation for sense 0 : 69419\n",
      "\t\t\tword  69419 is in output for sense 0 with probability: 0.16273584905660377 and weight: 0.25\n",
      "\t\tword from annotation for sense 0 : 57460\n",
      "\t\t\tword  57460 is in output for sense 0 with probability: 0.1320754716981132 and weight: 1.0\n",
      "\t\tword from annotation for sense 0 : 114587\n",
      "\t\tword from annotation for sense 0 : 42071\n",
      "\t\t\tword  42071 is in output for sense 0 with probability: 0.09669811320754716 and weight: 1.0\n",
      "\t\tword from annotation for sense 0 : 35267\n",
      "\t\tword from annotation for sense 0 : 51647\n",
      "\t\tword from annotation for sense 0 : 64448\n",
      "\t\tword from annotation for sense 0 : 45980\n",
      "\t\tword from annotation for sense 0 : 53826\n",
      "\texpert sense number  2 mus-2\n",
      "\t\tword from annotation for sense 0 : 28355\n",
      "\t\tword from annotation for sense 0 : 69419\n",
      "\t\t\tword  69419 is in output for sense 0 with probability: 0.16273584905660377 and weight: 0.25\n",
      "\t\tword from annotation for sense 0 : 57460\n",
      "\t\tword from annotation for sense 0 : 114587\n",
      "\t\tword from annotation for sense 0 : 42071\n",
      "\t\tword from annotation for sense 0 : 35267\n",
      "\t\tword from annotation for sense 0 : 51647\n",
      "\t\tword from annotation for sense 0 : 64448\n",
      "\t\tword from annotation for sense 0 : 45980\n",
      "\t\t\tword  45980 is in output for sense 0 with probability: 0.042452830188679236 and weight: 0.5\n",
      "\t\tword from annotation for sense 0 : 53826\n",
      "\texpert sense number  3 w\n",
      "\t\tword from annotation for sense 0 : 28355\n",
      "\t\tword from annotation for sense 0 : 69419\n",
      "\t\t\tword  69419 is in output for sense 0 with probability: 0.16273584905660377 and weight: 0.25\n",
      "\t\tword from annotation for sense 0 : 57460\n",
      "\t\tword from annotation for sense 0 : 114587\n",
      "\t\tword from annotation for sense 0 : 42071\n",
      "\t\tword from annotation for sense 0 : 35267\n",
      "\t\tword from annotation for sense 0 : 51647\n",
      "\t\tword from annotation for sense 0 : 64448\n",
      "\t\tword from annotation for sense 0 : 45980\n",
      "\t\tword from annotation for sense 0 : 53826\n",
      "\t\t\tword  53826 is in output for sense 0 with probability: 0.04009433962264151 and weight: 1.0\n",
      "output sense 1\n",
      "\texpert sense number  0 mus-1\n",
      "\t\tword from annotation for sense 1 : 79223\n",
      "\t\t\tword  79223 is in output for sense 1 with probability: 0.17260273972602735 and weight: 0.5\n",
      "\t\tword from annotation for sense 1 : 92927\n",
      "\t\tword from annotation for sense 1 : 46574\n",
      "\t\tword from annotation for sense 1 : 67660\n",
      "\t\tword from annotation for sense 1 : 103085\n",
      "\t\tword from annotation for sense 1 : 86112\n",
      "\t\tword from annotation for sense 1 : 101982\n",
      "\t\tword from annotation for sense 1 : 75808\n",
      "\t\tword from annotation for sense 1 : 68539\n",
      "\t\tword from annotation for sense 1 : 54607\n",
      "\texpert sense number  1 mus-4\n",
      "\t\tword from annotation for sense 1 : 79223\n",
      "\t\tword from annotation for sense 1 : 92927\n",
      "\t\tword from annotation for sense 1 : 46574\n",
      "\t\tword from annotation for sense 1 : 67660\n",
      "\t\tword from annotation for sense 1 : 103085\n",
      "\t\t\tword  103085 is in output for sense 1 with probability: 0.09863013698630134 and weight: 1.0\n",
      "\t\tword from annotation for sense 1 : 86112\n",
      "\t\tword from annotation for sense 1 : 101982\n",
      "\t\tword from annotation for sense 1 : 75808\n",
      "\t\tword from annotation for sense 1 : 68539\n",
      "\t\tword from annotation for sense 1 : 54607\n",
      "\texpert sense number  2 mus-2\n",
      "\t\tword from annotation for sense 1 : 79223\n",
      "\t\tword from annotation for sense 1 : 92927\n",
      "\t\tword from annotation for sense 1 : 46574\n",
      "\t\tword from annotation for sense 1 : 67660\n",
      "\t\tword from annotation for sense 1 : 103085\n",
      "\t\tword from annotation for sense 1 : 86112\n",
      "\t\tword from annotation for sense 1 : 101982\n",
      "\t\t\tword  101982 is in output for sense 1 with probability: 0.07945205479452053 and weight: 1.0\n",
      "\t\tword from annotation for sense 1 : 75808\n",
      "\t\tword from annotation for sense 1 : 68539\n",
      "\t\t\tword  68539 is in output for sense 1 with probability: 0.06575342465753423 and weight: 1.0\n",
      "\t\tword from annotation for sense 1 : 54607\n",
      "\texpert sense number  3 w\n",
      "\t\tword from annotation for sense 1 : 79223\n",
      "\t\t\tword  79223 is in output for sense 1 with probability: 0.17260273972602735 and weight: 0.5\n",
      "\t\tword from annotation for sense 1 : 92927\n",
      "\t\t\tword  92927 is in output for sense 1 with probability: 0.1561643835616438 and weight: 1.0\n",
      "\t\tword from annotation for sense 1 : 46574\n",
      "\t\tword from annotation for sense 1 : 67660\n",
      "\t\tword from annotation for sense 1 : 103085\n",
      "\t\tword from annotation for sense 1 : 86112\n",
      "\t\t\tword  86112 is in output for sense 1 with probability: 0.08219178082191778 and weight: 1.0\n",
      "\t\tword from annotation for sense 1 : 101982\n",
      "\t\tword from annotation for sense 1 : 75808\n",
      "\t\tword from annotation for sense 1 : 68539\n",
      "\t\tword from annotation for sense 1 : 54607\n",
      "output sense 2\n",
      "\texpert sense number  0 mus-1\n",
      "\t\tword from annotation for sense 2 : 62258\n",
      "\t\tword from annotation for sense 2 : 64586\n",
      "\t\t\tword  64586 is in output for sense 2 with probability: 0.12745098039215685 and weight: 1.0\n",
      "\t\tword from annotation for sense 2 : 58271\n",
      "\t\tword from annotation for sense 2 : nlsj86871\n",
      "\t\tword from annotation for sense 2 : 101851\n",
      "\t\tword from annotation for sense 2 : nlsj183\n",
      "\t\tword from annotation for sense 2 : 75653\n",
      "\t\tword from annotation for sense 2 : nlsj59923\n",
      "\t\t\tword  nlsj59923 is in output for sense 2 with probability: 0.07352941176470587 and weight: 1.0\n",
      "\t\tword from annotation for sense 2 : 70105\n",
      "\t\t\tword  70105 is in output for sense 2 with probability: 0.06372549019607843 and weight: 1.0\n",
      "\t\tword from annotation for sense 2 : 82758\n",
      "\t\t\tword  82758 is in output for sense 2 with probability: 0.058823529411764705 and weight: 0.5\n",
      "\texpert sense number  1 mus-4\n",
      "\t\tword from annotation for sense 2 : 62258\n",
      "\t\tword from annotation for sense 2 : 64586\n",
      "\t\tword from annotation for sense 2 : 58271\n",
      "\t\tword from annotation for sense 2 : nlsj86871\n",
      "\t\tword from annotation for sense 2 : 101851\n",
      "\t\t\tword  101851 is in output for sense 2 with probability: 0.09558823529411764 and weight: 1.0\n",
      "\t\tword from annotation for sense 2 : nlsj183\n",
      "\t\tword from annotation for sense 2 : 75653\n",
      "\t\tword from annotation for sense 2 : nlsj59923\n",
      "\t\tword from annotation for sense 2 : 70105\n",
      "\t\tword from annotation for sense 2 : 82758\n",
      "\texpert sense number  2 mus-2\n",
      "\t\tword from annotation for sense 2 : 62258\n",
      "\t\t\tword  62258 is in output for sense 2 with probability: 0.19607843137254902 and weight: 1.0\n",
      "\t\tword from annotation for sense 2 : 64586\n",
      "\t\tword from annotation for sense 2 : 58271\n",
      "\t\t\tword  58271 is in output for sense 2 with probability: 0.11764705882352941 and weight: 1.0\n",
      "\t\tword from annotation for sense 2 : nlsj86871\n",
      "\t\t\tword  nlsj86871 is in output for sense 2 with probability: 0.10049019607843138 and weight: 1.0\n",
      "\t\tword from annotation for sense 2 : 101851\n",
      "\t\tword from annotation for sense 2 : nlsj183\n",
      "\t\tword from annotation for sense 2 : 75653\n",
      "\t\t\tword  75653 is in output for sense 2 with probability: 0.07598039215686274 and weight: 1.0\n",
      "\t\tword from annotation for sense 2 : nlsj59923\n",
      "\t\tword from annotation for sense 2 : 70105\n",
      "\t\tword from annotation for sense 2 : 82758\n",
      "\t\t\tword  82758 is in output for sense 2 with probability: 0.058823529411764705 and weight: 0.5\n",
      "\texpert sense number  3 w\n",
      "\t\tword from annotation for sense 2 : 62258\n",
      "\t\tword from annotation for sense 2 : 64586\n",
      "\t\tword from annotation for sense 2 : 58271\n",
      "\t\tword from annotation for sense 2 : nlsj86871\n",
      "\t\tword from annotation for sense 2 : 101851\n",
      "\t\tword from annotation for sense 2 : nlsj183\n",
      "\t\tword from annotation for sense 2 : 75653\n",
      "\t\tword from annotation for sense 2 : nlsj59923\n",
      "\t\tword from annotation for sense 2 : 70105\n",
      "\t\tword from annotation for sense 2 : 82758\n",
      "output sense 3\n",
      "\texpert sense number  0 mus-1\n",
      "\t\tword from annotation for sense 3 : 102000\n",
      "\t\t\tword  102000 is in output for sense 3 with probability: 0.14893617021276598 and weight: 0.5\n",
      "\t\tword from annotation for sense 3 : 70495\n",
      "\t\tword from annotation for sense 3 : 7182\n",
      "\t\t\tword  7182 is in output for sense 3 with probability: 0.12340425531914895 and weight: 1.0\n",
      "\t\tword from annotation for sense 3 : 70958\n",
      "\t\t\tword  70958 is in output for sense 3 with probability: 0.1021276595744681 and weight: 0.5\n",
      "\t\tword from annotation for sense 3 : 49589\n",
      "\t\t\tword  49589 is in output for sense 3 with probability: 0.08936170212765958 and weight: 0.5\n",
      "\t\tword from annotation for sense 3 : 83174\n",
      "\t\tword from annotation for sense 3 : 106676\n",
      "\t\tword from annotation for sense 3 : 22209\n",
      "\t\t\tword  22209 is in output for sense 3 with probability: 0.07659574468085106 and weight: 0.5\n",
      "\t\tword from annotation for sense 3 : 93388\n",
      "\t\t\tword  93388 is in output for sense 3 with probability: 0.0723404255319149 and weight: 0.3333333333333333\n",
      "\t\tword from annotation for sense 3 : 16132\n",
      "\texpert sense number  1 mus-4\n",
      "\t\tword from annotation for sense 3 : 102000\n",
      "\t\t\tword  102000 is in output for sense 3 with probability: 0.14893617021276598 and weight: 0.5\n",
      "\t\tword from annotation for sense 3 : 70495\n",
      "\t\t\tword  70495 is in output for sense 3 with probability: 0.14893617021276598 and weight: 1.0\n",
      "\t\tword from annotation for sense 3 : 7182\n",
      "\t\tword from annotation for sense 3 : 70958\n",
      "\t\tword from annotation for sense 3 : 49589\n",
      "\t\tword from annotation for sense 3 : 83174\n",
      "\t\t\tword  83174 is in output for sense 3 with probability: 0.08936170212765958 and weight: 1.0\n",
      "\t\tword from annotation for sense 3 : 106676\n",
      "\t\t\tword  106676 is in output for sense 3 with probability: 0.08085106382978724 and weight: 1.0\n",
      "\t\tword from annotation for sense 3 : 22209\n",
      "\t\t\tword  22209 is in output for sense 3 with probability: 0.07659574468085106 and weight: 0.5\n",
      "\t\tword from annotation for sense 3 : 93388\n",
      "\t\t\tword  93388 is in output for sense 3 with probability: 0.0723404255319149 and weight: 0.3333333333333333\n",
      "\t\tword from annotation for sense 3 : 16132\n",
      "\texpert sense number  2 mus-2\n",
      "\t\tword from annotation for sense 3 : 102000\n",
      "\t\tword from annotation for sense 3 : 70495\n",
      "\t\tword from annotation for sense 3 : 7182\n",
      "\t\tword from annotation for sense 3 : 70958\n",
      "\t\tword from annotation for sense 3 : 49589\n",
      "\t\tword from annotation for sense 3 : 83174\n",
      "\t\tword from annotation for sense 3 : 106676\n",
      "\t\tword from annotation for sense 3 : 22209\n",
      "\t\tword from annotation for sense 3 : 93388\n",
      "\t\t\tword  93388 is in output for sense 3 with probability: 0.0723404255319149 and weight: 0.3333333333333333\n",
      "\t\tword from annotation for sense 3 : 16132\n",
      "\texpert sense number  3 w\n",
      "\t\tword from annotation for sense 3 : 102000\n",
      "\t\tword from annotation for sense 3 : 70495\n",
      "\t\tword from annotation for sense 3 : 7182\n",
      "\t\tword from annotation for sense 3 : 70958\n",
      "\t\t\tword  70958 is in output for sense 3 with probability: 0.1021276595744681 and weight: 0.5\n",
      "\t\tword from annotation for sense 3 : 49589\n",
      "\t\t\tword  49589 is in output for sense 3 with probability: 0.08936170212765958 and weight: 0.5\n",
      "\t\tword from annotation for sense 3 : 83174\n",
      "\t\tword from annotation for sense 3 : 106676\n",
      "\t\tword from annotation for sense 3 : 22209\n",
      "\t\tword from annotation for sense 3 : 93388\n",
      "\t\tword from annotation for sense 3 : 16132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tword  16132 is in output for sense 3 with probability: 0.06808510638297872 and weight: 1.0\n",
      "output sense 4\n",
      "\texpert sense number  0 mus-1\n",
      "\t\tword from annotation for sense 4 : nlsj5634\n",
      "\t\tword from annotation for sense 4 : 61925\n",
      "\t\t\tword  61925 is in output for sense 4 with probability: 0.13909774436090228 and weight: 1.0\n",
      "\t\tword from annotation for sense 4 : 12620\n",
      "\t\tword from annotation for sense 4 : 69419\n",
      "\t\t\tword  69419 is in output for sense 4 with probability: 0.10150375939849625 and weight: 0.25\n",
      "\t\tword from annotation for sense 4 : 104421\n",
      "\t\t\tword  104421 is in output for sense 4 with probability: 0.10150375939849625 and weight: 0.5\n",
      "\t\tword from annotation for sense 4 : 91085\n",
      "\t\t\tword  91085 is in output for sense 4 with probability: 0.09022556390977446 and weight: 1.0\n",
      "\t\tword from annotation for sense 4 : 71308\n",
      "\t\t\tword  71308 is in output for sense 4 with probability: 0.08270676691729324 and weight: 0.5\n",
      "\t\tword from annotation for sense 4 : 115748\n",
      "\t\tword from annotation for sense 4 : 19641\n",
      "\t\t\tword  19641 is in output for sense 4 with probability: 0.07518796992481204 and weight: 0.5\n",
      "\t\tword from annotation for sense 4 : 5390\n",
      "\texpert sense number  1 mus-4\n",
      "\t\tword from annotation for sense 4 : nlsj5634\n",
      "\t\tword from annotation for sense 4 : 61925\n",
      "\t\tword from annotation for sense 4 : 12620\n",
      "\t\tword from annotation for sense 4 : 69419\n",
      "\t\t\tword  69419 is in output for sense 4 with probability: 0.10150375939849625 and weight: 0.25\n",
      "\t\tword from annotation for sense 4 : 104421\n",
      "\t\tword from annotation for sense 4 : 91085\n",
      "\t\tword from annotation for sense 4 : 71308\n",
      "\t\tword from annotation for sense 4 : 115748\n",
      "\t\t\tword  115748 is in output for sense 4 with probability: 0.07894736842105265 and weight: 1.0\n",
      "\t\tword from annotation for sense 4 : 19641\n",
      "\t\tword from annotation for sense 4 : 5390\n",
      "\texpert sense number  2 mus-2\n",
      "\t\tword from annotation for sense 4 : nlsj5634\n",
      "\t\t\tword  nlsj5634 is in output for sense 4 with probability: 0.14661654135338348 and weight: 1.0\n",
      "\t\tword from annotation for sense 4 : 61925\n",
      "\t\tword from annotation for sense 4 : 12620\n",
      "\t\tword from annotation for sense 4 : 69419\n",
      "\t\t\tword  69419 is in output for sense 4 with probability: 0.10150375939849625 and weight: 0.25\n",
      "\t\tword from annotation for sense 4 : 104421\n",
      "\t\tword from annotation for sense 4 : 91085\n",
      "\t\tword from annotation for sense 4 : 71308\n",
      "\t\tword from annotation for sense 4 : 115748\n",
      "\t\tword from annotation for sense 4 : 19641\n",
      "\t\t\tword  19641 is in output for sense 4 with probability: 0.07518796992481204 and weight: 0.5\n",
      "\t\tword from annotation for sense 4 : 5390\n",
      "\texpert sense number  3 w\n",
      "\t\tword from annotation for sense 4 : nlsj5634\n",
      "\t\tword from annotation for sense 4 : 61925\n",
      "\t\tword from annotation for sense 4 : 12620\n",
      "\t\tword from annotation for sense 4 : 69419\n",
      "\t\t\tword  69419 is in output for sense 4 with probability: 0.10150375939849625 and weight: 0.25\n",
      "\t\tword from annotation for sense 4 : 104421\n",
      "\t\t\tword  104421 is in output for sense 4 with probability: 0.10150375939849625 and weight: 0.5\n",
      "\t\tword from annotation for sense 4 : 91085\n",
      "\t\tword from annotation for sense 4 : 71308\n",
      "\t\t\tword  71308 is in output for sense 4 with probability: 0.08270676691729324 and weight: 0.5\n",
      "\t\tword from annotation for sense 4 : 115748\n",
      "\t\tword from annotation for sense 4 : 19641\n",
      "\t\tword from annotation for sense 4 : 5390\n",
      "\t\t\tword  5390 is in output for sense 4 with probability: 0.07142857142857144 and weight: 1.0\n"
     ]
    }
   ],
   "source": [
    "for key in k_words_with_prob.keys():\n",
    "    print(\"output sense\",key)\n",
    "    for i in range(0,number_of_s):\n",
    "        print(\"\\texpert sense number \", i, expert_senses[i])\n",
    "        for second_key in k_words_with_prob[key].keys(): # Barbara's note: shouldn't it be k_words_with_prob[i] here?\n",
    "            print(\"\\t\\tword from annotation for sense\", key, \":\", second_key)\n",
    "            if second_key in dict_of_words[expert_senses[i]]:\n",
    "                print(\"\\t\\t\\tword \", second_key, \"is in output for sense\", key, \"with probability:\", k_words_with_prob[key][second_key], \"and weight:\", word_weight[second_key])\n",
    "\n",
    "                \n",
    "# Here we get all the senses and for each sense we do a matching between the k words and s words and get the probability\n",
    "# For some reason the first word for each sense arrives several times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of expert senses s: 4\n",
      "number of model output senses k: 5\n",
      "\n",
      "\n",
      "Choose best match for k = 0\n",
      "k = 0 \t s = 0 (= expert sense mus-1 )\t conf[k,s] = 8.360259433962197\n",
      "k = 0 \t s = 1 (= expert sense mus-4 )\t conf[k,s] = 8.100825471698055\n",
      "k = 0 \t s = 2 (= expert sense mus-2 )\t conf[k,s] = 7.671580188679194\n",
      "k = 0 \t s = 3 (= expert sense w )\t conf[k,s] = 7.647995283018815\n",
      "\n",
      "\n",
      "Choose best match for k = 1\n",
      "k = 1 \t s = 0 (= expert sense mus-1 )\t conf[k,s] = 0.3452054794520548\n",
      "k = 1 \t s = 1 (= expert sense mus-4 )\t conf[k,s] = 0.19726027397260265\n",
      "k = 1 \t s = 2 (= expert sense mus-2 )\t conf[k,s] = 0.6219178082191781\n",
      "k = 1 \t s = 3 (= expert sense w )\t conf[k,s] = 0.5835616438356163\n",
      "\n",
      "\n",
      "Choose best match for k = 2\n",
      "k = 2 \t s = 0 (= expert sense mus-1 )\t conf[k,s] = 0.4509803921568625\n",
      "k = 2 \t s = 1 (= expert sense mus-4 )\t conf[k,s] = 0.4779411764705884\n",
      "k = 2 \t s = 2 (= expert sense mus-2 )\t conf[k,s] = 2.012254901960786\n",
      "k = 2 \t s = 3 (= expert sense w )\t conf[k,s] = 0\n",
      "\n",
      "\n",
      "Choose best match for k = 3\n",
      "k = 3 \t s = 0 (= expert sense mus-1 )\t conf[k,s] = 1.9390070921985874\n",
      "k = 3 \t s = 1 (= expert sense mus-4 )\t conf[k,s] = 3.5539007092198607\n",
      "k = 3 \t s = 2 (= expert sense mus-2 )\t conf[k,s] = 0.09645390070921984\n",
      "k = 3 \t s = 3 (= expert sense w )\t conf[k,s] = 0.457446808510638\n",
      "\n",
      "\n",
      "Choose best match for k = 4\n",
      "k = 4 \t s = 0 (= expert sense mus-1 )\t conf[k,s] = 5.87312030075191\n",
      "k = 4 \t s = 1 (= expert sense mus-4 )\t conf[k,s] = 4.982142857142889\n",
      "k = 4 \t s = 2 (= expert sense mus-2 )\t conf[k,s] = 5.1550751879699614\n",
      "k = 4 \t s = 3 (= expert sense w )\t conf[k,s] = 5.452067669172961\n"
     ]
    }
   ],
   "source": [
    "## Calculating confidence score for each (words_of_k,words_of_s) pair\n",
    "\n",
    "# conf(k,s) = (p1*match(w1,s)+p2*match(w1,s)+px(wx,s))/10\n",
    "        # match(wx,s) =   1/number_of_senses_assigned_to_wx if s_is_one_of_them \n",
    "\n",
    "##### TODO: for now conf[k,s] is multiplied by the number of expert senses --- FIX\n",
    "    \n",
    "print(\"number of expert senses s:\",number_of_s)\n",
    "print(\"number of model output senses k:\",len(k_words_with_prob.keys()))\n",
    "compteur = 0\n",
    "\n",
    "match = dict()\n",
    "conf = dict()\n",
    "for k in k_words_with_prob.keys():  # for each output sense, we go through...\n",
    "    print(\"\\n\")\n",
    "    print(\"Choose best match for k =\",k)\n",
    "    for s in range(0,number_of_s):       # each expert sense\n",
    "        \n",
    "        conf[k,s] = 0 \n",
    "        \n",
    "        #print(\"expert sense\",s)\n",
    "        for mot in k_words_with_prob[k]:      # for each word within output by the model for the output sense\n",
    "            #print(k,mot)\n",
    "            #print(expert_senses[s])\n",
    "            \n",
    "            if mot in dict_of_words[expert_senses[s]]:  # if that word exists in the list of expert words for that sense\n",
    "                \n",
    "                #print(s,dict_of_words[expert_senses[s]])\n",
    "                #print(k_words_with_prob[k][mot])\n",
    "                \n",
    "                for word in list_of_all_words:  # this help getting a key for a dictionary later on\n",
    "                    if mot == word:\n",
    "                        match_weighted = float((k_words_with_prob[k][mot]))*word_weight[word] #this dictionary cfr comment on line 24\n",
    "                        # word_weight[word] is already \"1/number_of_expert_senses_assigned_to_this_word\"\n",
    "                        \n",
    "                        #print(\"sense\",expert_senses[s],\"word\",word,\"match_weighted\",match_weighted)\n",
    "                        \n",
    "                        #print(k,s,conf[k,s])\n",
    "\n",
    "                        \n",
    "                        # To fix? \n",
    "                        # The way the code works is that all matches happen number_of_s times\n",
    "                        # (number_of_s = number of expert senses)\n",
    "                        # easy fix is to divide the match score by number_of_s\n",
    "                        \n",
    "                        conf[k,s] = conf[k,s] + match_weighted/4\n",
    "                        \n",
    "                    #else: \n",
    "                        #print(word,\"has no match for sense\",expert_senses[s])\n",
    "                        #print(word,word_weight[word],\"match\",k_words_with_prob[k][mot],\"match weighted\",match_weighted)\n",
    "                    #print(\"test1\")\n",
    "                #print(\"test2\")\n",
    "                \n",
    "                    #compteur += 1\n",
    "                \n",
    "        if (k,s) in conf.keys():\n",
    "        \n",
    "            conf[k,s] = conf[k,s] \n",
    "            print(\"k =\",k,\"\\t s =\",s,\"(= expert sense\",expert_senses[s],\")\\t conf[k,s] =\",conf[k,s])\n",
    "            \n",
    "            #print(compteur)\n",
    "            \n",
    "    #print(k_words_with_prob[k])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Barbara's note: \"key\" and \"i\" may be different senses; for example, for key=0, this is the first sense in the output, and i=0 is the first sense annotated by the expert. I think what we want here is to try matching all key values with all i values; for each (key, i) pair, we get the conf(key, i), as in pages 5 ff. of the Goals and plan.docx document.\n",
    "\n",
    "Then, when possible, we can pick the best \"i\" for each \"key\"; we haven't yet decided how, but it will probably have to do with the maximum conf value.\n",
    "\n",
    "Once we have a key-->i mapping, we can calculate precision and recall.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: How well does the model assign the right words to a given sense of the target word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-f315d04cacf9>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-f315d04cacf9>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    for each k:\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# For each k, we use the words given by the expert as unquestionable truth.\n",
    "# Judging the model's assignment of words to a given sense becomes a question of precision and recall.\n",
    "\n",
    "# precision is all correct w weighted by their respective probabilities / all w weighted by their probabilities\n",
    "\n",
    "for each k:\n",
    "    for each w:\n",
    "        if w in expert_list:\n",
    "            w_weight = p*1\n",
    "            numerator += w_weight\n",
    "        w_weight = p*1\n",
    "        denominator += w_weight\n",
    "    precision = numerator/denominator\n",
    "    \n",
    "# recall is all correct w weighted by their respective probabilities / all w assigned to the sense by the expert\n",
    "for each k:\n",
    "    for each w:\n",
    "        if w in expert_list:\n",
    "            w_weight = p*1\n",
    "            numerator += w_weight\n",
    "    denominator = len(expert_list)\n",
    "    recall = numerator/denominator\n",
    "    \n",
    "# f-score can be used as well\n",
    "\n",
    "for each k:\n",
    "    f_score = 2 * precision * recall / (precision+recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For pair ks 0,0 :\n",
      "The RECALL is 7.311320754716979 / 836 = 0.008745598988895908\n",
      "The PRECISION is 0.7311320754716979 / 0.9999999999999999 = 0.731132075471698 \n",
      "\n",
      "The F-SCORE is 0.01728444622864534 \n",
      "\n",
      "For pair ks 0,1 :\n",
      "The RECALL is 3.915094339622641 / 337 = 0.01161749062202564\n",
      "The PRECISION is 0.3915094339622641 / 0.9999999999999999 = 0.3915094339622642 \n",
      "\n",
      "The F-SCORE is 0.022565385242781788 \n",
      "\n",
      "For pair ks 0,2 :\n",
      "The RECALL is 2.05188679245283 / 225 = 0.009119496855345912\n",
      "The PRECISION is 0.205188679245283 / 0.9999999999999999 = 0.20518867924528303 \n",
      "\n",
      "The F-SCORE is 0.01746286631874749 \n",
      "\n",
      "For pair ks 0,3 :\n",
      "The RECALL is 2.0283018867924527 / 114 = 0.017792121813968883\n",
      "The PRECISION is 0.20283018867924527 / 0.9999999999999999 = 0.2028301886792453 \n",
      "\n",
      "The F-SCORE is 0.032714546561168595 \n",
      "\n",
      "For pair ks 1,0 :\n",
      "The RECALL is 1.7260273972602735 / 836 = 0.002064626073277839\n",
      "The PRECISION is 0.17260273972602735 / 0.9999999999999997 = 0.1726027397260274 \n",
      "\n",
      "The F-SCORE is 0.004080443019527833 \n",
      "\n",
      "For pair ks 1,1 :\n",
      "The RECALL is 0.9863013698630134 / 337 = 0.0029267102963294166\n",
      "The PRECISION is 0.09863013698630134 / 0.9999999999999997 = 0.09863013698630137 \n",
      "\n",
      "The F-SCORE is 0.005684734120247915 \n",
      "\n",
      "For pair ks 1,2 :\n",
      "The RECALL is 1.4520547945205475 / 225 = 0.006453576864535767\n",
      "The PRECISION is 0.14520547945205475 / 0.9999999999999997 = 0.1452054794520548 \n",
      "\n",
      "The F-SCORE is 0.012357913144855725 \n",
      "\n",
      "For pair ks 1,3 :\n",
      "The RECALL is 4.109589041095889 / 114 = 0.03604902667627973\n",
      "The PRECISION is 0.4109589041095889 / 0.9999999999999997 = 0.410958904109589 \n",
      "\n",
      "The F-SCORE is 0.06628369421122403 \n",
      "\n",
      "For pair ks 2,0 :\n",
      "The RECALL is 3.2352941176470584 / 836 = 0.0038699690402476776\n",
      "The PRECISION is 0.32352941176470584 / 0.9999999999999999 = 0.3235294117647059 \n",
      "\n",
      "The F-SCORE is 0.007648449450702266 \n",
      "\n",
      "For pair ks 2,1 :\n",
      "The RECALL is 0.9558823529411764 / 337 = 0.002836446151160761\n",
      "The PRECISION is 0.09558823529411764 / 0.9999999999999999 = 0.09558823529411765 \n",
      "\n",
      "The F-SCORE is 0.005509408374300728 \n",
      "\n",
      "For pair ks 2,2 :\n",
      "The RECALL is 5.490196078431372 / 225 = 0.024400871459694985\n",
      "The PRECISION is 0.5490196078431372 / 0.9999999999999999 = 0.5490196078431373 \n",
      "\n",
      "The F-SCORE is 0.046725073007926575 \n",
      "\n",
      "For pair ks 2,3 :\n",
      "The RECALL is 0 / 114 = 0.0\n",
      "The PRECISION IS NA\n",
      "No F-SCORE, can't divide by 0\n",
      "\n",
      "\n",
      "For pair ks 3,0 :\n",
      "The RECALL is 6.127659574468086 / 836 = 0.0073297363330957965\n",
      "The PRECISION is 0.6127659574468086 / 1.0000000000000002 = 0.6127659574468085 \n",
      "\n",
      "The F-SCORE is 0.014486192847442283 \n",
      "\n",
      "For pair ks 3,1 :\n",
      "The RECALL is 6.170212765957448 / 337 = 0.018309236694235752\n",
      "The PRECISION is 0.6170212765957448 / 1.0000000000000002 = 0.6170212765957447 \n",
      "\n",
      "The F-SCORE is 0.0355631859709363 \n",
      "\n",
      "For pair ks 3,2 :\n",
      "The RECALL is 0.723404255319149 / 225 = 0.003215130023640662\n",
      "The PRECISION is 0.0723404255319149 / 1.0000000000000002 = 0.07234042553191489 \n",
      "\n",
      "The F-SCORE is 0.006156631960162971 \n",
      "\n",
      "For pair ks 3,3 :\n",
      "The RECALL is 2.595744680851064 / 114 = 0.02276969018290407\n",
      "The PRECISION is 0.25957446808510637 / 1.0000000000000002 = 0.2595744680851063 \n",
      "\n",
      "The F-SCORE is 0.04186684969114619 \n",
      "\n",
      "For pair ks 4,0 :\n",
      "The RECALL is 5.902255639097746 / 836 = 0.007060114400834624\n",
      "The PRECISION is 0.5902255639097745 / 1.0 = 0.5902255639097745 \n",
      "\n",
      "The F-SCORE is 0.01395332302387174 \n",
      "\n",
      "For pair ks 4,1 :\n",
      "The RECALL is 1.804511278195489 / 337 = 0.005354632872983647\n",
      "The PRECISION is 0.18045112781954892 / 1.0 = 0.18045112781954892 \n",
      "\n",
      "The F-SCORE is 0.010400641372884663 \n",
      "\n",
      "For pair ks 4,2 :\n",
      "The RECALL is 3.2330827067669174 / 225 = 0.014369256474519632\n",
      "The PRECISION is 0.32330827067669177 / 1.0 = 0.32330827067669177 \n",
      "\n",
      "The F-SCORE is 0.027515597504399292 \n",
      "\n",
      "For pair ks 4,3 :\n",
      "The RECALL is 3.571428571428572 / 114 = 0.03132832080200502\n",
      "The PRECISION is 0.3571428571428572 / 1.0 = 0.3571428571428572 \n",
      "\n",
      "The F-SCORE is 0.05760368663594471 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now that we have prob + word for each k,s pair we can calculate precision and recall \n",
    "\n",
    "list_with_ks = list()  # this list stores the k,s matches found above\n",
    "list_with_ks = [\"0,0\",\"0,1\",\"0,2\",\"0,3\",\"1,0\",\"1,1\",\"1,2\",\"1,3\",\"2,0\",\"2,1\",\"2,2\",\"2,3\",\"3,0\",\"3,1\",\"3,2\",\"3,3\",\"4,0\",\"4,1\",\"4,2\",\"4,3\"]\n",
    "\n",
    "\n",
    "for key in list_with_ks:\n",
    "    numerator_recall = 0\n",
    "    denominator_precision = 0\n",
    "    numerator_precision = 0\n",
    "    for word in k_words_with_prob[int(key[0])]: \n",
    "        w_weight_precision = k_words_with_prob[int(key[0])][word] * 1\n",
    "        if word in dict_of_words[expert_senses[int(key[2])]]:   \n",
    "            w_weight_recall = k_words_with_prob[int(key[0])][word] * 1\n",
    "            numerator_recall += float(w_weight_recall)\n",
    "            #w_weight_precision = k_words_with_prob[int(key[0])][word] * 1  this was moved above the if\n",
    "            # cfr Valerio's email from March 28\n",
    "            numerator_precision += float(w_weight_precision)\n",
    "    \n",
    "        \n",
    "        denominator_precision += float(w_weight_precision)\n",
    "    denominator_recall = len(dict_of_words[expert_senses[int(key[2])]])\n",
    "    numerator_recall = numerator_recall*10\n",
    "            \n",
    "    print(\"For pair ks\",key,\":\")\n",
    "    print(\"The RECALL is\",numerator_recall,\"/\",denominator_recall,\"=\",numerator_recall/denominator_recall)\n",
    "    if numerator_precision == 0:\n",
    "        print(\"The PRECISION IS NA\")\n",
    "    else:\n",
    "        print(\"The PRECISION is\",numerator_precision,\"/\",denominator_precision,\"=\",numerator_precision/denominator_precision,\"\\n\")\n",
    "    if (numerator_precision/denominator_precision)+(numerator_recall/denominator_recall) != 0:\n",
    "        print(\"The F-SCORE is\", (2*(numerator_precision/denominator_precision)*(numerator_recall/denominator_recall)/((numerator_precision/denominator_precision)+(numerator_recall/denominator_recall))),\"\\n\")\n",
    "    else:\n",
    "        print(\"No F-SCORE, can't divide by 0\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'28355': 0.2075471698113207, '69419': 0.16273584905660377, '57460': 0.1320754716981132, '114587': 0.0990566037735849, '42071': 0.09669811320754716, '35267': 0.08254716981132075, '51647': 0.08254716981132075, '64448': 0.05424528301886792, '45980': 0.042452830188679236, '53826': 0.04009433962264151}, 1: {'79223': 0.17260273972602735, '92927': 0.1561643835616438, '46574': 0.10410958904109587, '67660': 0.10410958904109587, '103085': 0.09863013698630134, '86112': 0.08219178082191778, '101982': 0.07945205479452053, '75808': 0.07123287671232874, '68539': 0.06575342465753423, '54607': 0.06575342465753423}, 2: {'62258': 0.19607843137254902, '64586': 0.12745098039215685, '58271': 0.11764705882352941, 'nlsj86871': 0.10049019607843138, '101851': 0.09558823529411764, 'nlsj183': 0.0906862745098039, '75653': 0.07598039215686274, 'nlsj59923': 0.07352941176470587, '70105': 0.06372549019607843, '82758': 0.058823529411764705}, 3: {'102000': 0.14893617021276598, '70495': 0.14893617021276598, '7182': 0.12340425531914895, '70958': 0.1021276595744681, '49589': 0.08936170212765958, '83174': 0.08936170212765958, '106676': 0.08085106382978724, '22209': 0.07659574468085106, '93388': 0.0723404255319149, '16132': 0.06808510638297872}, 4: {'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228, '12620': 0.11278195488721805, '69419': 0.10150375939849625, '104421': 0.10150375939849625, '91085': 0.09022556390977446, '71308': 0.08270676691729324, '115748': 0.07894736842105265, '19641': 0.07518796992481204, '5390': 0.07142857142857144}}\n"
     ]
    }
   ],
   "source": [
    "print(k_words_with_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mus-1', 'mus-4', 'mus-2', 'w']\n"
     ]
    }
   ],
   "source": [
    "print(expert_senses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-17-d6839a6dbd08>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-d6839a6dbd08>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    #print(i,dict_of_words[i])\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "for i in expert_senses:\n",
    "    #print(i,dict_of_words[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qx: Model(s) comparison against annotated subcorpus (sense importance evolution + sense emergence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode\n",
    "\n",
    "1. Parse senses_target.txt to get:\n",
    "\n",
    "    1.1 the date\n",
    "    \n",
    "    1.2 the number of senses at that date\n",
    "    \n",
    "    1.3 the number of uses of each sense at that date\n",
    "    \n",
    "    \n",
    "2. Using the numbers found in 1.3, plot the emergence of new senses and the distribution of others\n",
    "\n",
    "\n",
    "confidence interval!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new earliest sense: -430\n",
      "new latest sense: -430\n",
      "new latest sense: -425\n",
      "new latest sense: -422\n",
      "new latest sense: -420\n",
      "new latest sense: -415\n",
      "new latest sense: -370\n",
      "new latest sense: -362\n",
      "new latest sense: -355\n",
      "new latest sense: -350\n",
      "new latest sense: -335\n",
      "new latest sense: -300\n",
      "new latest sense: -270\n",
      "new latest sense: -250\n",
      "new latest sense: -150\n",
      "new latest sense: -35\n",
      "new latest sense: -7\n",
      "new latest sense: 90\n",
      "new latest sense: 93\n",
      "new latest sense: 95\n",
      "new latest sense: 100\n",
      "new latest sense: 108\n",
      "new latest sense: 150\n",
      "new latest sense: 170\n",
      "new latest sense: 175\n",
      "new latest sense: 176\n",
      "new latest sense: 180\n",
      "new latest sense: 185\n",
      "new latest sense: 195\n",
      "new latest sense: 200\n",
      "new latest sense: 220\n",
      "new latest sense: 228\n",
      "new latest sense: 230\n",
      "new latest sense: 238\n",
      "new latest sense: 359\n"
     ]
    }
   ],
   "source": [
    "# this is for the expert senses (gold standard truth of sense predominance in our corpus)\n",
    "\n",
    "\n",
    "expert_senses_chart = list() # list where we store all sense ids provided by expert\n",
    "sense_year = dict()\n",
    "earliest_sense = 10000 #just to be safe\n",
    "latest_sense = -10000\n",
    "\n",
    "for s in file_senses:\n",
    "    s = s.split(\"\\t\")\n",
    "    sense = s[11] # The sense ID is after the 10th tab\n",
    "    \n",
    "    \n",
    "    try: \n",
    "        sense_year[sense].append(int(s[0])) # The date is the first tab\n",
    "    except KeyError:\n",
    "        sense_year[sense] = list()\n",
    "        sense_year[sense].append(int(s[0])) # The date is the first tab\n",
    "    \n",
    "    expert_senses_chart.append(sense)\n",
    "\n",
    "    if int(s[0]) < earliest_sense:\n",
    "        earliest_sense = int(s[0])\n",
    "        print(\"new earliest sense:\",earliest_sense)\n",
    "        \n",
    "    if int(s[0]) > latest_sense:\n",
    "        latest_sense = int(s[0])\n",
    "        print(\"new latest sense:\",latest_sense)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating the slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lol\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112.71428571428571\n",
      "-430 359\n"
     ]
    }
   ],
   "source": [
    "number_of_slices = 7 # that's what the model outputs now\n",
    "slice_duration = (latest_sense - earliest_sense)/number_of_slices\n",
    "print(slice_duration)\n",
    "print(earliest_sense,latest_sense)\n",
    "slice_years = dict()\n",
    "\n",
    "for period in range(0,number_of_slices):\n",
    "    slice_years[period] = list()\n",
    "    \n",
    "    for i in range(earliest_sense,latest_sense):\n",
    "        if i > int(period*slice_duration) + earliest_sense:\n",
    "            if i < int((period+1)*slice_duration) + earliest_sense:\n",
    "                slice_years[period].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [-429, -428, -427, -426, -425, -424, -423, -422, -421, -420, -419, -418, -417, -416, -415, -414, -413, -412, -411, -410, -409, -408, -407, -406, -405, -404, -403, -402, -401, -400, -399, -398, -397, -396, -395, -394, -393, -392, -391, -390, -389, -388, -387, -386, -385, -384, -383, -382, -381, -380, -379, -378, -377, -376, -375, -374, -373, -372, -371, -370, -369, -368, -367, -366, -365, -364, -363, -362, -361, -360, -359, -358, -357, -356, -355, -354, -353, -352, -351, -350, -349, -348, -347, -346, -345, -344, -343, -342, -341, -340, -339, -338, -337, -336, -335, -334, -333, -332, -331, -330, -329, -328, -327, -326, -325, -324, -323, -322, -321, -320, -319] \n",
      "\n",
      "1 [-317, -316, -315, -314, -313, -312, -311, -310, -309, -308, -307, -306, -305, -304, -303, -302, -301, -300, -299, -298, -297, -296, -295, -294, -293, -292, -291, -290, -289, -288, -287, -286, -285, -284, -283, -282, -281, -280, -279, -278, -277, -276, -275, -274, -273, -272, -271, -270, -269, -268, -267, -266, -265, -264, -263, -262, -261, -260, -259, -258, -257, -256, -255, -254, -253, -252, -251, -250, -249, -248, -247, -246, -245, -244, -243, -242, -241, -240, -239, -238, -237, -236, -235, -234, -233, -232, -231, -230, -229, -228, -227, -226, -225, -224, -223, -222, -221, -220, -219, -218, -217, -216, -215, -214, -213, -212, -211, -210, -209, -208, -207, -206] \n",
      "\n",
      "2 [-204, -203, -202, -201, -200, -199, -198, -197, -196, -195, -194, -193, -192, -191, -190, -189, -188, -187, -186, -185, -184, -183, -182, -181, -180, -179, -178, -177, -176, -175, -174, -173, -172, -171, -170, -169, -168, -167, -166, -165, -164, -163, -162, -161, -160, -159, -158, -157, -156, -155, -154, -153, -152, -151, -150, -149, -148, -147, -146, -145, -144, -143, -142, -141, -140, -139, -138, -137, -136, -135, -134, -133, -132, -131, -130, -129, -128, -127, -126, -125, -124, -123, -122, -121, -120, -119, -118, -117, -116, -115, -114, -113, -112, -111, -110, -109, -108, -107, -106, -105, -104, -103, -102, -101, -100, -99, -98, -97, -96, -95, -94, -93] \n",
      "\n",
      "3 [-91, -90, -89, -88, -87, -86, -85, -84, -83, -82, -81, -80, -79, -78, -77, -76, -75, -74, -73, -72, -71, -70, -69, -68, -67, -66, -65, -64, -63, -62, -61, -60, -59, -58, -57, -56, -55, -54, -53, -52, -51, -50, -49, -48, -47, -46, -45, -44, -43, -42, -41, -40, -39, -38, -37, -36, -35, -34, -33, -32, -31, -30, -29, -28, -27, -26, -25, -24, -23, -22, -21, -20, -19, -18, -17, -16, -15, -14, -13, -12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] \n",
      "\n",
      "4 [21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132] \n",
      "\n",
      "5 [134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245] \n",
      "\n",
      "6 [247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in slice_years.keys():\n",
    "    print(key,slice_years[key],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting the number of hits per sense per period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sense: mus-1\n",
      "mus-1 106\n",
      "Sense: mus-4\n",
      "mus-4 35\n",
      "Sense: w\n",
      "w 7\n",
      "Sense: mus-2\n",
      "mus-2 24\n",
      "{('mus-1', 0): 10, ('mus-1', 1): 13, ('mus-1', 2): 17, ('mus-1', 3): 28, ('mus-1', 4): 55, ('mus-1', 5): 106, ('mus-1', 6): 106, ('mus-4', 0): 22, ('mus-4', 1): 23, ('mus-4', 2): 23, ('mus-4', 3): 25, ('mus-4', 4): 26, ('mus-4', 5): 35, ('mus-4', 6): 35, ('w', 0): 2, ('w', 1): 2, ('w', 2): 2, ('w', 3): 2, ('w', 4): 3, ('w', 5): 7, ('w', 6): 7, ('mus-2', 0): 0, ('mus-2', 1): 0, ('mus-2', 2): 0, ('mus-2', 3): 0, ('mus-2', 4): 0, ('mus-2', 5): 24, ('mus-2', 6): 24}\n"
     ]
    }
   ],
   "source": [
    "sense_date_amount = dict()\n",
    "\n",
    "for sense in sense_year.keys():\n",
    "   \n",
    "    print(\"Sense:\",sense)\n",
    "    counter = 0\n",
    "    for i in range(0,number_of_slices):\n",
    "        #print(\"period\",i,\"years for that sense in that period\",sense_year[sense])\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(sense_year[sense])\n",
    "        for year in sense_year[sense]:\n",
    "        \n",
    "            if year in slice_years[i]:\n",
    "                counter += 1\n",
    "                #print(sense_year[sense][i])\n",
    "                \n",
    "        sense_date_amount[sense,i] = counter           \n",
    "    print(sense,counter)\n",
    "    \n",
    "print(sense_date_amount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting the relative number of hits per sense per period\n",
    "(for plotting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total period {0: 34, 1: 38, 2: 42, 3: 55, 4: 84, 5: 172, 6: 172}\n",
      "sense date amount {('mus-1', 0): 10, ('mus-1', 1): 13, ('mus-1', 2): 17, ('mus-1', 3): 28, ('mus-1', 4): 55, ('mus-1', 5): 106, ('mus-1', 6): 106, ('mus-4', 0): 22, ('mus-4', 1): 23, ('mus-4', 2): 23, ('mus-4', 3): 25, ('mus-4', 4): 26, ('mus-4', 5): 35, ('mus-4', 6): 35, ('w', 0): 2, ('w', 1): 2, ('w', 2): 2, ('w', 3): 2, ('w', 4): 3, ('w', 5): 7, ('w', 6): 7, ('mus-2', 0): 0, ('mus-2', 1): 0, ('mus-2', 2): 0, ('mus-2', 3): 0, ('mus-2', 4): 0, ('mus-2', 5): 24, ('mus-2', 6): 24}\n",
      "('mus-1', 0) total for this sense at this period 10 total period 34\n",
      "relative 0.29411764705882354\n",
      "('mus-1', 1) total for this sense at this period 13 total period 38\n",
      "relative 0.34210526315789475\n",
      "('mus-1', 2) total for this sense at this period 17 total period 42\n",
      "relative 0.40476190476190477\n",
      "('mus-1', 3) total for this sense at this period 28 total period 55\n",
      "relative 0.509090909090909\n",
      "('mus-1', 4) total for this sense at this period 55 total period 84\n",
      "relative 0.6547619047619048\n",
      "('mus-1', 5) total for this sense at this period 106 total period 172\n",
      "relative 0.6162790697674418\n",
      "('mus-1', 6) total for this sense at this period 106 total period 172\n",
      "relative 0.6162790697674418\n",
      "('mus-4', 0) total for this sense at this period 22 total period 34\n",
      "relative 0.6470588235294118\n",
      "('mus-4', 1) total for this sense at this period 23 total period 38\n",
      "relative 0.6052631578947368\n",
      "('mus-4', 2) total for this sense at this period 23 total period 42\n",
      "relative 0.5476190476190477\n",
      "('mus-4', 3) total for this sense at this period 25 total period 55\n",
      "relative 0.45454545454545453\n",
      "('mus-4', 4) total for this sense at this period 26 total period 84\n",
      "relative 0.30952380952380953\n",
      "('mus-4', 5) total for this sense at this period 35 total period 172\n",
      "relative 0.20348837209302326\n",
      "('mus-4', 6) total for this sense at this period 35 total period 172\n",
      "relative 0.20348837209302326\n",
      "('w', 0) total for this sense at this period 2 total period 34\n",
      "relative 0.058823529411764705\n",
      "('w', 1) total for this sense at this period 2 total period 38\n",
      "relative 0.05263157894736842\n",
      "('w', 2) total for this sense at this period 2 total period 42\n",
      "relative 0.047619047619047616\n",
      "('w', 3) total for this sense at this period 2 total period 55\n",
      "relative 0.03636363636363636\n",
      "('w', 4) total for this sense at this period 3 total period 84\n",
      "relative 0.03571428571428571\n",
      "('w', 5) total for this sense at this period 7 total period 172\n",
      "relative 0.040697674418604654\n",
      "('w', 6) total for this sense at this period 7 total period 172\n",
      "relative 0.040697674418604654\n",
      "('mus-2', 0) total for this sense at this period 0 total period 34\n",
      "relative 0.0\n",
      "('mus-2', 1) total for this sense at this period 0 total period 38\n",
      "relative 0.0\n",
      "('mus-2', 2) total for this sense at this period 0 total period 42\n",
      "relative 0.0\n",
      "('mus-2', 3) total for this sense at this period 0 total period 55\n",
      "relative 0.0\n",
      "('mus-2', 4) total for this sense at this period 0 total period 84\n",
      "relative 0.0\n",
      "('mus-2', 5) total for this sense at this period 24 total period 172\n",
      "relative 0.13953488372093023\n",
      "('mus-2', 6) total for this sense at this period 24 total period 172\n",
      "relative 0.13953488372093023\n",
      "{('mus-1', 0): 0.29411764705882354, ('mus-1', 1): 0.34210526315789475, ('mus-1', 2): 0.40476190476190477, ('mus-1', 3): 0.509090909090909, ('mus-1', 4): 0.6547619047619048, ('mus-1', 5): 0.6162790697674418, ('mus-1', 6): 0.6162790697674418, ('mus-4', 0): 0.6470588235294118, ('mus-4', 1): 0.6052631578947368, ('mus-4', 2): 0.5476190476190477, ('mus-4', 3): 0.45454545454545453, ('mus-4', 4): 0.30952380952380953, ('mus-4', 5): 0.20348837209302326, ('mus-4', 6): 0.20348837209302326, ('w', 0): 0.058823529411764705, ('w', 1): 0.05263157894736842, ('w', 2): 0.047619047619047616, ('w', 3): 0.03636363636363636, ('w', 4): 0.03571428571428571, ('w', 5): 0.040697674418604654, ('w', 6): 0.040697674418604654, ('mus-2', 0): 0.0, ('mus-2', 1): 0.0, ('mus-2', 2): 0.0, ('mus-2', 3): 0.0, ('mus-2', 4): 0.0, ('mus-2', 5): 0.13953488372093023, ('mus-2', 6): 0.13953488372093023}\n",
      "mus-1 0 0.29411764705882354\n",
      "mus-4 0 0.6470588235294118\n",
      "mus-2 0 0.0\n",
      "w 0 0.058823529411764705\n",
      "mus-1 1 0.34210526315789475\n",
      "mus-4 1 0.6052631578947368\n",
      "mus-2 1 0.0\n",
      "w 1 0.05263157894736842\n",
      "mus-1 2 0.40476190476190477\n",
      "mus-4 2 0.5476190476190477\n",
      "mus-2 2 0.0\n",
      "w 2 0.047619047619047616\n",
      "mus-1 3 0.509090909090909\n",
      "mus-4 3 0.45454545454545453\n",
      "mus-2 3 0.0\n",
      "w 3 0.03636363636363636\n",
      "mus-1 4 0.6547619047619048\n",
      "mus-4 4 0.30952380952380953\n",
      "mus-2 4 0.0\n",
      "w 4 0.03571428571428571\n",
      "mus-1 5 0.6162790697674418\n",
      "mus-4 5 0.20348837209302326\n",
      "mus-2 5 0.13953488372093023\n",
      "w 5 0.040697674418604654\n",
      "mus-1 6 0.6162790697674418\n",
      "mus-4 6 0.20348837209302326\n",
      "mus-2 6 0.13953488372093023\n",
      "w 6 0.040697674418604654\n",
      "{0: [0.29411764705882354, 0.6470588235294118, 0.0, 0.058823529411764705], 1: [0.34210526315789475, 0.6052631578947368, 0.0, 0.05263157894736842], 2: [0.40476190476190477, 0.5476190476190477, 0.0, 0.047619047619047616], 3: [0.509090909090909, 0.45454545454545453, 0.0, 0.03636363636363636], 4: [0.6547619047619048, 0.30952380952380953, 0.0, 0.03571428571428571], 5: [0.6162790697674418, 0.20348837209302326, 0.13953488372093023, 0.040697674418604654], 6: [0.6162790697674418, 0.20348837209302326, 0.13953488372093023, 0.040697674418604654]}\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "total_period = dict()\n",
    "N = number_of_slices\n",
    "\n",
    "sense_period_relative = dict()\n",
    "\n",
    "for i in range(0,number_of_slices):\n",
    "    for entry in expert_senses:\n",
    "        \n",
    "# for period i we store for each sense the number of times the sense is seen\n",
    "        \n",
    "        try:\n",
    "            total_period[i] += sense_date_amount[entry,i]\n",
    "        except KeyError:\n",
    "            total_period[i] = 0\n",
    "            total_period[i] += sense_date_amount[entry,i]\n",
    "            \n",
    "        #print(i,entry,\"+\",sense_date_amount[entry,i],\"=\",total_period[i])\n",
    "        \n",
    "        \n",
    "print(\"total period\",total_period)\n",
    "print(\"sense date amount\",sense_date_amount)\n",
    "        \n",
    "for key in sense_date_amount:\n",
    "    \n",
    "    # for each (sense,period) pair we divide the number by the total number of words at that period\n",
    "    \n",
    "    print(key,\"total for this sense at this period\",sense_date_amount[key],\"total period\",total_period[key[1]])\n",
    "    \n",
    "    sense_period_relative[key] = float(sense_date_amount[key]/total_period[key[1]])\n",
    "    print(\"relative\",sense_period_relative[key])\n",
    "  \n",
    "print(sense_period_relative)        \n",
    "\n",
    "period_relative = dict()\n",
    "temp_list = list()\n",
    "\n",
    "for i in range(0,number_of_slices):\n",
    "    temp_list = list()\n",
    "    for entry in expert_senses:\n",
    "        if len(temp_list) < len(expert_senses):\n",
    "            temp_list.append(sense_period_relative[entry,i])\n",
    "            print(entry,i,sense_period_relative[entry,i])\n",
    "        \n",
    "    period_relative[i] = temp_list\n",
    "        \n",
    "        \n",
    "print(period_relative)\n",
    "print(number_of_slices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting expert annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'period_relative' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-daed9cc862a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mvaleurs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperiod_relative\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mcolours\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'g'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'm'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#to make sure colours remain the same throughout all slices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'period_relative' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "valeurs = period_relative\n",
    "colours = ['b','g','r','c','m','y','k'] #to make sure colours remain the same throughout all slices\n",
    "\n",
    "valeurs2 = dict()\n",
    "\n",
    "for key in valeurs.keys():\n",
    "    #print(key)\n",
    "    list_temp = list()\n",
    "    for item in valeurs[key]:\n",
    "        list_temp.append(int(item*100))   # let's have percentages and not .xx\n",
    "    valeurs2[key] = list_temp\n",
    "\n",
    "    #for value in valeurs\n",
    "    \n",
    "\n",
    "for key,vals in valeurs2.items():\n",
    "    print(key,vals)\n",
    "    \n",
    "    for i in range(0,len(vals)):        \n",
    "        if i == 0:\n",
    "            previous = 0\n",
    "            plt.bar(x=key, height=vals[i],bottom=previous,color=colours[i])\n",
    "            \n",
    "        else:         \n",
    "            previous = vals[i-1] + previous\n",
    "            plt.bar(x=key, height=vals[i],bottom=previous,color=colours[i])\n",
    "            \n",
    "plt.xticks(range(len(valeurs2)), valeurs2.keys())\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "image = plt.gcf()\n",
    "image.savefig(dir_out+\"/mus_expert.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "liste_number_year = list() # creating a list because matplotlib wants a tuple\n",
    "for key in sense_date_amount.keys():\n",
    "    #print(key)\n",
    "    liste_number_year.append(sense_date_amount[key])\n",
    "    \n",
    "tuple_number_year = tuple(liste_number_year)\n",
    "#print(tuple_number_year)\n",
    "\n",
    "period_number = dict()\n",
    "\n",
    "for key in sense_date_amount.keys():\n",
    "    compteur = 0\n",
    "    if key[1] in range(0,number_of_slices):\n",
    "        print(key,sense_date_amount[key[0],key[1]])\n",
    "        compteur += sense_date_amount[key[0],key[1]]\n",
    "        \n",
    "        try :\n",
    "            period_number[key[1]] += compteur\n",
    "        except KeyError:\n",
    "            period_number[key[1]] = 0\n",
    "            period_number[key[1]] += compteur\n",
    "            \n",
    "        \n",
    "for entry in period_number:\n",
    "    print(\"période\",entry,\"number of uses\",period_number[entry])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading model output for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_output_plot = output_senses.split(\"===============  per time  ===============\")[1].split(\"\\n\")\n",
    "period_relative_model = dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 []\n",
      "0 [0.04083955068731151]\n",
      "0 [0.04083955068731151, 0.008486796949771783]\n",
      "0 [0.04083955068731151, 0.008486796949771783, 0.00597187964362235]\n",
      "0 [0.04083955068731151, 0.008486796949771783, 0.00597187964362235, 0.7697746391951782]\n",
      "0 [0.04083955068731151, 0.008486796949771783, 0.00597187964362235, 0.7697746391951782, 0.17492713352411612]\n",
      "1 []\n",
      "1 [0.10907371060119478]\n",
      "1 [0.10907371060119478, 0.014298730684103153]\n",
      "1 [0.10907371060119478, 0.014298730684103153, 0.03384641373162034]\n",
      "1 [0.10907371060119478, 0.014298730684103153, 0.03384641373162034, 0.5064813344844331]\n",
      "1 [0.10907371060119478, 0.014298730684103153, 0.03384641373162034, 0.5064813344844331, 0.3362998104986487]\n",
      "2 []\n",
      "2 [0.08757407290535785]\n",
      "2 [0.08757407290535785, 0.015051254608967434]\n",
      "2 [0.08757407290535785, 0.015051254608967434, 0.024163835560131568]\n",
      "2 [0.08757407290535785, 0.015051254608967434, 0.024163835560131568, 0.6984925663043847]\n",
      "2 [0.08757407290535785, 0.015051254608967434, 0.024163835560131568, 0.6984925663043847, 0.17471827062115847]\n",
      "3 []\n",
      "3 [0.07407106626171724]\n",
      "3 [0.07407106626171724, 0.03404926797763053]\n",
      "3 [0.07407106626171724, 0.03404926797763053, 0.04501432410891889]\n",
      "3 [0.07407106626171724, 0.03404926797763053, 0.04501432410891889, 0.6654202704667155]\n",
      "3 [0.07407106626171724, 0.03404926797763053, 0.04501432410891889, 0.6654202704667155, 0.18144507118501788]\n",
      "4 []\n",
      "4 [0.1803754795434305]\n",
      "4 [0.1803754795434305, 0.04005687932522156]\n",
      "4 [0.1803754795434305, 0.04005687932522156, 0.10369804638106733]\n",
      "4 [0.1803754795434305, 0.04005687932522156, 0.10369804638106733, 0.43394041107060427]\n",
      "4 [0.1803754795434305, 0.04005687932522156, 0.10369804638106733, 0.43394041107060427, 0.24192918367967622]\n",
      "5 []\n",
      "5 [0.17698941573659951]\n",
      "5 [0.17698941573659951, 0.03704265374613105]\n",
      "5 [0.17698941573659951, 0.03704265374613105, 0.14841983538917458]\n",
      "5 [0.17698941573659951, 0.03704265374613105, 0.14841983538917458, 0.41587711391800924]\n",
      "5 [0.17698941573659951, 0.03704265374613105, 0.14841983538917458, 0.41587711391800924, 0.2216709812100856]\n",
      "6 []\n",
      "6 [0.20603546151577476]\n",
      "6 [0.20603546151577476, 0.0777841384954993]\n",
      "6 [0.20603546151577476, 0.0777841384954993, 0.2523433469634035]\n",
      "6 [0.20603546151577476, 0.0777841384954993, 0.2523433469634035, 0.3203623104634722]\n",
      "6 [0.20603546151577476, 0.0777841384954993, 0.2523433469634035, 0.3203623104634722, 0.14347474256185028]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(lines_output_plot)):\n",
    "    if lines_output_plot[i][0:5] == \"Time=\":  # if a line starts with \"time\" we take it into account\n",
    "        for x in range(i,i+number_of_the_k+1): # for every \"number of  the k\" lines that follow\n",
    "            #print(lines_output_plot[x])\n",
    "            if lines_output_plot[x][0:5] == \"Time=\": # if a line starts with \"time\" we take the value for the slice\n",
    "                period = lines_output_plot[x][5:6]\n",
    "                templist = list()\n",
    "                \n",
    "            if lines_output_plot[x][0:5] != \"Time=\":  # if a line doesn't start with \"time\" but is considered(cf line3)\n",
    "                ligne = re.split(\"\\s{3,}\",lines_output_plot[x]) # we take the first part of the line (importance of that K)\n",
    "                templist.append(float(ligne[0]))\n",
    "            print(period,templist)\n",
    "            \n",
    "        period_relative_model[str(period)] = templist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': [0.04083955068731151, 0.008486796949771783, 0.00597187964362235, 0.7697746391951782, 0.17492713352411612], '1': [0.10907371060119478, 0.014298730684103153, 0.03384641373162034, 0.5064813344844331, 0.3362998104986487], '2': [0.08757407290535785, 0.015051254608967434, 0.024163835560131568, 0.6984925663043847, 0.17471827062115847], '3': [0.07407106626171724, 0.03404926797763053, 0.04501432410891889, 0.6654202704667155, 0.18144507118501788], '4': [0.1803754795434305, 0.04005687932522156, 0.10369804638106733, 0.43394041107060427, 0.24192918367967622], '5': [0.17698941573659951, 0.03704265374613105, 0.14841983538917458, 0.41587711391800924, 0.2216709812100856], '6': [0.20603546151577476, 0.0777841384954993, 0.2523433469634035, 0.3203623104634722, 0.14347474256185028]}\n"
     ]
    }
   ],
   "source": [
    "print(period_relative_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.04083955068731151, 0.008486796949771783, 0.00597187964362235, 0.7697746391951782, 0.17492713352411612]\n",
      "<class 'str'> <class 'list'>\n",
      "1 [0.10907371060119478, 0.014298730684103153, 0.03384641373162034, 0.5064813344844331, 0.3362998104986487]\n",
      "<class 'str'> <class 'list'>\n",
      "2 [0.08757407290535785, 0.015051254608967434, 0.024163835560131568, 0.6984925663043847, 0.17471827062115847]\n",
      "<class 'str'> <class 'list'>\n",
      "3 [0.07407106626171724, 0.03404926797763053, 0.04501432410891889, 0.6654202704667155, 0.18144507118501788]\n",
      "<class 'str'> <class 'list'>\n",
      "4 [0.1803754795434305, 0.04005687932522156, 0.10369804638106733, 0.43394041107060427, 0.24192918367967622]\n",
      "<class 'str'> <class 'list'>\n",
      "5 [0.17698941573659951, 0.03704265374613105, 0.14841983538917458, 0.41587711391800924, 0.2216709812100856]\n",
      "<class 'str'> <class 'list'>\n",
      "6 [0.20603546151577476, 0.0777841384954993, 0.2523433469634035, 0.3203623104634722, 0.14347474256185028]\n",
      "<class 'str'> <class 'list'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "valeurs = period_relative_model\n",
    "colours = ['b','g','r','c','m','y','k'] #to make sure colours remain the same throughout all slices\n",
    "    \n",
    "\n",
    "for key,vals in valeurs.items():\n",
    "    print(key,vals)\n",
    "    print(type(key),type(vals))\n",
    "    \n",
    "    for i in range(0,len(vals)):        \n",
    "        if i == 0:\n",
    "            previous = 0\n",
    "            plt.bar(x=key, height=vals[i],bottom=previous,color=colours[i])\n",
    "            \n",
    "        else:         \n",
    "            previous = vals[i-1] + previous\n",
    "            plt.bar(x=key, height=vals[i],bottom=previous,color=colours[i])\n",
    "            \n",
    "plt.xticks(range(len(valeurs)), valeurs.keys())\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.savefig(dir_out+\"/mus_model.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
