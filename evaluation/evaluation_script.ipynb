{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a work-in-progress notebook\n",
    "\n",
    "We wish to know this:\n",
    "\n",
    "1. How well does the model identify the correct number of senses for the target word?\n",
    "2. **How well does the model identify the correct senses for the target word?**\n",
    "3. **How well does the model assign the right words to a given sense of the target word?**\n",
    "4. How well does the model assign the senses to the time intervals for the target word?\n",
    "\n",
    "The script will evaluate **Q2** and **Q3**. Q4 will follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic variables and imports:\n",
    "\n",
    "import codecs, csv, os, time, re, io\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "# directories\n",
    "dir_in = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), \"evaluation\", \"evaluation_input\"))\n",
    "dir_out = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), \"evaluation\", \"evaluation_output\"))\n",
    "\n",
    "\n",
    "s_senses = io.open(dir_in+\"/senses_69419.txt\",\"r\")\n",
    "k_senses = io.open(dir_in+\"/mus.dat\",\"r\")\n",
    "\n",
    "# DEBUG:\n",
    "#s_senses = io.open(dir_in+\"/senses_69419_debug.txt\",\"r\")\n",
    "#k_senses = io.open(dir_in+\"/mus_debug.dat\",\"r\")\n",
    "# k0 = mus4\n",
    "# k1 = mus3\n",
    "# k2 = mus2\n",
    "# k3 = mus1\n",
    "# k4 = nothing\n",
    "\n",
    "file_senses = s_senses.readlines()[1:]\n",
    "output_senses = k_senses.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- ~~create the notebook~~\n",
    "- ~~organise the notebook~~\n",
    "- ~~write \"general idea\" pseudocode for the evaluation~~\n",
    "- ~~get input files~~\n",
    "- ~~figure out data structures to store the variables~~\n",
    "- ~~write actual code~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: How well does the model identify the correct senses for the target word?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-151-82fa54601c9a>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-151-82fa54601c9a>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    for each k:\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# For each target word, we have a list of senses  s (given by the expert)\n",
    "# For each target word, we have a list of senses k (given by the model)\n",
    "# This Q consists in matching s and k, and doing so in a confident way --> confidence score\n",
    "\n",
    "for each k:\n",
    "    for each s:\n",
    "        create conf(k,s)\n",
    "\n",
    "# What is conf(k,s)?\n",
    "        conf(k,s) = (p1*match(w1,s)+p2*match(w1,s)+px(wx,s))/10 WHERE\n",
    "    \n",
    "            px = probability of word wx \n",
    "                \n",
    "                and\n",
    "            \n",
    "            match(wx,s) =   1/number_of_senses_assigned_to_wx if s_is_one_of_them \n",
    "            \n",
    "                    or \n",
    "                            0 if w_is_not_associated_to_s\n",
    "                \n",
    "# Once we have gone through all s for one k, we have to choose the best k for s. How? (TBD, cfr Valerio and Barbara)\n",
    "\n",
    "# Once all ks have been assigned to all ss (or NA), we can calculate a general confidence score for the model.\n",
    "# One easy way to do that: \n",
    "\n",
    "conf_score_model = number_of_non_NA/k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real code\n",
    "\n",
    "Steps:\n",
    "\n",
    "- extract all senses from the file\n",
    "- use those senses as keys for a dictionary, `dict_of_words`\n",
    "- fill the dictionary: for each key, we store a list of words pertaining to that sense\n",
    "- transform the lists as sets so as to remove duplicates within the same sense\n",
    "- create a dictionary with a word as a key and its weight as a value, depending on how many senses it appears\n",
    "- parse the model output and get the probability weights for each word\n",
    "- do not take into account the first line\n",
    "- take care of empty lines\n",
    "\n",
    "Todo:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of senses: 4\n",
      "mus-1 {'16430', '54812', '18937', '7944', '5521', '21812', '12641', '112472', '76335', '42890', '81343', '101716', '24003', '59708', '48867', '4670', '31562', '21431', '66746', 'nlsj47984', '23690', '3986', '34855', '50093', '38547', '104872', '23468', '95853', '74127', '105344', '94483', '71262', '5009', '21802', '68970', '12349', '113060', '75910', '19641', '403', '34603', '26197', 'nlsj80462', '108536', '11072', '107167', '25007', 'nlsj68228', '37851', '62205', '63814', '65565', '2224', '66173', '55997', '678', '17730', '75948', '92662', '84725', '80557', '21487', '79595', '5112', '52332', '42842', '97285', '37776', '24801', '27629', '31948', '53956', '106502', '74735', '80042', '17381', 'nlsj9526', '7498', '116059', '104355', '28360', '80664', '40545', '13427', '6174', '13039', 'nlsj98815', 'nlsj8979', '19268', '79223', '92406', '31491', '10056', '103972', '54113', '95074', '46176', '93449', '41502', '70105', '2219', '260', '15047', '26943', '42887', '51241', 'nlsj80958', '6449', '6684', '104311', '65983', '116244', '7561', '64956', '72610', '113560', '49331', 'nlsj4012', 'nlsj801', '66639', '65097', '65552', '45513', '41619', '4068', '63212', 'nlsj114103', '97147', '46123', '88464', '102989', '2767', '77928', '35708', '53942', '58478', '2749', '62383', '7159', '62303', '38732', '104860', '50616', '19788', '86352', '103975', '61925', '34366', '14050', '1083', '22502', '72627', 'nlsj78558', '8665', 'nlsj96345', '1377', 'nlsj60710', '5607', '41705', '83774', 'nlsj77405', '116470', 'nlsj106628', 'nlsj10580', '103221', '7779', '17007', '29624', '33241', '78716', '108882', '2671', '60402', '98241', '82959', '26114', '72928', '89807', '34848', '80327', '21830', '15618', '47735', '20195', '104289', '24261', '55499', '100774', '93022', '28569', '4778', '95654', '105550', 'nlsj5437', '29828', '97386', '26136', '49589', '93388', '72716', '76703', '7133', '70550', '61176', '102669', 'nlsj4784', '28355', '5131', '11583', 'nlsj57918', '27209', '60225', '82954', '114731', '8505', '74819', '16400', '83756', '85306', '23678', 'nlsj1499', '43977', '46966', '7612', '95258', '84094', '72202', '41357', '11305', '114615', '75954', '72287', 'nlsj32160', '23628', '436', '54399', '47665', 'nlsj40132', '3241', 'nlsj96033', '48704', '31964', '74631', '112347', '85953', '18166', '80239', '78746', 'nlsj69856', '27415', 'nlsj17074', '7832', '112816', '102847', '22209', '26499', '61040', 'nlsj61205', '98031', '70958', '110114', '20728', '83783', '103942', '41032', '26447', 'nlsj5904', '22036', '66777', '51647', '84422', '29962', '2845', '45996', '70482', '37488', '66262', '39313', '2731', '21901', '53695', '47617', '18128', 'nlsj105676', '2583', '104538', '33671', '70477', '95432', '19346', '51815', '52460', '116050', '53161', '69419', '51369', '116058', '15121', '99638', 'nlsj7783', '100524', '15893', '90329', '22100', '103637', '76431', '50451', '11058', '65975', '18334', '2340', '10933', '91085', '7182', '82665', '105070', '36165', '115193', '71673', 'nlsj4320', '65628', '66494', '112720', '108537', '28566', '19452', '85672', '103922', '49875', '63845', '25870', 'nlsj59923', '103012', '1692', '102869', '39847', '83869', '20179', '36390', '69714', '18271', '104639', '75263', '56991', '73707', '28002', '61291', '22997', '39190', '59588', '72275', '93536', '44888', '31709', '3464', '56874', '51376', '941', '68791', '7062', '13579', '1564', '71314', '51727', '20674', '47447', '8586', '113741', '113711', '11206', '26233', '42686', '94098', '84234', '26684', '48498', '2834', 'nlsj11198', '18331', '71308', '64914', '31556', '56406', 'nlsj40053', '4927', '101445', '65697', '110302', '61056', '46474', '94474', '48670', '95221', '40323', 'nlsj71743', '83718', '86305', '98723', '46195', '96089', '112467', '86219', '110655', '93812', '79947', '114587', 'nlsj48763', '49955', '33074', 'nlsj86496', '80555', '72321', '85807', '47917', '18706', '4335', '34372', '6325', '105816', '83665', '111895', 'nlsj43224', 'nlsj11066', '22882', '114847', '92162', '100965', '55898', 'nlsj106503', '15162', '69052', '80761', '91055', '75352', '110606', 'nlsj5738', '114842', '31996', '113823', '27005', '41918', '110484', '13317', '45656', '60308', '84534', '83113', '83434', '110089', '63352', '76765', '51300', '26032', '56810', '20945', '72273', '51819', '86307', '24226', '72268', '63827', '107959', '28036', 'nlsj4113', '59005', '39195', '115810', '3128', '93417', '49437', '57321', '48770', '47964', 'nlsj32167', '103871', '83928', '112934', 'nlsj100912', '46074', '70708', 'nlsj8671', '111207', '31161', '61177', '23242', '48341', '61855', '64586', '110639', '56003', 'nlsj10130', '103993', '32686', '50644', '54971', '3398', '33647', '43124', '65757', '49227', '8909', '41536', '13608', '71559', '67104', '77698', '95522', '69863', '33160', '3327', '23794', '104225', '19972', 'nlsj7583', '30911', '30700', '83310', '92077', '61265', '95523', '16051', '113379', '67762', '68641', '83760', '68174', '69036', '98234', '80107', '4906', '66294', '110598', '4493', '67250', '96979', '112769', '67619', '106974', '43206', '41633', '34476', '45917', '63713', '4536', '15571', '59124', 'nlsj75598', 'nlsj68688', '2764', '19282', '14362', '51259', '33494', '64757', '9757', '109403', '26207', '49506', '102381', '74602', '67815', '107905', '76564', '11970', '74571', '46646', '80043', 'nlsj9035', '31565', '103701', '75552', '110027', '95368', '63143', '47513', '46216', '27764', '23658', '109730', '40001', '14181', '19546', '54592', '83251', '42659', '41082', '25018', '70768', '52035', '37616', '49933', '21783', '51256', '35710', '94713', '4274', '3237', 'nlsj10876', '72357', '80011', '26048', 'nlsj105619', '116293', '38488', '113556', '41538', 'nlsj4579', '62528', '49886', '62816', '29883', '101853', '112943', '73972', '111416', '74523', '37726', '67485', '112833', '108780', '113251', '50679', '84248', '7712', '2061', '86429', 'nlsj33192', '84494', '79103', '47745', '12035', '64448', '80357', '51358', '29974', '42827', '43305', '103957', 'nlsj2729', '24856', '83253', '16052', '15763', '15858', '71118', '37711', '95819', '463', '12176', '76184', '35092', '16171', '11197', '35267', '19711', '84552', '21920', '96565', 'nlsj5118', '71065', '62093', '91944', '48479', '73128', '82758', '15380', '45980', '38743', '102000', '105176', '50073', '7529', '6384', '92010', '21335', '4845', '19536', '92171', '110456', 'nlsj2610', '112351', '116416', '12409', '91800', '94316', '13002', '104421', '73277', '59501', '99622', '17197', '24523', '53442', '40161', '115845', '42830', 'nlsj52509', '79261', '35570', '2873', '54946', '69252', '33770', '4378', '38966', '25402', '109687', '61856', 'nlsj8970', '25228', '43089', 'nlsj28819', '69469', '22739', '55532', '33956', '98312', '36790', '110284', '41481', '65089', '106114', '75316', '51849', '94097', '32657', '91516', '74126', '57057', '51373', '88716', '104690', '112169', '19123', '82756', '84263', '89366', '76530', '23283', '76910', '73064', '48095', '112070', '12973', '20621', '55137', '3800', '64316', '106064', '42581', '100906', '16609', '79697', '77699', 'nlsj12527', '31236', 'nlsj7856', '75306', '39125', '76157', '114816', '62204', '24444', '100693', '48291', '11229', '55139', '18788', '19891', '112559', '50824', '91351', '40156', '102474', '106566', '52571', '84150', '7177', '114972', '104655', '114706', '55149', '28592', '37274', '4548', '50252', 'nlsj1611', '55498', '59276', '61885', '67868', '21589', '5132', '4587', '90504', '46804', '15378', '37095', '89309', '17962', '67974', '12485', '22316', '57262', '85420', '84475', '55815', '7724', '61791', '47876', '114548', '24292', '114688', '88498', '63314', '12948', '3289', '73221', '85417', '13098', '43060', '45671', '37244', '34071', '98173', '45285', '103152', 'nlsj114757', '36571', '1984', 'nlsj6617', '93796', '83266', '51661', '84434', '52095'}\n",
      "\n",
      "\n",
      "\n",
      "mus-4 {'74378', '57457', '42071', '48436', '38472', '37262', '83665', '88855', '31562', '112307', '27092', '55391', '49629', '21651', '97605', '98584', '106222', '85665', '58142', '18790', '3630', '43616', '84534', '74752', '76765', '84421', '83834', '66761', 'nlsj40134', '70495', '95970', '13942', '50521', '6339', '114553', '4470', '60840', '70364', '99647', '70556', '50908', '61946', 'nlsj1147', '22425', '63873', '114753', '100964', 'nlsj112819', '77640', '106502', '53015', '74735', '19528', '111764', '24340', '104355', '96999', '48504', '26039', '108652', '2898', '56189', '6174', '35166', '110639', '94677', '13039', '57466', '36216', '91634', '96503', '70558', '85876', '8909', '105548', '39180', '28349', '35417', '115374', '101714', '19972', '621', '115212', '15609', '61128', '51241', '16051', '37405', '67762', '114387', '113560', '68641', '72610', '35244', '69532', '98234', '96560', '65552', '103085', '4493', '55497', '71585', '115102', '79069', '109729', '66342', '94365', '24184', '67352', '97606', '28588', '6616', 'nlsj99416', '34366', '75989', '33238', '90334', '65703', 'nlsj177', '46646', '87413', '13852', '34522', '39238', '83605', '14266', '55981', '75552', '97241', '71984', '22660', '116218', '33426', '34982', '65191', 'nlsj33345', '106263', '67555', '44660', '10223', '67009', '20836', '83468', '47735', '22210', '82451', '98869', '65966', '64703', '101034', '31609', '28569', '62528', '36289', '93388', '13315', '39543', '29883', '115748', '13985', '61176', '73672', '100495', '70521', '37726', '11583', '108780', '39016', '24946', '93341', '112703', '30317', '97963', '51358', '2392', '91583', '20830', '45153', '51785', '16804', '93379', 'nlsj86473', '85306', '67526', '37075', '76028', '104624', '72202', '26226', '114615', '93332', '36938', '90166', '113477', '48704', '74621', '112794', '1199', '71373', '97252', '71065', 'nlsj79694', '85953', '63772', '58972', '15786', '115887', '102000', '95961', '22209', '14269', '76513', '110114', '4845', '33969', '7608', '116416', '26034', '114339', '59516', '31607', '3364', '13376', '66295', '90347', '45245', '84034', '37961', '45996', 'nlsj82141', '93435', '35570', '60646', '42214', '33770', '100379', '106206', '18062', '55313', '61231', '89405', '19503', '61245', '50236', '63550', '69419', 'nlsj82490', '109733', '51369', '41570', '13715', '75005', '41271', '116408', '32657', '68996', '15893', '83732', '37720', '105458', '11058', '82800', '18334', '104690', '10933', '20885', '98417', '3325', '71404', '20621', '64316', '67507', '106676', '58429', '75595', '24289', '114653', '66494', '112720', '112367', 'nlsj35116', '26002', '83869', '92926', '101851', '66750', '102795', '36180', '6898', '56991', '104639', '62204', '48291', '83650', '96981', '61553', '13813', '43975', '112559', '33486', '85099', '52571', '4474', '83825', '76516', '82190', 'nlsj42558', '7793', '102676', 'nlsj61122', '109918', '96698', '39078', '15011', '90504', '58319', '111722', '84234', '114083', '86108', '66389', '60703', '116216', '63928', '55383', '57460', '96853', '24132', 'nlsj71743', '26046', '83174', '74463', '57474', '39299', '93812', '66755', '47442'}\n",
      "\n",
      "\n",
      "\n",
      "w {'114624', '35642', '92927', '11058', '293', 'nlsj5856', '98712', '2219', '260', '12625', '16132', '46178', '73064', '31562', '84410', '61016', '58444', '31537', '71492', '16400', '85306', '30790', '65552', '63845', '41357', '64985', '31236', '38714', '72287', '86112', 'nlsj32160', 'nlsj5105', '104374', '23903', '48549', '62204', '21920', '98541', '112350', '113060', '56607', '61708', '31333', '63772', 'nlsj4603', '96193', '11072', '19282', '1379', '12210', '96106', '39186', '1083', '90410', '104655', '70958', '71976', '28592', '4548', '1093', '71312', '24436', '8665', '98747', '26447', '1377', '34522', '116416', '35541', '103701', '75552', '104429', '104421', '46804', '65747', 'nlsj7765', '109757', '45996', '114753', '23658', '115213', '33631', 'nlsj56179', '43260', '71308', '4958', '76801', '27847', '61073', '42214', '24340', '23799', '53826', '86305', '67132', '28569', '69419', '5390', '46195', '79223', '44552', '114558', 'nlsj7011', '35581', '1984', '49589', '65295', '3398', '54356', '78297', '26886', '1989', '32657', '12252'}\n",
      "\n",
      "\n",
      "\n",
      "mus-2 {'110127', '95144', '74378', '26818', '95095', '20508', '75652', '76335', 'nlsj86871', '18762', '53254', '49444', 'nlsj33192', '53222', '47745', '68539', '214', '100673', '74675', '4670', 'nlsj2729', 'nlsj68042', '92475', '74165', '15763', '19623', '60425', 'nlsj112110', '17228', '104624', '62535', '49439', '106023', '93880', '54990', '80525', '37343', '41918', '62274', '104637', '6184', '49182', '94967', '42678', '21920', '56954', '93624', '19641', '7362', '82758', '45980', '49176', '18561', '101790', '63032', '60317', '65565', '82623', '20413', '16440', '95540', '4562', '1904', '21490', '56223', 'nlsj10103', '98955', '51737', '114381', '74610', '106267', '23861', '19789', '53286', '65544', '45996', '113881', '45182', '112733', 'nlsj32819', '4763', '103038', '67422', 'nlsj100389', '110060', '116273', 'nlsj61217', '106206', '15198', '58527', '91198', '4698', '53538', 'nlsj7772', '69751', '65934', '46452', '69419', '110639', '29134', '97266', '75992', '98114', '60878', '69220', '115638', '38744', '30564', '102930', '59626', '45526', '93842', 'nlsj3264', '103637', '74126', '91541', '83212', '101257', '5591', '114347', '29390', '20469', 'nlsj8425', 'nlsj5634', '84089', '101982', '19236', '76910', '104416', 'nlsj75531', '64642', '83458', '65983', 'nlsj4805', '58429', '103667', '67762', '64483', '45525', 'nlsj61519', '66494', '103648', 'nlsj12526', '59121', '68706', 'nlsj4249', '85672', 'nlsj114797', 'nlsj10202', 'nlsj3072', '65552', '49875', '20270', '62337', '60316', '89321', 'nlsj76740', '92553', '105557', '33457', '60113', '62258', '21168', '75539', '58271', '114437', '105691', '66342', '26328', '16986', 'nlsj29538', '35633', 'nlsj99416', '102474', '61185', '61622', '52571', '67360', '57356', '94164', '83209', '112512', '32263', '20674', '64983', '45757', '67364', '2586', 'nlsj6085', '74225', '75653', '80188', '22316', '16884', '90350', '22166', '103773', '86213', '57262', '34243', '68185', '64559', '58262', 'nlsj3887', '21205', '47735', '30569', 'nlsj36416', '70768', '22389', 'nlsj3191', '90303', '112811', '58133', '93840', 'nlsj114757', '68144', '93388', '92051', '112724', '94291', 'nlsj70384', '113163', '73672', '33822'}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expert_senses = list() # list where we store all sense ids provided by expert\n",
    "for s in file_senses: # 60 for testing purposes\n",
    "    s = s.split(\"\\t\")\n",
    "    sense = s[11] # The sense ID is after the 10th tab\n",
    "    expert_senses.append(sense)\n",
    "    \n",
    "#print(len(expert_senses),expert_senses,len(set(expert_senses)))\n",
    "\n",
    "expert_senses = list(set(expert_senses)) # we only keep the unique senses\n",
    "number_of_s = len(expert_senses)  # we create a variable that stores the number of unique senses\n",
    "print(\"Number of senses:\",number_of_s)\n",
    "\n",
    "# This dictionary has a sense as a key, and a list of words as a value. \n",
    "dict_of_words = dict()\n",
    "# This list stores all words\n",
    "list_of_all_words = list()\n",
    "# This dictionary stores all words as keys and their weight as value\n",
    "word_weight = dict()\n",
    "\n",
    "for i in range(0,number_of_s): # for each sense, we create a dictionary entry which has a list as value\n",
    "    dict_of_words[expert_senses[i]] = list()\n",
    "\n",
    "    for s in file_senses: # we go back in the file\n",
    "        s = s.split(\"\\t\") # splitting on tabs\n",
    "        \n",
    "        sentence_of_ids = s[8] # 8 is for IDs, 9 is for words\n",
    "        list_of_ids = sentence_of_ids.split(\" \")  # splitting on spaces\n",
    "        for word_id in list_of_ids:\n",
    "            if s[11] == expert_senses[i]:      # we store all words for one sense \n",
    "                dict_of_words[expert_senses[i]].append(word_id)\n",
    "            list_of_all_words.append(word_id) # we store all words, we'll iterate over that for scores\n",
    "        \n",
    "\n",
    "    # Here, we remove duplicates\n",
    "    #dict_of_words[expert_senses[i]].append(\"79223\") #testing\n",
    "    dict_of_words[expert_senses[i]] = set(dict_of_words[expert_senses[i]]) \n",
    "      \n",
    "    print(expert_senses[i],set(dict_of_words[expert_senses[i]]))\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every word in the list of words that we have\n",
    "# we count the number of senses it appears in\n",
    "# we use that number to divide its importance: 1 sense = 1 importance; 2 senses = 0.5 importance\n",
    "# this can be finetuned\n",
    "\n",
    "for word in list_of_all_words:\n",
    "    x = 0\n",
    "    for i in range(0,number_of_s):\n",
    "        if word in dict_of_words[expert_senses[i]]:\n",
    "            x += 1 \n",
    "        if x != 0:\n",
    "            word_weight[word] = 1/x\n",
    "        \n",
    "    \n",
    "    #print(word,word_weight[word])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parsing output.dat\n",
    "- split on \"===============  per time  ===============\" and keep first part\n",
    "- transform that into a list, then\n",
    "- get lines that start with \"p(w|s)\"\n",
    "- count those, k = that number\n",
    "- split the line on \":\", keep the second part\n",
    "- split the rest on \";\", it's [ID] = prob_from_this_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word id 28355 ; probability 0.088\n",
      "word id 69419 ; probability 0.069\n",
      "word id 57460 ; probability 0.056\n",
      "word id 114587 ; probability 0.042\n",
      "word id 42071 ; probability 0.041\n",
      "word id 35267 ; probability 0.035\n",
      "word id 51647 ; probability 0.035\n",
      "word id 64448 ; probability 0.023\n",
      "word id 45980 ; probability 0.018\n",
      "word id 53826 ; probability 0.017\n",
      "{'28355': 0.2075471698113207}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377, '57460': 0.1320754716981132}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377, '57460': 0.1320754716981132, '114587': 0.0990566037735849}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377, '57460': 0.1320754716981132, '114587': 0.0990566037735849, '42071': 0.09669811320754716}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377, '57460': 0.1320754716981132, '114587': 0.0990566037735849, '42071': 0.09669811320754716, '35267': 0.08254716981132075}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377, '57460': 0.1320754716981132, '114587': 0.0990566037735849, '42071': 0.09669811320754716, '35267': 0.08254716981132075, '51647': 0.08254716981132075}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377, '57460': 0.1320754716981132, '114587': 0.0990566037735849, '42071': 0.09669811320754716, '35267': 0.08254716981132075, '51647': 0.08254716981132075, '64448': 0.05424528301886792}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377, '57460': 0.1320754716981132, '114587': 0.0990566037735849, '42071': 0.09669811320754716, '35267': 0.08254716981132075, '51647': 0.08254716981132075, '64448': 0.05424528301886792, '45980': 0.042452830188679236}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377, '57460': 0.1320754716981132, '114587': 0.0990566037735849, '42071': 0.09669811320754716, '35267': 0.08254716981132075, '51647': 0.08254716981132075, '64448': 0.05424528301886792, '45980': 0.042452830188679236, '53826': 0.04009433962264151}\n",
      "{'28355': 0.2075471698113207, '69419': 0.16273584905660377, '57460': 0.1320754716981132, '114587': 0.0990566037735849, '42071': 0.09669811320754716, '35267': 0.08254716981132075, '51647': 0.08254716981132075, '64448': 0.05424528301886792, '45980': 0.042452830188679236, '53826': 0.04009433962264151}\n",
      "word id 79223 ; probability 0.063\n",
      "word id 92927 ; probability 0.057\n",
      "word id 46574 ; probability 0.038\n",
      "word id 67660 ; probability 0.038\n",
      "word id 103085 ; probability 0.036\n",
      "word id 86112 ; probability 0.030\n",
      "word id 101982 ; probability 0.029\n",
      "word id 75808 ; probability 0.026\n",
      "word id 68539 ; probability 0.024\n",
      "word id 54607 ; probability 0.024\n",
      "{'79223': 0.17260273972602735}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438, '46574': 0.10410958904109587}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438, '46574': 0.10410958904109587, '67660': 0.10410958904109587}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438, '46574': 0.10410958904109587, '67660': 0.10410958904109587, '103085': 0.09863013698630134}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438, '46574': 0.10410958904109587, '67660': 0.10410958904109587, '103085': 0.09863013698630134, '86112': 0.08219178082191778}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438, '46574': 0.10410958904109587, '67660': 0.10410958904109587, '103085': 0.09863013698630134, '86112': 0.08219178082191778, '101982': 0.07945205479452053}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438, '46574': 0.10410958904109587, '67660': 0.10410958904109587, '103085': 0.09863013698630134, '86112': 0.08219178082191778, '101982': 0.07945205479452053, '75808': 0.07123287671232874}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438, '46574': 0.10410958904109587, '67660': 0.10410958904109587, '103085': 0.09863013698630134, '86112': 0.08219178082191778, '101982': 0.07945205479452053, '75808': 0.07123287671232874, '68539': 0.06575342465753423}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438, '46574': 0.10410958904109587, '67660': 0.10410958904109587, '103085': 0.09863013698630134, '86112': 0.08219178082191778, '101982': 0.07945205479452053, '75808': 0.07123287671232874, '68539': 0.06575342465753423, '54607': 0.06575342465753423}\n",
      "{'79223': 0.17260273972602735, '92927': 0.1561643835616438, '46574': 0.10410958904109587, '67660': 0.10410958904109587, '103085': 0.09863013698630134, '86112': 0.08219178082191778, '101982': 0.07945205479452053, '75808': 0.07123287671232874, '68539': 0.06575342465753423, '54607': 0.06575342465753423}\n",
      "word id 62258 ; probability 0.080\n",
      "word id 64586 ; probability 0.052\n",
      "word id 58271 ; probability 0.048\n",
      "word id nlsj86871 ; probability 0.041\n",
      "word id 101851 ; probability 0.039\n",
      "word id nlsj183 ; probability 0.037\n",
      "word id 75653 ; probability 0.031\n",
      "word id nlsj59923 ; probability 0.030\n",
      "word id 70105 ; probability 0.026\n",
      "word id 82758 ; probability 0.024\n",
      "{'62258': 0.19607843137254902}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685, '58271': 0.11764705882352941}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685, '58271': 0.11764705882352941, 'nlsj86871': 0.10049019607843138}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685, '58271': 0.11764705882352941, 'nlsj86871': 0.10049019607843138, '101851': 0.09558823529411764}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685, '58271': 0.11764705882352941, 'nlsj86871': 0.10049019607843138, '101851': 0.09558823529411764, 'nlsj183': 0.0906862745098039}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685, '58271': 0.11764705882352941, 'nlsj86871': 0.10049019607843138, '101851': 0.09558823529411764, 'nlsj183': 0.0906862745098039, '75653': 0.07598039215686274}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685, '58271': 0.11764705882352941, 'nlsj86871': 0.10049019607843138, '101851': 0.09558823529411764, 'nlsj183': 0.0906862745098039, '75653': 0.07598039215686274, 'nlsj59923': 0.07352941176470587}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685, '58271': 0.11764705882352941, 'nlsj86871': 0.10049019607843138, '101851': 0.09558823529411764, 'nlsj183': 0.0906862745098039, '75653': 0.07598039215686274, 'nlsj59923': 0.07352941176470587, '70105': 0.06372549019607843}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685, '58271': 0.11764705882352941, 'nlsj86871': 0.10049019607843138, '101851': 0.09558823529411764, 'nlsj183': 0.0906862745098039, '75653': 0.07598039215686274, 'nlsj59923': 0.07352941176470587, '70105': 0.06372549019607843, '82758': 0.058823529411764705}\n",
      "{'62258': 0.19607843137254902, '64586': 0.12745098039215685, '58271': 0.11764705882352941, 'nlsj86871': 0.10049019607843138, '101851': 0.09558823529411764, 'nlsj183': 0.0906862745098039, '75653': 0.07598039215686274, 'nlsj59923': 0.07352941176470587, '70105': 0.06372549019607843, '82758': 0.058823529411764705}\n",
      "word id 102000 ; probability 0.035\n",
      "word id 70495 ; probability 0.035\n",
      "word id 7182 ; probability 0.029\n",
      "word id 70958 ; probability 0.024\n",
      "word id 49589 ; probability 0.021\n",
      "word id 83174 ; probability 0.021\n",
      "word id 106676 ; probability 0.019\n",
      "word id 22209 ; probability 0.018\n",
      "word id 93388 ; probability 0.017\n",
      "word id 16132 ; probability 0.016\n",
      "{'102000': 0.14893617021276598}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598, '7182': 0.12340425531914895}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598, '7182': 0.12340425531914895, '70958': 0.1021276595744681}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598, '7182': 0.12340425531914895, '70958': 0.1021276595744681, '49589': 0.08936170212765958}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598, '7182': 0.12340425531914895, '70958': 0.1021276595744681, '49589': 0.08936170212765958, '83174': 0.08936170212765958}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598, '7182': 0.12340425531914895, '70958': 0.1021276595744681, '49589': 0.08936170212765958, '83174': 0.08936170212765958, '106676': 0.08085106382978724}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598, '7182': 0.12340425531914895, '70958': 0.1021276595744681, '49589': 0.08936170212765958, '83174': 0.08936170212765958, '106676': 0.08085106382978724, '22209': 0.07659574468085106}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598, '7182': 0.12340425531914895, '70958': 0.1021276595744681, '49589': 0.08936170212765958, '83174': 0.08936170212765958, '106676': 0.08085106382978724, '22209': 0.07659574468085106, '93388': 0.0723404255319149}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598, '7182': 0.12340425531914895, '70958': 0.1021276595744681, '49589': 0.08936170212765958, '83174': 0.08936170212765958, '106676': 0.08085106382978724, '22209': 0.07659574468085106, '93388': 0.0723404255319149, '16132': 0.06808510638297872}\n",
      "{'102000': 0.14893617021276598, '70495': 0.14893617021276598, '7182': 0.12340425531914895, '70958': 0.1021276595744681, '49589': 0.08936170212765958, '83174': 0.08936170212765958, '106676': 0.08085106382978724, '22209': 0.07659574468085106, '93388': 0.0723404255319149, '16132': 0.06808510638297872}\n",
      "word id nlsj5634 ; probability 0.039\n",
      "word id 61925 ; probability 0.037\n",
      "word id 12620 ; probability 0.030\n",
      "word id 69419 ; probability 0.027\n",
      "word id 104421 ; probability 0.027\n",
      "word id 91085 ; probability 0.024\n",
      "word id 71308 ; probability 0.022\n",
      "word id 115748 ; probability 0.021\n",
      "word id 19641 ; probability 0.020\n",
      "word id 5390 ; probability 0.019\n",
      "{'nlsj5634': 0.14661654135338348}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228, '12620': 0.11278195488721805}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228, '12620': 0.11278195488721805, '69419': 0.10150375939849625}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228, '12620': 0.11278195488721805, '69419': 0.10150375939849625, '104421': 0.10150375939849625}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228, '12620': 0.11278195488721805, '69419': 0.10150375939849625, '104421': 0.10150375939849625, '91085': 0.09022556390977446}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228, '12620': 0.11278195488721805, '69419': 0.10150375939849625, '104421': 0.10150375939849625, '91085': 0.09022556390977446, '71308': 0.08270676691729324}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228, '12620': 0.11278195488721805, '69419': 0.10150375939849625, '104421': 0.10150375939849625, '91085': 0.09022556390977446, '71308': 0.08270676691729324, '115748': 0.07894736842105265}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228, '12620': 0.11278195488721805, '69419': 0.10150375939849625, '104421': 0.10150375939849625, '91085': 0.09022556390977446, '71308': 0.08270676691729324, '115748': 0.07894736842105265, '19641': 0.07518796992481204}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228, '12620': 0.11278195488721805, '69419': 0.10150375939849625, '104421': 0.10150375939849625, '91085': 0.09022556390977446, '71308': 0.08270676691729324, '115748': 0.07894736842105265, '19641': 0.07518796992481204, '5390': 0.07142857142857144}\n",
      "{'nlsj5634': 0.14661654135338348, '61925': 0.13909774436090228, '12620': 0.11278195488721805, '69419': 0.10150375939849625, '104421': 0.10150375939849625, '91085': 0.09022556390977446, '71308': 0.08270676691729324, '115748': 0.07894736842105265, '19641': 0.07518796992481204, '5390': 0.07142857142857144}\n"
     ]
    }
   ],
   "source": [
    "lines_output = output_senses.split(\"===============  per time  ===============\")[0].split(\"\\n\")\n",
    "\n",
    "number_of_the_k = 0\n",
    "\n",
    "k_words_with_prob = dict()\n",
    "\n",
    "for line in lines_output:\n",
    "    if line[:6] == \"p(w|s)\":\n",
    "        line = line.split(\":\")[1]\n",
    "        line = line.split(\";\")\n",
    "        #print(number_of_the_k,line)\n",
    "        dico_word_prob = dict()\n",
    "        temp_dict = dict()\n",
    "        k_words_with_prob[number_of_the_k] = list()\n",
    "        \n",
    "        line = line[:-1] # last item of the list is empty\n",
    "        \n",
    "        total_probability = 0 # to have relative probs\n",
    "        for word_prob in line:\n",
    "            \n",
    "\n",
    "        \n",
    "            #word_prob = word_prob.split(\",\")\n",
    "            #for word in word_prob:\n",
    "            probability = re.findall(\"([\\d.\\w]*)\",word_prob)\n",
    "            if probability:\n",
    "                probability = list(filter(None,probability))\n",
    "                    \n",
    "            total_probability += float(probability[1])\n",
    "            print(\"word id\",probability[0],\"; probability\",probability[1])\n",
    "        \n",
    "            dico_word_prob[probability[0]] = float(probability[1])\n",
    "        #print(type(k_words_with_prob[number_of_the_k]))\n",
    "        \n",
    "        for i in dico_word_prob.keys():\n",
    "            \n",
    "            temp_dict[i] = float(dico_word_prob[i]/total_probability)\n",
    "            k_words_with_prob[number_of_the_k] = temp_dict\n",
    "            \n",
    "            print(k_words_with_prob[number_of_the_k])\n",
    "            \n",
    "        #k_words_with_prob[number_of_the_k] = [float(dico_word_prob[i]/total_probability) for i in dico_word_prob]\n",
    "        #print(k_words_with_prob[number_of_the_k])\n",
    "        print(temp_dict)\n",
    "        number_of_the_k += 1\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k_words_with_prob\n",
    "This dictionary has the sense number 'k' as keys and the a dictionary of [word] = probability as values.\n",
    "Example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "#print(\"Probability for word ID 5390 in sense k = 4:\",k_words_with_prob[4][\"5390\"])\n",
    "print(type(k_words_with_prob[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output sense 0\n",
      "\texpert sense number  0 mus-1\n",
      "\t\tword from annotation for sense 0 : 28355\n",
      "\t\t\tword  28355 is in output for sense 0 with probability: 0.2075471698113207 and weight: 1.0\n",
      "\t\tword from annotation for sense 0 : 69419\n",
      "\t\t\tword  69419 is in output for sense 0 with probability: 0.16273584905660377 and weight: 0.25\n",
      "\t\tword from annotation for sense 0 : 57460\n",
      "\t\tword from annotation for sense 0 : 114587\n",
      "\t\t\tword  114587 is in output for sense 0 with probability: 0.0990566037735849 and weight: 1.0\n",
      "\t\tword from annotation for sense 0 : 42071\n",
      "\t\tword from annotation for sense 0 : 35267\n",
      "\t\t\tword  35267 is in output for sense 0 with probability: 0.08254716981132075 and weight: 1.0\n",
      "\t\tword from annotation for sense 0 : 51647\n",
      "\t\t\tword  51647 is in output for sense 0 with probability: 0.08254716981132075 and weight: 1.0\n",
      "\t\tword from annotation for sense 0 : 64448\n",
      "\t\t\tword  64448 is in output for sense 0 with probability: 0.05424528301886792 and weight: 1.0\n",
      "\t\tword from annotation for sense 0 : 45980\n",
      "\t\t\tword  45980 is in output for sense 0 with probability: 0.042452830188679236 and weight: 0.5\n",
      "\t\tword from annotation for sense 0 : 53826\n",
      "\texpert sense number  1 mus-4\n",
      "\t\tword from annotation for sense 0 : 28355\n",
      "\t\tword from annotation for sense 0 : 69419\n",
      "\t\t\tword  69419 is in output for sense 0 with probability: 0.16273584905660377 and weight: 0.25\n",
      "\t\tword from annotation for sense 0 : 57460\n",
      "\t\t\tword  57460 is in output for sense 0 with probability: 0.1320754716981132 and weight: 1.0\n",
      "\t\tword from annotation for sense 0 : 114587\n",
      "\t\tword from annotation for sense 0 : 42071\n",
      "\t\t\tword  42071 is in output for sense 0 with probability: 0.09669811320754716 and weight: 1.0\n",
      "\t\tword from annotation for sense 0 : 35267\n",
      "\t\tword from annotation for sense 0 : 51647\n",
      "\t\tword from annotation for sense 0 : 64448\n",
      "\t\tword from annotation for sense 0 : 45980\n",
      "\t\tword from annotation for sense 0 : 53826\n",
      "\texpert sense number  2 w\n",
      "\t\tword from annotation for sense 0 : 28355\n",
      "\t\tword from annotation for sense 0 : 69419\n",
      "\t\t\tword  69419 is in output for sense 0 with probability: 0.16273584905660377 and weight: 0.25\n",
      "\t\tword from annotation for sense 0 : 57460\n",
      "\t\tword from annotation for sense 0 : 114587\n",
      "\t\tword from annotation for sense 0 : 42071\n",
      "\t\tword from annotation for sense 0 : 35267\n",
      "\t\tword from annotation for sense 0 : 51647\n",
      "\t\tword from annotation for sense 0 : 64448\n",
      "\t\tword from annotation for sense 0 : 45980\n",
      "\t\tword from annotation for sense 0 : 53826\n",
      "\t\t\tword  53826 is in output for sense 0 with probability: 0.04009433962264151 and weight: 1.0\n",
      "\texpert sense number  3 mus-2\n",
      "\t\tword from annotation for sense 0 : 28355\n",
      "\t\tword from annotation for sense 0 : 69419\n",
      "\t\t\tword  69419 is in output for sense 0 with probability: 0.16273584905660377 and weight: 0.25\n",
      "\t\tword from annotation for sense 0 : 57460\n",
      "\t\tword from annotation for sense 0 : 114587\n",
      "\t\tword from annotation for sense 0 : 42071\n",
      "\t\tword from annotation for sense 0 : 35267\n",
      "\t\tword from annotation for sense 0 : 51647\n",
      "\t\tword from annotation for sense 0 : 64448\n",
      "\t\tword from annotation for sense 0 : 45980\n",
      "\t\t\tword  45980 is in output for sense 0 with probability: 0.042452830188679236 and weight: 0.5\n",
      "\t\tword from annotation for sense 0 : 53826\n",
      "output sense 1\n",
      "\texpert sense number  0 mus-1\n",
      "\t\tword from annotation for sense 1 : 79223\n",
      "\t\t\tword  79223 is in output for sense 1 with probability: 0.17260273972602735 and weight: 0.5\n",
      "\t\tword from annotation for sense 1 : 92927\n",
      "\t\tword from annotation for sense 1 : 46574\n",
      "\t\tword from annotation for sense 1 : 67660\n",
      "\t\tword from annotation for sense 1 : 103085\n",
      "\t\tword from annotation for sense 1 : 86112\n",
      "\t\tword from annotation for sense 1 : 101982\n",
      "\t\tword from annotation for sense 1 : 75808\n",
      "\t\tword from annotation for sense 1 : 68539\n",
      "\t\tword from annotation for sense 1 : 54607\n",
      "\texpert sense number  1 mus-4\n",
      "\t\tword from annotation for sense 1 : 79223\n",
      "\t\tword from annotation for sense 1 : 92927\n",
      "\t\tword from annotation for sense 1 : 46574\n",
      "\t\tword from annotation for sense 1 : 67660\n",
      "\t\tword from annotation for sense 1 : 103085\n",
      "\t\t\tword  103085 is in output for sense 1 with probability: 0.09863013698630134 and weight: 1.0\n",
      "\t\tword from annotation for sense 1 : 86112\n",
      "\t\tword from annotation for sense 1 : 101982\n",
      "\t\tword from annotation for sense 1 : 75808\n",
      "\t\tword from annotation for sense 1 : 68539\n",
      "\t\tword from annotation for sense 1 : 54607\n",
      "\texpert sense number  2 w\n",
      "\t\tword from annotation for sense 1 : 79223\n",
      "\t\t\tword  79223 is in output for sense 1 with probability: 0.17260273972602735 and weight: 0.5\n",
      "\t\tword from annotation for sense 1 : 92927\n",
      "\t\t\tword  92927 is in output for sense 1 with probability: 0.1561643835616438 and weight: 1.0\n",
      "\t\tword from annotation for sense 1 : 46574\n",
      "\t\tword from annotation for sense 1 : 67660\n",
      "\t\tword from annotation for sense 1 : 103085\n",
      "\t\tword from annotation for sense 1 : 86112\n",
      "\t\t\tword  86112 is in output for sense 1 with probability: 0.08219178082191778 and weight: 1.0\n",
      "\t\tword from annotation for sense 1 : 101982\n",
      "\t\tword from annotation for sense 1 : 75808\n",
      "\t\tword from annotation for sense 1 : 68539\n",
      "\t\tword from annotation for sense 1 : 54607\n",
      "\texpert sense number  3 mus-2\n",
      "\t\tword from annotation for sense 1 : 79223\n",
      "\t\tword from annotation for sense 1 : 92927\n",
      "\t\tword from annotation for sense 1 : 46574\n",
      "\t\tword from annotation for sense 1 : 67660\n",
      "\t\tword from annotation for sense 1 : 103085\n",
      "\t\tword from annotation for sense 1 : 86112\n",
      "\t\tword from annotation for sense 1 : 101982\n",
      "\t\t\tword  101982 is in output for sense 1 with probability: 0.07945205479452053 and weight: 1.0\n",
      "\t\tword from annotation for sense 1 : 75808\n",
      "\t\tword from annotation for sense 1 : 68539\n",
      "\t\t\tword  68539 is in output for sense 1 with probability: 0.06575342465753423 and weight: 1.0\n",
      "\t\tword from annotation for sense 1 : 54607\n",
      "output sense 2\n",
      "\texpert sense number  0 mus-1\n",
      "\t\tword from annotation for sense 2 : 62258\n",
      "\t\tword from annotation for sense 2 : 64586\n",
      "\t\t\tword  64586 is in output for sense 2 with probability: 0.12745098039215685 and weight: 1.0\n",
      "\t\tword from annotation for sense 2 : 58271\n",
      "\t\tword from annotation for sense 2 : nlsj86871\n",
      "\t\tword from annotation for sense 2 : 101851\n",
      "\t\tword from annotation for sense 2 : nlsj183\n",
      "\t\tword from annotation for sense 2 : 75653\n",
      "\t\tword from annotation for sense 2 : nlsj59923\n",
      "\t\t\tword  nlsj59923 is in output for sense 2 with probability: 0.07352941176470587 and weight: 1.0\n",
      "\t\tword from annotation for sense 2 : 70105\n",
      "\t\t\tword  70105 is in output for sense 2 with probability: 0.06372549019607843 and weight: 1.0\n",
      "\t\tword from annotation for sense 2 : 82758\n",
      "\t\t\tword  82758 is in output for sense 2 with probability: 0.058823529411764705 and weight: 0.5\n",
      "\texpert sense number  1 mus-4\n",
      "\t\tword from annotation for sense 2 : 62258\n",
      "\t\tword from annotation for sense 2 : 64586\n",
      "\t\tword from annotation for sense 2 : 58271\n",
      "\t\tword from annotation for sense 2 : nlsj86871\n",
      "\t\tword from annotation for sense 2 : 101851\n",
      "\t\t\tword  101851 is in output for sense 2 with probability: 0.09558823529411764 and weight: 1.0\n",
      "\t\tword from annotation for sense 2 : nlsj183\n",
      "\t\tword from annotation for sense 2 : 75653\n",
      "\t\tword from annotation for sense 2 : nlsj59923\n",
      "\t\tword from annotation for sense 2 : 70105\n",
      "\t\tword from annotation for sense 2 : 82758\n",
      "\texpert sense number  2 w\n",
      "\t\tword from annotation for sense 2 : 62258\n",
      "\t\tword from annotation for sense 2 : 64586\n",
      "\t\tword from annotation for sense 2 : 58271\n",
      "\t\tword from annotation for sense 2 : nlsj86871\n",
      "\t\tword from annotation for sense 2 : 101851\n",
      "\t\tword from annotation for sense 2 : nlsj183\n",
      "\t\tword from annotation for sense 2 : 75653\n",
      "\t\tword from annotation for sense 2 : nlsj59923\n",
      "\t\tword from annotation for sense 2 : 70105\n",
      "\t\tword from annotation for sense 2 : 82758\n",
      "\texpert sense number  3 mus-2\n",
      "\t\tword from annotation for sense 2 : 62258\n",
      "\t\t\tword  62258 is in output for sense 2 with probability: 0.19607843137254902 and weight: 1.0\n",
      "\t\tword from annotation for sense 2 : 64586\n",
      "\t\tword from annotation for sense 2 : 58271\n",
      "\t\t\tword  58271 is in output for sense 2 with probability: 0.11764705882352941 and weight: 1.0\n",
      "\t\tword from annotation for sense 2 : nlsj86871\n",
      "\t\t\tword  nlsj86871 is in output for sense 2 with probability: 0.10049019607843138 and weight: 1.0\n",
      "\t\tword from annotation for sense 2 : 101851\n",
      "\t\tword from annotation for sense 2 : nlsj183\n",
      "\t\tword from annotation for sense 2 : 75653\n",
      "\t\t\tword  75653 is in output for sense 2 with probability: 0.07598039215686274 and weight: 1.0\n",
      "\t\tword from annotation for sense 2 : nlsj59923\n",
      "\t\tword from annotation for sense 2 : 70105\n",
      "\t\tword from annotation for sense 2 : 82758\n",
      "\t\t\tword  82758 is in output for sense 2 with probability: 0.058823529411764705 and weight: 0.5\n",
      "output sense 3\n",
      "\texpert sense number  0 mus-1\n",
      "\t\tword from annotation for sense 3 : 102000\n",
      "\t\t\tword  102000 is in output for sense 3 with probability: 0.14893617021276598 and weight: 0.5\n",
      "\t\tword from annotation for sense 3 : 70495\n",
      "\t\tword from annotation for sense 3 : 7182\n",
      "\t\t\tword  7182 is in output for sense 3 with probability: 0.12340425531914895 and weight: 1.0\n",
      "\t\tword from annotation for sense 3 : 70958\n",
      "\t\t\tword  70958 is in output for sense 3 with probability: 0.1021276595744681 and weight: 0.5\n",
      "\t\tword from annotation for sense 3 : 49589\n",
      "\t\t\tword  49589 is in output for sense 3 with probability: 0.08936170212765958 and weight: 0.5\n",
      "\t\tword from annotation for sense 3 : 83174\n",
      "\t\tword from annotation for sense 3 : 106676\n",
      "\t\tword from annotation for sense 3 : 22209\n",
      "\t\t\tword  22209 is in output for sense 3 with probability: 0.07659574468085106 and weight: 0.5\n",
      "\t\tword from annotation for sense 3 : 93388\n",
      "\t\t\tword  93388 is in output for sense 3 with probability: 0.0723404255319149 and weight: 0.3333333333333333\n",
      "\t\tword from annotation for sense 3 : 16132\n",
      "\texpert sense number  1 mus-4\n",
      "\t\tword from annotation for sense 3 : 102000\n",
      "\t\t\tword  102000 is in output for sense 3 with probability: 0.14893617021276598 and weight: 0.5\n",
      "\t\tword from annotation for sense 3 : 70495\n",
      "\t\t\tword  70495 is in output for sense 3 with probability: 0.14893617021276598 and weight: 1.0\n",
      "\t\tword from annotation for sense 3 : 7182\n",
      "\t\tword from annotation for sense 3 : 70958\n",
      "\t\tword from annotation for sense 3 : 49589\n",
      "\t\tword from annotation for sense 3 : 83174\n",
      "\t\t\tword  83174 is in output for sense 3 with probability: 0.08936170212765958 and weight: 1.0\n",
      "\t\tword from annotation for sense 3 : 106676\n",
      "\t\t\tword  106676 is in output for sense 3 with probability: 0.08085106382978724 and weight: 1.0\n",
      "\t\tword from annotation for sense 3 : 22209\n",
      "\t\t\tword  22209 is in output for sense 3 with probability: 0.07659574468085106 and weight: 0.5\n",
      "\t\tword from annotation for sense 3 : 93388\n",
      "\t\t\tword  93388 is in output for sense 3 with probability: 0.0723404255319149 and weight: 0.3333333333333333\n",
      "\t\tword from annotation for sense 3 : 16132\n",
      "\texpert sense number  2 w\n",
      "\t\tword from annotation for sense 3 : 102000\n",
      "\t\tword from annotation for sense 3 : 70495\n",
      "\t\tword from annotation for sense 3 : 7182\n",
      "\t\tword from annotation for sense 3 : 70958\n",
      "\t\t\tword  70958 is in output for sense 3 with probability: 0.1021276595744681 and weight: 0.5\n",
      "\t\tword from annotation for sense 3 : 49589\n",
      "\t\t\tword  49589 is in output for sense 3 with probability: 0.08936170212765958 and weight: 0.5\n",
      "\t\tword from annotation for sense 3 : 83174\n",
      "\t\tword from annotation for sense 3 : 106676\n",
      "\t\tword from annotation for sense 3 : 22209\n",
      "\t\tword from annotation for sense 3 : 93388\n",
      "\t\tword from annotation for sense 3 : 16132\n",
      "\t\t\tword  16132 is in output for sense 3 with probability: 0.06808510638297872 and weight: 1.0\n",
      "\texpert sense number  3 mus-2\n",
      "\t\tword from annotation for sense 3 : 102000\n",
      "\t\tword from annotation for sense 3 : 70495\n",
      "\t\tword from annotation for sense 3 : 7182\n",
      "\t\tword from annotation for sense 3 : 70958\n",
      "\t\tword from annotation for sense 3 : 49589\n",
      "\t\tword from annotation for sense 3 : 83174\n",
      "\t\tword from annotation for sense 3 : 106676\n",
      "\t\tword from annotation for sense 3 : 22209\n",
      "\t\tword from annotation for sense 3 : 93388\n",
      "\t\t\tword  93388 is in output for sense 3 with probability: 0.0723404255319149 and weight: 0.3333333333333333\n",
      "\t\tword from annotation for sense 3 : 16132\n",
      "output sense 4\n",
      "\texpert sense number  0 mus-1\n",
      "\t\tword from annotation for sense 4 : nlsj5634\n",
      "\t\tword from annotation for sense 4 : 61925\n",
      "\t\t\tword  61925 is in output for sense 4 with probability: 0.13909774436090228 and weight: 1.0\n",
      "\t\tword from annotation for sense 4 : 12620\n",
      "\t\tword from annotation for sense 4 : 69419\n",
      "\t\t\tword  69419 is in output for sense 4 with probability: 0.10150375939849625 and weight: 0.25\n",
      "\t\tword from annotation for sense 4 : 104421\n",
      "\t\t\tword  104421 is in output for sense 4 with probability: 0.10150375939849625 and weight: 0.5\n",
      "\t\tword from annotation for sense 4 : 91085\n",
      "\t\t\tword  91085 is in output for sense 4 with probability: 0.09022556390977446 and weight: 1.0\n",
      "\t\tword from annotation for sense 4 : 71308\n",
      "\t\t\tword  71308 is in output for sense 4 with probability: 0.08270676691729324 and weight: 0.5\n",
      "\t\tword from annotation for sense 4 : 115748\n",
      "\t\tword from annotation for sense 4 : 19641\n",
      "\t\t\tword  19641 is in output for sense 4 with probability: 0.07518796992481204 and weight: 0.5\n",
      "\t\tword from annotation for sense 4 : 5390\n",
      "\texpert sense number  1 mus-4\n",
      "\t\tword from annotation for sense 4 : nlsj5634\n",
      "\t\tword from annotation for sense 4 : 61925\n",
      "\t\tword from annotation for sense 4 : 12620\n",
      "\t\tword from annotation for sense 4 : 69419\n",
      "\t\t\tword  69419 is in output for sense 4 with probability: 0.10150375939849625 and weight: 0.25\n",
      "\t\tword from annotation for sense 4 : 104421\n",
      "\t\tword from annotation for sense 4 : 91085\n",
      "\t\tword from annotation for sense 4 : 71308\n",
      "\t\tword from annotation for sense 4 : 115748\n",
      "\t\t\tword  115748 is in output for sense 4 with probability: 0.07894736842105265 and weight: 1.0\n",
      "\t\tword from annotation for sense 4 : 19641\n",
      "\t\tword from annotation for sense 4 : 5390\n",
      "\texpert sense number  2 w\n",
      "\t\tword from annotation for sense 4 : nlsj5634\n",
      "\t\tword from annotation for sense 4 : 61925\n",
      "\t\tword from annotation for sense 4 : 12620\n",
      "\t\tword from annotation for sense 4 : 69419\n",
      "\t\t\tword  69419 is in output for sense 4 with probability: 0.10150375939849625 and weight: 0.25\n",
      "\t\tword from annotation for sense 4 : 104421\n",
      "\t\t\tword  104421 is in output for sense 4 with probability: 0.10150375939849625 and weight: 0.5\n",
      "\t\tword from annotation for sense 4 : 91085\n",
      "\t\tword from annotation for sense 4 : 71308\n",
      "\t\t\tword  71308 is in output for sense 4 with probability: 0.08270676691729324 and weight: 0.5\n",
      "\t\tword from annotation for sense 4 : 115748\n",
      "\t\tword from annotation for sense 4 : 19641\n",
      "\t\tword from annotation for sense 4 : 5390\n",
      "\t\t\tword  5390 is in output for sense 4 with probability: 0.07142857142857144 and weight: 1.0\n",
      "\texpert sense number  3 mus-2\n",
      "\t\tword from annotation for sense 4 : nlsj5634\n",
      "\t\t\tword  nlsj5634 is in output for sense 4 with probability: 0.14661654135338348 and weight: 1.0\n",
      "\t\tword from annotation for sense 4 : 61925\n",
      "\t\tword from annotation for sense 4 : 12620\n",
      "\t\tword from annotation for sense 4 : 69419\n",
      "\t\t\tword  69419 is in output for sense 4 with probability: 0.10150375939849625 and weight: 0.25\n",
      "\t\tword from annotation for sense 4 : 104421\n",
      "\t\tword from annotation for sense 4 : 91085\n",
      "\t\tword from annotation for sense 4 : 71308\n",
      "\t\tword from annotation for sense 4 : 115748\n",
      "\t\tword from annotation for sense 4 : 19641\n",
      "\t\t\tword  19641 is in output for sense 4 with probability: 0.07518796992481204 and weight: 0.5\n",
      "\t\tword from annotation for sense 4 : 5390\n"
     ]
    }
   ],
   "source": [
    "for key in k_words_with_prob.keys():\n",
    "    print(\"output sense\",key)\n",
    "    for i in range(0,number_of_s):\n",
    "        print(\"\\texpert sense number \", i, expert_senses[i])\n",
    "        for second_key in k_words_with_prob[key].keys(): # Barbara's note: shouldn't it be k_words_with_prob[i] here?\n",
    "            print(\"\\t\\tword from annotation for sense\", key, \":\", second_key)\n",
    "            if second_key in dict_of_words[expert_senses[i]]:\n",
    "                print(\"\\t\\t\\tword \", second_key, \"is in output for sense\", key, \"with probability:\", k_words_with_prob[key][second_key], \"and weight:\", word_weight[second_key])\n",
    "\n",
    "                \n",
    "# Here we get all the senses and for each sense we do a matching between the k words and s words and get the probability\n",
    "# For some reason the first word for each sense arrives several times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of expert senses s: 4\n",
      "number of model output senses k: 5\n",
      "\n",
      "\n",
      "Choose best match for k = 0\n",
      "k = 0 \t s = 0 (= expert sense mus-1 )\t conf[k,s] = 0.8360259433962197\n",
      "k = 0 \t s = 1 (= expert sense mus-4 )\t conf[k,s] = 0.8100825471698055\n",
      "k = 0 \t s = 2 (= expert sense w )\t conf[k,s] = 0.7647995283018816\n",
      "k = 0 \t s = 3 (= expert sense mus-2 )\t conf[k,s] = 0.7671580188679193\n",
      "\n",
      "\n",
      "Choose best match for k = 1\n",
      "k = 1 \t s = 0 (= expert sense mus-1 )\t conf[k,s] = 0.03452054794520548\n",
      "k = 1 \t s = 1 (= expert sense mus-4 )\t conf[k,s] = 0.019726027397260266\n",
      "k = 1 \t s = 2 (= expert sense w )\t conf[k,s] = 0.05835616438356163\n",
      "k = 1 \t s = 3 (= expert sense mus-2 )\t conf[k,s] = 0.06219178082191781\n",
      "\n",
      "\n",
      "Choose best match for k = 2\n",
      "k = 2 \t s = 0 (= expert sense mus-1 )\t conf[k,s] = 0.045098039215686246\n",
      "k = 2 \t s = 1 (= expert sense mus-4 )\t conf[k,s] = 0.04779411764705884\n",
      "k = 2 \t s = 2 (= expert sense w )\t conf[k,s] = 0.0\n",
      "k = 2 \t s = 3 (= expert sense mus-2 )\t conf[k,s] = 0.2012254901960786\n",
      "\n",
      "\n",
      "Choose best match for k = 3\n",
      "k = 3 \t s = 0 (= expert sense mus-1 )\t conf[k,s] = 0.19390070921985875\n",
      "k = 3 \t s = 1 (= expert sense mus-4 )\t conf[k,s] = 0.35539007092198605\n",
      "k = 3 \t s = 2 (= expert sense w )\t conf[k,s] = 0.0457446808510638\n",
      "k = 3 \t s = 3 (= expert sense mus-2 )\t conf[k,s] = 0.009645390070921984\n",
      "\n",
      "\n",
      "Choose best match for k = 4\n",
      "k = 4 \t s = 0 (= expert sense mus-1 )\t conf[k,s] = 0.587312030075191\n",
      "k = 4 \t s = 1 (= expert sense mus-4 )\t conf[k,s] = 0.4982142857142889\n",
      "k = 4 \t s = 2 (= expert sense w )\t conf[k,s] = 0.5452067669172961\n",
      "k = 4 \t s = 3 (= expert sense mus-2 )\t conf[k,s] = 0.5155075187969962\n"
     ]
    }
   ],
   "source": [
    "## Calculating confidence score for each (words_of_k,words_of_s) pair\n",
    "\n",
    "# conf(k,s) = (p1*match(w1,s)+p2*match(w1,s)+px(wx,s))/10\n",
    "        # match(wx,s) =   1/number_of_senses_assigned_to_wx if s_is_one_of_them \n",
    "\n",
    "##### TODO: for now conf[k,s] is multiplied by the number of expert senses --- FIX\n",
    "    \n",
    "print(\"number of expert senses s:\",number_of_s)\n",
    "print(\"number of model output senses k:\",len(k_words_with_prob.keys()))\n",
    "compteur = 0\n",
    "\n",
    "match = dict()\n",
    "conf = dict()\n",
    "for k in k_words_with_prob.keys():  # for each output sense, we go through...\n",
    "    print(\"\\n\")\n",
    "    print(\"Choose best match for k =\",k)\n",
    "    for s in range(0,number_of_s):       # each expert sense\n",
    "        \n",
    "        conf[k,s] = 0 \n",
    "        \n",
    "        #print(\"expert sense\",s)\n",
    "        for mot in k_words_with_prob[k]:      # for each word within output by the model for the output sense\n",
    "            #print(k,mot)\n",
    "            #print(expert_senses[s])\n",
    "            \n",
    "            if mot in dict_of_words[expert_senses[s]]:  # if that word exists in the list of expert words for that sense\n",
    "                \n",
    "                #print(s,dict_of_words[expert_senses[s]])\n",
    "                #print(k_words_with_prob[k][mot])\n",
    "                \n",
    "                for word in list_of_all_words:  # this help getting a key for a dictionary later on\n",
    "                    if mot == word:\n",
    "                        match_weighted = float((k_words_with_prob[k][mot]))*word_weight[word] #this dictionary cfr comment on line 24\n",
    "                        # word_weight[word] is already \"1/number_of_expert_senses_assigned_to_this_word\"\n",
    "                        \n",
    "                        #print(\"sense\",expert_senses[s],\"word\",word,\"match_weighted\",match_weighted)\n",
    "                        \n",
    "                        #print(k,s,conf[k,s])\n",
    "\n",
    "                        \n",
    "                        # To fix? \n",
    "                        # The way the code works is that all matches happen number_of_s times\n",
    "                        # (number_of_s = number of expert senses)\n",
    "                        # easy fix is to divide the match score by number_of_s\n",
    "                        \n",
    "                        conf[k,s] = conf[k,s] + match_weighted/4\n",
    "                        \n",
    "                    #else: \n",
    "                        #print(word,\"has no match for sense\",expert_senses[s])\n",
    "                        #print(word,word_weight[word],\"match\",k_words_with_prob[k][mot],\"match weighted\",match_weighted)\n",
    "                    #print(\"test1\")\n",
    "                #print(\"test2\")\n",
    "                \n",
    "                    #compteur += 1\n",
    "                \n",
    "        if (k,s) in conf.keys():\n",
    "        \n",
    "            conf[k,s] = conf[k,s]/10  # 10 is hardcoded, that's the number of words the model outputs\n",
    "            print(\"k =\",k,\"\\t s =\",s,\"(= expert sense\",expert_senses[s],\")\\t conf[k,s] =\",conf[k,s])\n",
    "            \n",
    "            #print(compteur)\n",
    "            \n",
    "    #print(k_words_with_prob[k])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Barbara's note: \"key\" and \"i\" may be different senses; for example, for key=0, this is the first sense in the output, and i=0 is the first sense annotated by the expert. I think what we want here is to try matching all key values with all i values; for each (key, i) pair, we get the conf(key, i), as in pages 5 ff. of the Goals and plan.docx document.\n",
    "\n",
    "Then, when possible, we can pick the best \"i\" for each \"key\"; we haven't yet decided how, but it will probably have to do with the maximum conf value.\n",
    "\n",
    "Once we have a key-->i mapping, we can calculate precision and recall.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: How well does the model assign the right words to a given sense of the target word?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-158-f315d04cacf9>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-158-f315d04cacf9>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    for each k:\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# For each k, we use the words given by the expert as unquestionable truth.\n",
    "# Judging the model's assignment of words to a given sense becomes a question of precision and recall.\n",
    "\n",
    "# precision is all correct w weighted by their respective probabilities / all w weighted by their probabilities\n",
    "\n",
    "for each k:\n",
    "    for each w:\n",
    "        if w in expert_list:\n",
    "            w_weight = p*1\n",
    "            numerator += w_weight\n",
    "        w_weight = p*1\n",
    "        denominator += w_weight\n",
    "    precision = numerator/denominator\n",
    "    \n",
    "# recall is all correct w weighted by their respective probabilities / all w assigned to the sense by the expert\n",
    "for each k:\n",
    "    for each w:\n",
    "        if w in expert_list:\n",
    "            w_weight = p*1\n",
    "            numerator += w_weight\n",
    "    denominator = len(expert_list)\n",
    "    recall = numerator/denominator\n",
    "    \n",
    "# f-score can be used as well\n",
    "\n",
    "for each k:\n",
    "    f_score = 2 * precision * recall / (precision+recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For pair ks 0,0 :\n",
      "The RECALL is 0.31000000000000005 / 836 = 0.0003708133971291867\n",
      "The PRECISION is 0.31000000000000005 / 0.43900000000000006 = 0.7061503416856493 \n",
      "\n",
      "The F-SCORE is 0.0007412375558767587 \n",
      "\n",
      "For pair ks 0,1 :\n",
      "The RECALL is 0.166 / 337 = 0.0004925816023738873\n",
      "The PRECISION is 0.166 / 0.44499999999999995 = 0.3730337078651686 \n",
      "\n",
      "The F-SCORE is 0.0009838640371023428 \n",
      "\n",
      "For pair ks 0,2 :\n",
      "The RECALL is 0.08600000000000001 / 114 = 0.0007543859649122807\n",
      "The PRECISION is 0.08600000000000001 / 0.61 = 0.14098360655737707 \n",
      "\n",
      "The F-SCORE is 0.0015007416455806648 \n",
      "\n",
      "For pair ks 0,3 :\n",
      "The RECALL is 0.08700000000000001 / 225 = 0.0003866666666666667\n",
      "The PRECISION is 0.08700000000000001 / 0.536 = 0.1623134328358209 \n",
      "\n",
      "The F-SCORE is 0.0007714954597048809 \n",
      "\n",
      "For pair ks 1,0 :\n",
      "The RECALL is 0.063 / 836 = 7.535885167464115e-05\n",
      "The PRECISION is 0.063 / 0.6299999999999999 = 0.10000000000000002 \n",
      "\n",
      "The F-SCORE is 0.00015060420974624385 \n",
      "\n",
      "For pair ks 1,1 :\n",
      "The RECALL is 0.036 / 337 = 0.00010682492581602373\n",
      "The PRECISION is 0.036 / 0.46799999999999986 = 0.07692307692307694 \n",
      "\n",
      "The F-SCORE is 0.00021335356241184344 \n",
      "\n",
      "For pair ks 1,2 :\n",
      "The RECALL is 0.15 / 114 = 0.0013157894736842105\n",
      "The PRECISION is 0.15 / 0.44100000000000006 = 0.34013605442176864 \n",
      "\n",
      "The F-SCORE is 0.002621438120953155 \n",
      "\n",
      "For pair ks 1,3 :\n",
      "The RECALL is 0.053000000000000005 / 225 = 0.00023555555555555558\n",
      "The PRECISION is 0.053000000000000005 / 0.28600000000000003 = 0.1853146853146853 \n",
      "\n",
      "The F-SCORE is 0.0004705130367621602 \n",
      "\n",
      "For pair ks 2,0 :\n",
      "The RECALL is 0.13199999999999998 / 836 = 0.00015789473684210524\n",
      "The PRECISION is 0.13199999999999998 / 0.41600000000000004 = 0.31730769230769224 \n",
      "\n",
      "The F-SCORE is 0.0003156324125793863 \n",
      "\n",
      "For pair ks 2,1 :\n",
      "The RECALL is 0.039 / 337 = 0.00011572700296735905\n",
      "The PRECISION is 0.039 / 0.32999999999999996 = 0.1181818181818182 \n",
      "\n",
      "The F-SCORE is 0.00023122758130021047 \n",
      "\n",
      "For pair ks 2,2 :\n",
      "The RECALL is 0 / 114 = 0.0\n",
      "The PRECISION is 0 / 0.38999999999999996 = 0.0 \n",
      "\n",
      "No F-SCORE, can't divide by 0\n",
      "\n",
      "\n",
      "For pair ks 2,3 :\n",
      "The RECALL is 0.224 / 225 = 0.0009955555555555555\n",
      "The PRECISION is 0.224 / 0.44800000000000006 = 0.49999999999999994 \n",
      "\n",
      "The F-SCORE is 0.0019871544657748125 \n",
      "\n",
      "For pair ks 3,0 :\n",
      "The RECALL is 0.14400000000000002 / 836 = 0.00017224880382775122\n",
      "The PRECISION is 0.14400000000000002 / 0.238 = 0.6050420168067228 \n",
      "\n",
      "The F-SCORE is 0.00034439956089055997 \n",
      "\n",
      "For pair ks 3,1 :\n",
      "The RECALL is 0.14500000000000002 / 337 = 0.0004302670623145401\n",
      "The PRECISION is 0.14500000000000002 / 0.267 = 0.5430711610486891 \n",
      "\n",
      "The F-SCORE is 0.000859852876207871 \n",
      "\n",
      "For pair ks 3,2 :\n",
      "The RECALL is 0.061 / 114 = 0.0005350877192982456\n",
      "The PRECISION is 0.061 / 0.196 = 0.3112244897959183 \n",
      "\n",
      "The F-SCORE is 0.001068338645836982 \n",
      "\n",
      "For pair ks 3,3 :\n",
      "The RECALL is 0.017 / 225 = 7.555555555555556e-05\n",
      "The PRECISION is 0.017 / 0.16200000000000003 = 0.10493827160493825 \n",
      "\n",
      "The F-SCORE is 0.0001510023893907498 \n",
      "\n",
      "For pair ks 4,0 :\n",
      "The RECALL is 0.15699999999999997 / 836 = 0.00018779904306220094\n",
      "The PRECISION is 0.15699999999999997 / 0.25299999999999995 = 0.6205533596837944 \n",
      "\n",
      "The F-SCORE is 0.0003754844526716197 \n",
      "\n",
      "For pair ks 4,1 :\n",
      "The RECALL is 0.048 / 337 = 0.000142433234421365\n",
      "The PRECISION is 0.048 / 0.23099999999999996 = 0.20779220779220783 \n",
      "\n",
      "The F-SCORE is 0.00028467133804424864 \n",
      "\n",
      "For pair ks 4,2 :\n",
      "The RECALL is 0.095 / 114 = 0.0008333333333333334\n",
      "The PRECISION is 0.095 / 0.22899999999999995 = 0.4148471615720525 \n",
      "\n",
      "The F-SCORE is 0.0016633254252422765 \n",
      "\n",
      "For pair ks 4,3 :\n",
      "The RECALL is 0.08600000000000001 / 225 = 0.0003822222222222223\n",
      "The PRECISION is 0.08600000000000001 / 0.29200000000000004 = 0.2945205479452055 \n",
      "\n",
      "The F-SCORE is 0.0007634536512614741 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: now that we have prob + word for each k,s pair we can calculate precision and recall \n",
    "\n",
    "list_with_ks = list()  # this list stores the k,s matches found above\n",
    "list_with_ks = [\"0,0\",\"0,1\",\"0,2\",\"0,3\",\"1,0\",\"1,1\",\"1,2\",\"1,3\",\"2,0\",\"2,1\",\"2,2\",\"2,3\",\"3,0\",\"3,1\",\"3,2\",\"3,3\",\"4,0\",\"4,1\",\"4,2\",\"4,3\"]\n",
    "\n",
    "\n",
    "for key in list_with_ks:\n",
    "    numerator_recall = 0\n",
    "    denominator_precision = 0\n",
    "    numerator_precision = 0\n",
    "    for word in k_words_with_prob[int(key[0])]:    \n",
    "        if word in dict_of_words[expert_senses[int(key[2])]]:    # this is bad but works for now\n",
    "            w_weight_recall = k_words_with_prob[int(key[0])][word] * 1\n",
    "            numerator_recall += float(w_weight_recall)\n",
    "            w_weight_precision = k_words_with_prob[int(key[0])][word] * 1\n",
    "            numerator_precision += float(w_weight_precision)\n",
    "    \n",
    "        denominator_precision += float(w_weight_precision)\n",
    "    denominator_recall = len(dict_of_words[expert_senses[int(key[2])]])\n",
    "    \n",
    "            \n",
    "    print(\"For pair ks\",key,\":\")\n",
    "    print(\"The RECALL is\",numerator_recall,\"/\",denominator_recall,\"=\",numerator_recall/denominator_recall)\n",
    "    print(\"The PRECISION is\",numerator_precision,\"/\",denominator_precision,\"=\",numerator_precision/denominator_precision,\"\\n\")\n",
    "    if (numerator_precision/denominator_precision)+(numerator_recall/denominator_recall) != 0:\n",
    "        print(\"The F-SCORE is\", (2*(numerator_precision/denominator_precision)*(numerator_recall/denominator_recall)/((numerator_precision/denominator_precision)+(numerator_recall/denominator_recall))),\"\\n\")\n",
    "    else:\n",
    "        print(\"No F-SCORE, can't divide by 0\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'28355': '0.088', '69419': '0.069', '57460': '0.056', '114587': '0.042', '42071': '0.041', '35267': '0.035', '51647': '0.035', '64448': '0.023', '45980': '0.018', '53826': '0.017'}, 1: {'79223': '0.063', '92927': '0.057', '46574': '0.038', '67660': '0.038', '103085': '0.036', '86112': '0.030', '101982': '0.029', '75808': '0.026', '68539': '0.024', '54607': '0.024'}, 2: {'62258': '0.080', '64586': '0.052', '58271': '0.048', 'nlsj86871': '0.041', '101851': '0.039', 'nlsj183': '0.037', '75653': '0.031', 'nlsj59923': '0.030', '70105': '0.026', '82758': '0.024'}, 3: {'102000': '0.035', '70495': '0.035', '7182': '0.029', '70958': '0.024', '49589': '0.021', '83174': '0.021', '106676': '0.019', '22209': '0.018', '93388': '0.017', '16132': '0.016'}, 4: {'nlsj5634': '0.039', '61925': '0.037', '12620': '0.030', '69419': '0.027', '104421': '0.027', '91085': '0.024', '71308': '0.022', '115748': '0.021', '19641': '0.020', '5390': '0.019'}}\n"
     ]
    }
   ],
   "source": [
    "print(k_words_with_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mus-1', 'mus-4', 'w', 'mus-2']\n"
     ]
    }
   ],
   "source": [
    "print(expert_senses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-169-d6839a6dbd08>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-169-d6839a6dbd08>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    #print(i,dict_of_words[i])\u001b[0m\n\u001b[0m                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "for i in expert_senses:\n",
    "    #print(i,dict_of_words[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qx: Model(s) comparison againstannotated subcorpus (sense importance evolution + sense emergence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode\n",
    "\n",
    "1. Parse senses_target.txt to get:\n",
    "\n",
    "    1.1 the date\n",
    "    \n",
    "    1.2 the number of senses at that date\n",
    "    \n",
    "    1.3 the number of uses of each sense at that date\n",
    "    \n",
    "    \n",
    "2. Using the numbers found in 1.3, plot the emergence of new senses and the distribution of others\n",
    "\n",
    "\n",
    "confidence interval!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
